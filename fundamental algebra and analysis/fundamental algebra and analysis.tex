\documentclass{book}
\include{封装模板.tex}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\numberwithin{equation}{section}

\usepackage{authblk} 

\title{}



\author[I]{Compiled by \textbf{Huayi Chen}}
\author[II]{$$\&$$Edited by \textbf{Jiete Xue}}
\author[III, IV, V]{$$-------------------------$$TAs:Postdoc \textbf{Jiedong Jiang,  Yijun Yuan,  Chunhui Liu}}
\author[VI]{$$$$Substitute Professor\thanks{Chapter 5: Group, Section1-4}: \textbf{Yigeng Zhao}}
\affil[I]{\itshape\small Department of Mathematics (ITS),  School of Science,  Westlake University}
\affil[II]{\itshape\small Undegraduate $\beta$ Collage,  Westlake University}
\affil[I]{\texttt{chenhuayi@westlake.edu.cn}}
\affil[II]{\texttt{xuejiete@westlake.edu.cn}}

\date{}


\begin{document}
\pagecolor{green!2} 
\setlength{\parindent}{0em}
\begin{box3}
        \begin{center}
            \Huge 
    FUNDAMENTAL \\
    ALGEBRA \& ANALYSIS
        \end{center}
    
    \end{box3}


\pagestyle{empty}
\newpage
\nopagecolor
\pagenumbering{roman}
\maketitle

\pagestyle{headings}
\pagenumbering{roman}
\setcounter{page}{1}
\tableofcontents
\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}




\chapter{Basic Logic}
\section{Statement}
\begin{definitionenv}
    We call statement a declarative sentence that is either true or false,  but not both(it can be potential).    
\end{definitionenv}
\begin{exampleenv}
    "$2>1$"(True)\quad "$1<0$"(False)
    \newline
If we specify the value of x , then "$x>2$"becomes a statement,  otherwise it is not a statement.
\end{exampleenv}
\begin{definitionenv}
    In a mathematical theory, 
    \newline 
    axiom refer to statements that accepted to be true without justification.
    \newline
    theorem refer to statements that are proved by assuming axioms.
    \newline
    proposition refer to the statements that are either easy or not used many times.
    \newline
    corollary refer to direct consequence of a theorem.
\end{definitionenv}




\section{Negation}
\begin{definitionenv}
    Let $p$ be a statement,  then the negation of $p$ is denoted by $\neg p$,  which is a statement that is true if and only if $p$ is false.
    In other words,  $p$ and $\neg p$ has opposite truth values.
\end{definitionenv}
\begin{propositionenv}
For any statement $p$,  $\neg \neg p$and $p$ have the same value.
\end{propositionenv}





\section{Conjunction and Disjunction}
\begin{definitionenv}
    Let $p$ and $q$ be statements, 
    \newline
    We denote by $p\wedge q $ the statement "$p$ and $q$".
    \newline
    We denote by $p\vee q $ the statement "$p$ or $q$".
    
\end{definitionenv}


    

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c}
    p & q & $p\wedge q$ & $p\vee q$ \\
    \hline
    T & T & T & T \\
    T & F & F & T \\
    F & T & F & T \\
    F & F & F & F \\    

\end{tabular}
\caption{Truth table for conjunction and disjunction} 
\end{center}
\end{table}

\begin{propositionenv}
    Let $P$and $Q$ be statements $(\neg P)\vee (\neg Q) \text{ and } \neg(P\wedge Q)$ have the same truth value.
\end{propositionenv}


\section{Conditional statements}
\begin{definitionenv}
    Let $P$ and $Q$ be statements, we denote by $P\Rightarrow Q$ the statement(if P then Q).
\end{definitionenv}
\begin{remark}
    It has the same truth value as that of $(\neg P\vee Q)$, only when $P$ is true and $Q$ is false,  otherwise it's true .
    \newline
    If one can prove Q is assuming that $P$ is true,  then $P\Rightarrow Q$ is true .
\end{remark}
\begin{propositionenv}
    Let $P$ and $Q$ be statements. If $P$ and $P\Rightarrow Q$ are true,  then $Q$ is also true.
\end{propositionenv}
\begin{propositionenv}
    Let $P, Q, R$ be statements. If $P \Rightarrow Q$ and $Q\Rightarrow R$ are true,  then $P\Rightarrow R$ is also true.
\end{propositionenv}
\begin{theoremenv}
    Let $P$and $Q$ be statements. $P\Rightarrow Q$ and $(\neg Q)\Rightarrow (\neg P)$ have the same truth value.
\end{theoremenv}
\begin{box2}
    $(\neg Q)\Rightarrow (\neg P)$ is called the contraposition of $P\Rightarrow Q$,  if we prove $(\neg Q)\Rightarrow (\neg P)$,  then $P\Rightarrow Q$ is also true.
\end{box2}

\begin{exampleenv}
    Prove that , let $n$ be an integer,  if $n^2$ is even,  then $n$ is even.
\end{exampleenv}    
    \begin{proofenv}
        Since $n$is an integer,  there exists$k\in \ZZ$ such that $n=2k+1$. Hence $n^2=4k^2+4k+1$ is not even.
    \end{proofenv}




\section{Biconditional statement}
\begin{definitionenv}
    Let $P$and $Q$ be statements. We denote by $P\Leftrightarrow Q$ the statement
    \begin{center}
        "$P$ if and only if $Q$"
    \end{center}
    its true when$P$and $Q$have the same truth value, it's false when they have the opposite truth value.
\end{definitionenv}
\begin{propositionenv}
    Let P and Q be statements.$P \Leftrightarrow Q$ has the same truth value as 
    $$(P\Rightarrow Q)\wedge (Q\Rightarrow P).$$
\end{propositionenv}
\begin{exampleenv}
    Let $n$ be an integer.$n$ is even if and only if $n^2$ is even.
\end{exampleenv}
\begin{definitionenv}
    Let $P$ and $Q$ be statements.
    \newline
    $Q\Rightarrow P$ is called the converse of$P\Rightarrow Q$.
    \newline
    $\neg P \Rightarrow \neg Q$is called the inverse of $P\Rightarrow Q$.
\end{definitionenv}
\begin{remark}
    If one proves $P\Rightarrow Q$and $\neg P\Rightarrow \neg Q$, then $P \Leftrightarrow Q$ is true.
\end{remark}




\section{Proof by Contradiction}
\begin{definitionenv}
    Let $P$ be a statement.If we assume $\neg P$is true and deduce that a certain statement is both true and false,  then we say that a contradiction happens and the assumption $\neg P$ is false. Thus the statement $P$ is true. Such a reasoning is called proof by contradiction.
\end{definitionenv}
\begin{exampleenv}
    Prove that the equation $x^2=2$ does not have solution in $\mathbb{Q}$.
\end{exampleenv}
    \begin{proofenv}
        By contradiction, we assume that $x:=\frac{p}{q}$ is a solution, where $p$ and $q$ are integers , which do not have common prime divisor. By $x^2=2$ we obtain $p^2=2q^2$
        So $p^2$ is even, $p$ is even.Let $p_1\in \ZZ$ such that $p=2p_1$
        Then $p^2=4p_1^2=2q^2$,  hence $q$ is even. Therefore $2$ is a common prime divisor of $p$ and $q$, which leads to a contradiction.
    \end{proofenv}


\section{Exercises}
\begin{enumerate}
    \item Let \(P\) and \(Q\) be statements. Use truth tables to determine the truth values of the following statements according to the truth values of \(P\) and \(Q\):
    \[P\wedge\neg P, \;P\vee\neg P, \;(P\lor Q)\Rightarrow(P\wedge Q), \;(P \Rightarrow Q)\Rightarrow(Q\Rightarrow P)\]
    
    \item Let \(P\) and \(Q\) be statements.
    \begin{enumerate}
        \item Show that \(P\Rightarrow(Q\wedge\neg Q)\) has the same truth value as \(\neg P\).
        \item Show that \((P\wedge\neg Q)\Rightarrow Q\) has the same truth value as \(P\Rightarrow Q\).
    \end{enumerate}
    
    \item Consider the following statements:
    \[P :=``\text{Little Bear is happy}", \]
    \[Q :=``\text{Little Bear has done her math homework}", \]
    \[R :=``\text{Little Rabbit is happy}".\]
    Express the following statements using \(P\),  \(Q\),  and \(R\),  along with logical connectives:
    \begin{enumerate}
        \item If Little Bear is happy and has done her math homework,  then Little Rabbit is happy.
        \item If Little Bear has done her math homework,  then she is happy.
        \item Little Bear is happy only if she has done her math homework.
    \end{enumerate}
    
    \item Does the following reasoning hold? Justify your answer.
    \begin{itemize}
        \item It is known that Little Bear is both smart and lazy,  or Little Bear is not smart.
        \item It is also known that Little Bear is smart.
        \item Therefore,  Little Bear is lazy.
    \end{itemize}
    
    \item Does the following reasoning hold? Justify your answer.
    \begin{itemize}
        \item It is known that at least one of the lion or the tiger is guilty.
        \item It is also known that either the lion is lying or the tiger is innocent.
        \item Therefore,  the lion is either lying or guilty.
    \end{itemize}
    
    \item An explorer arrives at a cave with three closed doors,  numbered 1,  2,  and 3. Exactly one door hides treasure,  while the other two conceal deadly traps.
    \begin{itemize}
        \item Door 1 states: ``\textit{The treasure is not here}'';
        \item Door 2 states: ``\textit{The treasure is not here}'';
        \item Door 3 states: ``\textit{The treasure is behind Door 2}''.
    \end{itemize}
    Only one of these statements is true. Which door should the explorer open to find the treasure?
    
    \item The Kingdom of Truth sent an envoy to the capital of the Kingdom of Lies. Upon entering the border,  the envoy encountered a fork with three paths: dirt,  stone,  and concrete. Each path had a signpost:
    
    \begin{itemize}
        \item The concrete path's sign: ``\textit{This path leads to the capital,  and if the dirt path leads to the capital,  then the stone path also does.}''
        \item The stone path's sign: ``\textit{Neither the concrete nor the dirt path leads to the capital.}''
        \item The dirt path's sign: ``\textit{The concrete path leads to the capital,  but the stone path does not.}''
    \end{itemize}
    All signposts lie. Which path should the envoy take?
    
    \item Let \(a\) and \(b\) be real numbers. Prove that,  if \(a\neq-1\) and \(b\neq-1\),  then \(ab+a+b\neq-1\).
    
    \item Let \(a\),  \(b\),  and \(c\) be positive real numbers such that \(abc>1\) and
    \[a+b+c<\frac{1}{a}+\frac{1}{b}+\frac{1}{c}.\]
    Prove the following:
    \begin{enumerate}
        \item None of \(a\),  \(b\),  or \(c\) equals 1.
        \item At least one of \(a\),  \(b\),  or \(c\) is greater than 1.
        \item At least one of \(a\),  \(b\),  or \(c\) is less than 1.
    \end{enumerate}
    
    \item Let \(a\neq 0\) and \(b\) be real numbers. For real numbers \(x\) and \(y\),  prove that if \(x\neq y\),  then \(ax+b\neq ay+b\).
    
    \item Let \(n\geq 2\) be an integer. Prove that if \(n\) is composite,  then there exists a prime number \(p\) dividing \(n\) such that \(p\leq\sqrt{n}\).
    
    \item Let \(n\) be an integer. Prove that either 4 divides \(n^{2}\) or 4 divides \(n^{2}-1\).
    
    \item Let \(n\) be an integer. Prove that 12 divides \(n^{2}(n^{2}-1)\).
    
    \item Prove that any integer divisible by 4 can be written as the difference of two perfect squares.
    
    \item Let \(x\) and \(y\) be non-zero integers. Prove that \(x^{2}-y^{2}\neq 1\).
    
    \item A plane has 300 seats and is fully booked. The first passenger ignores their assigned seat and chooses randomly. Subsequent passengers take their assigned seat if available; otherwise,  they choose randomly. What is the probability that the last passenger sits in their assigned seat?
    
    \item Little Bear,  Little Goat,  and Little Rabbit are all wearing hats. A parrot prepared four red feathers and four blue feathers to decorate their hats. The parrot selected two feathers for each hat-wearing animal to place on their hats. Each animal cannot see the feathers on their own hat but can see the feathers on the other animals' hats. Here is their conversation:
    \begin{itemize}
        \item Little Bear: \textit{I don't know what color the feathers on my hat are,  but I know the other animals also don't know what color the feathers on their hats are.}
        \item Little Goat: \textit{Haha,  now even without looking at Little Bear's hat,  I know what color the feathers on my hat are.}
        \item Little Rabbit: \textit{Now I know what color the feathers on my hat are.}
        \item Little Bear: \textit{Hmm,  now I also know what color the feathers on my hat are.}
    \end{itemize}
    Question: What color are the feathers on Little Goat's hat?
    
    \item The Sphinx tells the truth on one fixed weekday and lies on the other six. Cleopatra visits The Sphinx for three consecutive days:
    \begin{itemize}
        \item Day 1: The Sphinx declared,  ``\textit{I lie on Monday and Tuesday.}''
        \item Day 2: The Sphinx declared,  ``\textit{Today is either Thursday,  or Saturday,  or Sunday.}''
        \item Day 3: The Sphinx declared,  ``\textit{I lie on Wednesday and Friday.}''
    \end{itemize}
    On which day does the Sphinx tell the truth? On which days of the week did Cleopatra visit the Sphinx?
\end{enumerate}

\chapter{Set Theory}
\section{Roster Notation}
\begin{definitionenv}
    \quad
    \newline
    (1) We call a \textbf{set} a certain collection of distinct objects.
    \newline
    (2) An object in a collection considered as a set is called \textbf{element} of it .
    \newline
    (3) Two sets $A$ and $B$ are said to be \textbf{equal} if they have the same elements.We denoted by $A=B$ the statement "A and B are equal".
    \newline
    (4) If $A$ is a set and $x$ is an object,  $x \in A$ denotes $x$ is an element of $A$ (reads x belongs to A),  $x \notin A$ denotes "x is NOT an element of A".
\end{definitionenv}
\texttt{Notation Roster method: to be continue$\dots$}
\begin{exampleenv}
    \{1, 2, 3\}=\{3, 2, 1\}=\{1, 1, 2, 3\}
\end{exampleenv}
\begin{box2}
More generally,  if $I$ is a set,  and for any $i \in I$,  we fix an $x_i$,  then the set of all $x_i$ is noted as $$\{x_i|i\in I\}.$$
\end{box2}
\begin{exampleenv}
    $$\{2k+1|k\in \ZZ\}.$$
\end{exampleenv}
\section{Set-builder Notation}
\begin{definitionenv}
    Let $A$ be a set. If for any $x\in A $ we fix a statement $P(x)$,  then we say that $P(\cdot)$ is a \textbf{condition} on $A$. 
\end{definitionenv}
\begin{exampleenv}
    "$n$ is even" is a condition on $\NN$,  "$x>2$" is a condition on $\RR$.
\end{exampleenv}
\begin{definitionenv}
    Let $A$ be a set and $P(\cdot)$ be a condition on $A$ .If $x\in A$ is such that $P(x)$ is true,  then we say that $x$ satisfies the condition $P(\cdot)$.We noted by $$\{x \in A|P(x)\}$$the set of $x\in A$ that satisfies the condition $P(\cdot)$.
\end{definitionenv}
\begin{exampleenv}
    $\{x\in \RR|x>2\}$ denotes the set of real numbers that are $x>2$.
\end{exampleenv}
\begin{box2}
\begin{center}
    sometimes we combine the two methods of representation.
\end{center}
\end{box2}




\section{Subsets and Set Difference}
\begin{definitionenv}
    Let $A$ and $B$ be sets. If any element of $A$ is an element of $B$,  we say that $A$ is a subset of $B$, denoted as $A \subseteq B$ or $B\supseteq  A$.
\end{definitionenv}
\begin{exampleenv}
    \quad
   \begin{itemize}
    \item  We denote by $\varnothing $ the set that does not have any element.We consider it as a subset of any set.
   \item  Let $A$ be a set , then $A\subseteq A$
    \end{itemize}
\end{exampleenv}
\begin{definitionenv}
    Let $A$ be a set , we denote by $\wp  (A)$ the set of all subset of $A$,  called the power set of $A$.
\end{definitionenv}
\begin{exampleenv}
    $\wp (\varnothing)=\{\varnothing\}$
    , 
    $\wp (\wp (\varnothing))=\{\varnothing, \{\varnothing\}\}$.
\end{exampleenv}
\begin{definitionenv}
    Let $A$ and $B$ be sets. We denote by $B \backslash A$ the set $$\{x\in B|x\notin A\}.$$ This is a subset of $B$ called the \textbf{set difference of $\mathbf{B}$ and $\mathbf{A}$}.
    \newline
    If in condition $A \subseteq B$,  we say that $B\backslash A$ is the complement of $A$ inside $B$.
\end{definitionenv}
\begin{exampleenv}
    If $A$ is a set,  $P(\cdot)$ is  a condition on $A$,  then $$\{x\in A |\neg P(x)\}=A\backslash\{x\in A|P(x)\}.$$
\end{exampleenv}
\begin{propositionenv}
    Let $A$ and $B$ be sets.Then $$B\backslash A=\varnothing \Leftrightarrow B\subseteq A.$$
    If in condition $A$ is the subset of $B$,  then $$B\backslash A=\varnothing \Leftrightarrow A=B.$$
\end{propositionenv}


\section{Quantifiers}
\begin{definitionenv}
    Let $A$ be a set and $P(\cdot)$ be a condition on $A$ .We denote by 
    \newline
    "$\forall x\in A,  P(x)$" the statement $\{x\in A|P(x)\}=A$
    \newline
    "$\exists x\in A, P(x)$"denotes$\{x\in A|P(x)\}\not= \varnothing$.
\end{definitionenv}
\begin{exampleenv}
    $\forall x\in \varnothing , P(x)$ is true ; $\exists x\in \varnothing, P(x)$ is false.
\end{exampleenv}
\begin{theoremenv}\label{theorem2.4.1}
    Let $A$ be a set and $P(\cdot)$ be a condition on $A$
    \newline
    (1)$\exists x\in A, \neg P(x) $ and $ \forall x\in A, P(x)$ have opposite truth values.
    \newline
    (2)$\forall x\in A, \neg P(x)$ and $\exists x\in A, P(x)$ have opposite truth value.
\end{theoremenv}



\section{Sufficient and Necessary Condition}
\begin{definitionenv}
    Let $A$ be a set and $P(\cdot)$ and $Q(\cdot)$ be conditions on $A$.
    If $$\{x\in A|P(x)\}\subseteq\{x\in A|Q(x)\}, $$ we say that $P(\cdot)$ is a \textbf{sufficient condition} of $Q(\cdot)$ and $Q(\cdot)$ is a \textbf{necessary condition} of $P(\cdot)$. 
    If $\{x\in A|P(x)\}=\{x\in A|Q(x)\}$, we say that $P(\cdot)$ and $Q(\cdot)$ are equivalent.
\end{definitionenv}
\begin{propositionenv}
    Let $A$ be a set , $P(\cdot)$ and $Q(\cdot)$ be conditions on $A$.
    \newline
    (1)$P(\cdot)$ is a sufficient condition of $Q(\cdot)$ iff.$\forall x\in A, P(x)\Rightarrow Q(x)$
    \newline
    (2)$P(\cdot) $ is a necessary condition of $Q(\cdot)$ iff.$\forall x\in A, Q(x)\Rightarrow P(x)$
    \newline
    (3)$P(\cdot)$ and $Q(\cdot)$ are equivalent iff. $\forall x\in A, P(x)\Leftrightarrow Q(x)$
\end{propositionenv}
\begin{proofenv}
\begin{align*}
    \varnothing&=\{x\in A|P(x)\}- \{x\in A|Q(x)\}\\
    &=\{x\in A|P(x)\wedge (\neg Q(x))\}\\
    &=A\backslash\{x\in A|(\neg P(x))\vee  Q(x)\}\\
    &=A\backslash\{x\in A |P(x)\Rightarrow Q(x)\}.
\end{align*}
\end{proofenv}
\begin{box2}
\textbf{Russell's paradox} leads to: $P(A):=A\notin A$. The collection of all sets should not be considered as a set.
\end{box2}


\section{Union}
\begin{definitionenv}
    Let $I$ be a set , and for any $i \in I$,  let $A_i$ be a set , we say that $(A_i)_{i\in I}$ is a family of sets parametrized by $I$.We denote by $\cup_{i \in I}A_i$ the set of all elements of all $A_i$.It is also called the \textbf{union} of the sets $A_i, i\in I$. By definition, a mathematical object $x$ belongs to $\cup_{i \in I}A_i$ if and only if $$\exists i\in I, x\in A_i.$$ 
\end{definitionenv}
\begin{propositionenv}
    $ \displaystyle \bigcup_{i\in I}A_i\subseteq B$ if and only if $$\forall i\in I, A_i\subseteq B.$$
\end{propositionenv}
\begin{corollaryenv}\label{corollary2.6.1}
    Let $P_i(\cdot) $be a condition on $B$, then
    $$\{x\in B|\exists i\in I, P_i(x)\}=\bigcup_{i\in I}\{x\in B|P_i(x)\}.$$
\end{corollaryenv}
\begin{propositionenv}
    $$\left( \bigcup _{i\in I}A_i\right)\backslash B=\bigcup_{i\in I}\left( A_i\backslash B\right).$$
\end{propositionenv}
\section{Intersection}
\begin{definitionenv}
    Let $I$ be a \textbf{non-empty} set and $(A_i)_{i\in I} $ be a family os sets parametrized by $I$. We denote by $\displaystyle \bigcap_{i\in I}A_i $ the set of all common elements of $A_i, i\in I$.This set is called the \textbf{intersection} of $A_i, i\in I$.Note that , if $i_0$ is an arbitrary element of $I$ , the set-builder notation ensure that
    $$\{x\in A_{i_0}|\forall i\in I, x\in A_i\}$$ is a set. This set is the intersection of $(A_i)_{i\in I}$.
    \newline
    By definition,  an mathematical object $x$ belongs to $\cap _{i\in I}A_i$ if and only if $$\forall i\in I , x\in A_i.$$
    
\end{definitionenv}
\begin{remark}
    In set theory,  it does not make sense to consider the intersection of an empty family of sets .In fact,  if such an intersection exists as a sets , for any mathematical object $x$,  since the statement $$\forall i\in \varnothing, x \in A_i$$is true,  we obtain that $x$ belongs to $\cap_{i\in \varnothing}A_i$.By Russell's paradox,  this is impossible.
\end{remark}
\begin{propositionenv}\label{proposition2.7.1}
Let \( I \) be a non-empty set and \( (A_i)_{i \in I} \) be a set parametrised by \( I \). Let \( B \) be a set. Then \( B \subseteq \bigcap_{i \in I} A_i \) if and only if
\[
\forall\,  i \in I, \,  B \subseteq A_i.
\]
\end{propositionenv}


\begin{proofenv}
Let \( A = \bigcap_{i \in I} A_i \).

Suppose that \( B \subseteq A \). For any \( x \in B \),  one has \( x \in A \),  and hence
\[
\forall\,  i \in I, \,  x \in A_i.
\]
Therefore,  for any \( i \in I \),  \( B \) is contained in \( A_i \).

Suppose that,  for any \( i \in I \),  \( B \subseteq A_i \). Then,  for any \( x \in B \) and any \( i \in I \),  one has \( x \in A_i \). Hence,  for any \( x \in B \),  one has \( x \in A \). Therefore, 
\( B \subseteq A \).
\end{proofenv}

\begin{corollaryenv}\label{corollary2.7.1}
Let \( B \) be a set,  \( I \) be a non-empty set. For any \( i \in I \),  let \( P_i(\cdot) \) be a condition on \( B \). Then
\[
\{x \in B \mid \forall\,  i \in I, \,  P_i(x)\} = \bigcap_{i \in I} \{x \in B \mid P_i(x)\}.
\]
\end{corollaryenv}

\begin{proofenv}
Let
\[
A := \{x \in B \mid \forall\,  i \in I, \,  P_i(x)\}.
\]
For any \( i \in I \),  let
\[
A_i := \{x \in B \mid P_i(x)\}.
\]
For any \( x \in A \) and any \( i \in I \),  \( P_i(x) \) is true. Hence \( A \subseteq A_i \). By Proposition \ref{proposition2.7.1} we obtain
\[
A \subseteq \bigcap_{i \in I} A_i.
\]
Conversely,  if \( x \in \bigcap_{i \in I} A_i \),  then for any \( i \in I \),  one has \( x \in A_i \). Hence \( x \in B \),  and for any \( i \in I \),  \( P_i(x) \) is true. Thus \( x \in A \).
\end{proofenv}

\begin{propositionenv}
Let \( B \) be a set,  \( (A_i)_{i \in I} \) be a family of sets. The following equality holds
\[
\left( \bigcap_{i \in I} A_i \right) \setminus B = \bigcap_{i \in I} (A_i \setminus B).
\]
\end{propositionenv}

\begin{proofenv}
Let \( A := \bigcap_{i \in I} A_i \). For any \( i \in I \),  one has \( A \subseteq A_i \). Hence
\[
A \setminus B = \{x \in A \mid x \notin B\} \subseteq \{x \in A_i \mid x \notin B\}.
\]
By Proposition \ref{proposition2.7.1} we get
\[
A \setminus B \subseteq \bigcap_{i \in I} (A_i \setminus B).
\]
Conversely,  if \( x \in \bigcap_{i \in I} (A_i \setminus B) \),  then,  for any \( i \in I \),  one has \( x \in A_i \setminus B \),  namely \( x \in A_i \) and \( x \notin B \). Thus \( x \in \bigcap_{i \in I} A_i \) and \( x \notin B \). Therefore \( x \in A \setminus B \).
\end{proofenv}

\begin{propositionenv}
Let \( I \) be a set and \( (A_i)_{i \in I} \) be a family of sets parametrised by \( I \). For any set \( B \),  the following statements hold.
\begin{enumerate}
    \item \( B \cap \left( \bigcup_{i \in I} A_i \right) = \bigcup_{i \in I} (B \cap A_i) \).
    \item If \( I \neq \varnothing \),  \( B \cup \left( \bigcap_{i \in I} A_i \right) = \bigcap_{i \in I} (B \cup A_i) \), 
    \item If \( I \neq \varnothing \),  \( B \setminus \bigcup_{i \in I} A_i = \bigcap_{i \in I} (B \setminus A_i) \), 
    \item If \( I \neq \varnothing \),  \( B \setminus \bigcap_{i \in I} A_i = \bigcup_{i \in I} (B \setminus A_i) \).
\end{enumerate}
\end{propositionenv}

\begin{proofenv}
\begin{enumerate}
    \item By Corollary \ref{corollary2.7.1} we obtain
    \begin{align*}
    B \cap \left( \bigcup_{i \in I} A_i \right) &= \{x \in B \mid \exists\,  i \in I, \; x \in A_i\} \\
    &= \bigcup_{i \in I} \{x \in B \mid x \in A_i\} = \bigcup_{i \in I} (B \cap A_i).
    \end{align*}
    \item Let \( A := \bigcap_{i \in I} A_i \). By definition,  for any \( i \in I \),  one has \( A \subseteq A_i \) and hence \( B \cup A \subseteq B \cup A_i \). Thus,  by Proposition \ref{proposition2.7.1} we obtain
    \[
    B \cup \left( \bigcap_{i \in I} A_i \right) \subseteq \bigcap_{i \in I} (B \cup A_i).
    \]
    Conversely,  let \( x \in \bigcap_{i \in I} (B \cup A_i) \). For any \( i \in I \),  one has \( x \in B \cup A_i \). If \( x \in B \),  then \( x \in B \cup \left( \bigcap_{i \in I} A_i \right) \); otherwise one has
    \[
    \forall\,  i \in I, \; x \in A_i, 
    \]
    and we still get \( x \in B \cup \left( \bigcap_{i \in I} A_i \right) \).

    \item By Theorem\ref{theorem2.4.1}
    \begin{align*}
    B \setminus \bigcup_{i \in I} A_i &= \{x \in B \mid \neg (\exists\,  i \in I, \; x \in A_i) \} \\
    &= \{x \in B \mid \forall\,  i \in I, \; x \notin A_i \}.
    \end{align*}
    By Corollary \ref{corollary2.7.1} this is equal to
    \[
    \bigcap_{i \in I} \{x \in B \mid x \notin A_i\} = \bigcap_{i \in I} (B \setminus A_i).
    \]

    \item By Theorem\ref{theorem2.4.1}
    \begin{align*}
    B \setminus \bigcap_{i \in I} A_i &= \{x \in B \mid \neg (\forall\,  i \in I, \; x \in A_i) \} \\
    &= \{x \in B \mid \exists\,  i \in I, \; x \notin A_i \}.
    \end{align*}
    By Corollary \ref{corollary2.6.1} this is equal to
    \[
    \bigcup_{i \in I} \{x \in B \mid x \notin A_i\} = \bigcup_{i \in I} (B \setminus A_i).
    \]
\end{enumerate}
\end{proofenv}

\section{Cartesian Product}

\begin{definitionenv}
Let \(A\) and \(B\) be sets. We denote by \(A \times B\) the following set of ordered pairs
\[
\{(x,  y) \mid x \in A, \ y \in B\}, 
\]
and call it the \textbf{Cartesian product} of sets \(A\) and \(B\).

More generally,  if \(n\) is a positive integer and \(A_1,  \ldots,  A_n\) be sets,  we denote by
\[
A_1 \times \cdots \times A_n
\]
the set of all \(n\)-tuples \((x_1,  \ldots,  x_n)\),  where \(x_1 \in A_1,  \ldots,  x_n \in A_n\).
\end{definitionenv}

The following proposition shows ordered pairs can be realized through set-theoretic constructions.

\begin{propositionenv}
Let \(x\),  \(y\),  \(x^{\prime}\),  and \(y^{\prime}\) be mathematical objects. Then
\[
\{\{x\}, \{x, y\}\}=\{\{x^{\prime}\}, \{x^{\prime}, y^{\prime}\}\}
\]
if and only if \(x=x^{\prime}\) and \(y=y^{\prime}\).
\end{propositionenv}

\begin{proofenv}
If \(x=x^{\prime}\) and \(y=y^{\prime}\),  then the equality
\[
\{\{x\}, \{x, y\}\}=\{\{x^{\prime}\}, \{x^{\prime}, y^{\prime}\}\}
\]
certainly holds.

Conversely,  suppose the equality
\[
\{\{x\}, \{x, y\}\}=\{\{x^{\prime}\}, \{x^{\prime}, y^{\prime}\}\}
\]
holds. If \(x\neq x^{\prime}\),  then \(\{x\}\neq\{x^{\prime}\}\),  so \(\{x\}=\{x^{\prime}, y^{\prime}\}\). This still implies \(x=x^{\prime}\),  leading to a contradiction. Therefore,  \(x=x^{\prime}\) must hold.

Now,  assume \(y\neq y^{\prime}\). Then \(\{x, y\}\neq\{x^{\prime}, y^{\prime}\}\),  unless \(y=x^{\prime}\) and \(x=y^{\prime}\). Since \(x=x^{\prime}\),  this would imply \(y=y^{\prime}\),  which is a contradiction. Thus,  \(\{x, y\}=\{x^{\prime}\}\) and \(\{x^{\prime}, y^{\prime}\}=\{x\}\). This again leads to \(y=x^{\prime}\) and \(x=y^{\prime}\),  resulting in a contradiction. Hence,  \(y=y^{\prime}\) must hold.
\end{proofenv}

\chapter{Correspondence}
\section{Correspondence and its Inverse}
\begin{definitionenv}
    We call a \textbf{correspondence} any triplet of the form $$f=(\mathscr{D} _f, \mathscr{A}_f, \Gamma_f)$$
    where $\mathscr{D}_f, \mathscr{A}_f$ are two sets,  called respectively the \textbf{departure set }  and the \textbf{arrival set} of $f$ and $\Gamma_f$ is a subset of $\mathscr{D}_f\times \mathscr{A}_f$,  called the \textbf{graph} of $f$.
    \newline
    If $X, Y$ are two sets and $f$ is a correspondence of the form $(X, Y, \Gamma_f)$, we say that $f$ is a correspondence from $X$ to $Y$. 
\end{definitionenv}
\begin{definitionenv}
    Let $f$ be a correspondence.We denote by $f^{-1}$ the correspondence defined as follows:
    $$\mathscr{D}_f^{-1}:=\mathscr{A}_f, \mathscr{A_f}^{-1}:=\mathscr{D}_f, $$
    $$\Gamma_{f^{-1}}:=\{(y, x)\in \mathscr{D}_f\times \mathscr{A}_f|(x, y)\in \Gamma_f\}.$$
    The correspondence $f^{-1}$ is called the \textbf{inverse correspondence} of $f$. Clearly one has $$(f^{-1})^{-1}=f, $$
    namely $f$ is the inverse correspondence of $f^{-1}$.
\end{definitionenv}
\section{Illustration of a Correspondence}
\section{Image and Preimage}
\begin{definitionenv}
    Let $X, Y$ be sets , and $f$ be a correspondence from $X$ to $Y$. If $(x, y)$ is an element of $\Gamma_f$,  we say that $x$ is a \textbf{preimage } of $y$ under $f$,  and $y$ is an \textbf{image} of $x$ under $f$.
    \newline
    If $A$ is a set ,  we denote by $f(A)$ the set :
    $$\{y\in \mathscr{A}_f|\exists x\in A , (x, y)\in \Gamma_f\}, $$
    called the image of $A$ by the correspondence $f$.
    \newline 
    If $B$ is a set , the set $f^{-1} (B)$ is called the \textbf{preimage of $B$ by the correspondence $f$}. Note that it is by definition the image of $B$ by the inverse correspondence $f^{-1}$.
\end{definitionenv}
\begin{definitionenv}
    Let $f$ be correspondence. The set $f(\mathscr{D}_f)$ is called the \textbf{range} of $f$,  denoted as $\mathrm{Im} (f)$. The set $f^{-1}(\mathscr{A}_f)$ is called the \textbf{domain of definition }of $f$,  denoted as $\mathrm{Dom}( f) $.
    Note that the domain of definition of a correspondence $f$ is the projection of the graph $\Gamma_f$ to the arrival set $\mathscr{A}_f$.
\end{definitionenv}
\begin{box2}
    For any sets $ A$ and $B$, 
$$f(A)\subseteq \mathrm{Im}(f), f^{-1}(B)\subseteq\mathrm{Dom}(f), $$
$$\mathrm{Dom}(f)=\mathrm{Im}(f^{-1}), \mathrm{Im }(f)=\mathrm{Dom}(f^{-1}).$$
\end{box2}
\begin{propositionenv}\label{proposition3.3.1}
    Let $f$ be a correspondence.
    \newline
    (1) If $A$ and $A'$ are two sets such that $A'\subseteq A$ , then one has $f(A')\subseteq f(A)$.
    \newline
    (2) If $B$ and $B'$ are two sets such that $B'\subseteq B$ , then one has $f^{-1}(B')\subseteq f^{-1}(B)$.
\end{propositionenv}
\begin{proofenv}
    \begin{align*}
        f(B')&=\{y\in\mathrm{Im}(f)|\exists x\in B', (x, y)\in \Gamma_f\}\\
        &\subseteq\{y\in\mathrm{Im}(f)|\exists x\in B', (x, y)\in \Gamma_f\}\\
        &=f(B).
    \end{align*}
\end{proofenv}    
\begin{propositionenv}\label{proposition3.3.2}
    Let $f$ be a correspondence. The following equalities hold:
    $$\mathrm{Im}(f)=f(\mathrm{Dom}(f)), \mathrm{Dom}(f)=f^{-1}(\mathrm{Im}(f)).$$
\end{propositionenv}
\begin{proofenv}
    Since $\mathrm{Dom}(f)\subseteq\mathscr{D}_f$,  by proposition \ref{proposition3.3.1},  one has 
    $$f(\mathrm{Dom}(f))\subseteq f(\mathscr{D}_f)=\mathrm{Im}(f).$$
    Let $y$ be an element of $\mathrm{Im}(f)$,  there exist $x\in \mathscr{D}_f$ such that $(x, y)\in \Gamma_f$. By definition,  one has $x\in \mathrm{Dom }(f)$ and hence $y\in f(\mathrm{Dom }(f)), \mathrm{Im}(f)\subseteq f(\mathrm{Dom }(f))$. Therefore the equality $\mathrm{Im}(f)=f(\mathrm{Dom}(f))$ is true. Applying this equality to $f^{-1}$,  we obtain the second equality.
\end{proofenv}
\begin{propositionenv}
    Let $f$ be a correspondence,  $A$ be a set and $y$ be an mathematical object. Then $y$ belongs to $f(A)$ if and only if $A\cap f^{-1}(\{y\})\not=\varnothing$.
\end{propositionenv}
\begin{propositionenv}\label{proposition3.3.4}
    Let $f$ be a correspondence,  $I$ be a set and $(A_i)_{i\in I}$ be a family of sets parametrised by $I$. Then
    $$f\left( \bigcup_{i\in I}A_i\right)=\bigcup_{i\in I}f(A_i).$$
    Moreover, if $I$ is not empty,  then 
    $$f\left( \bigcap_{i\in I}A_i\right)\subseteq \bigcap_{i\in I}f(A_i).$$

\end{propositionenv}
\begin{proofenv}
   \begin{align*}
         f\left( \bigcup_{i\in I}A_i\right)&=\left\{ y\in Y|\left( \bigcup_{i\in I}A_i\right)\cap f^{-1}(\{y\})\not=\varnothing  \right\}\\
        &=\left\{ y\in Y|\bigcup_{i\in I}\left(A_i\cap f^{-1}(\{y\})\right)\not=\varnothing  \right\}\\
        &=\left\{y\in Y|\exists i \in I, A_i\cap f^{-1}(\{y\})\not =\varnothing\right\}=\bigcup_{i\in I}f(A_i).
   \end{align*}

    \begin{align*}
        f\left(\bigcap_{i\in I}A_i \right)&=\left\{ y\in Y|\left( \bigcap_{i\in I}A_i\right) \cap f^{-1}(\{y\})\not=\varnothing\right\}\\
        &=\left\{ y\in Y| \bigcap_{i\in I}\left(A_i \cap f^{-1}(\{y\})\right)\not=\varnothing\right\}\\
        &\subseteq \left\{  y\in Y|\forall i\in I,  A_i\cap f^{-1}(\{y\})\not=\varnothing\right\}\\
        &=\bigcap_{i\in I}f(A_i).
    \end{align*}
\end{proofenv}
\section{Composition}
\begin{definitionenv}
    Let $f$ and $g$ be correspondences. We define the \textbf{composite } of $g$ and $f$ as the correspondence $g\circ f$ from $\mathscr{D}_f$  to $\mathscr{A}_g$ whose graph $\Gamma_{g\circ f }$ is composed of the element $(x, z)$ of $\mathscr{D}_f\times \mathscr{A}_g$ such that there exists some objet $y$ satisfying $(x, y)\in \Gamma_f $ and $(y, z)\in \Gamma_g$. In other words, 
    $$\Gamma_{g\circ f }=\{(x, z)\in \mathscr{D}_f\times \mathscr{A}_g|\exists y\in \mathscr{A}_f\cap\mathscr{D}_g, (x, y)\in \Gamma_f \wedge (y, z)\in \Gamma_g\}.$$
\end{definitionenv}
\begin{propositionenv}
    Let $f$ and $g$ be correspondences. The following equality holds:
    \begin{equation}
        (g\circ f)^{-1}=f^{-1}\circ g^{-1}.
    \end{equation}
\end{propositionenv}
\begin{propositionenv}\label{proposition3.4.2}
    Let $f$ and $g$ be correspondences.The following equality holds:
    \begin{equation}
        h\circ (g\circ f)=(h\circ g)\circ f.
    \end{equation}
\end{propositionenv}
\begin{propositionenv}\label{proposition3.4.3}
    Let $X$ and $Y$ be sets , $f$ be a correspondence from $X$ to $Y$.Then the following equalities hold:
    $$f\circ \mathrm{Id}_X=f=\mathrm{Id}_Y\circ f.$$
\end{propositionenv}
Propositions above can be proved by definition.
\begin{propositionenv}\label{proposition3.4.4}
    Let $f$ and $g$ be correspondences.For any set $A$ , one has 
    $$(g\circ f)(A)=g(f(A)).$$
    In particular, 
    $$\mathrm{Im}(g\circ f)=g(\mathrm{Im}(f))\subseteq \mathrm{Im}(g).$$
    If in addition $\mathrm{Dom}(g)\subseteq \mathrm{Im}(f)$,  then the equality $\mathrm{Im}(g\circ f)=\mathrm{Im}(g)$ holds.
\end{propositionenv}
\begin{proofenv}
    By definition, 
    \begin{align*}
        (g\circ f )(A)&=\{z\in \mathscr{A}_g|\exists x\in A, (x, z)\in \Gamma_{g\circ f}\}\\
        &=\{z\in \mathscr{A}_g|\exists x\in A, \exists y\in \mathscr{A}_f, (x, y)\in \Gamma_f, (y, z)\in \Gamma_g\}\\
        &=\{z\in \mathscr{A}_g|\exists  y\in f(A), (y, z)\in \Gamma_g\}=g(f(A)).
    \end{align*}
    Applying this equality to the case where $A=\mathscr{D}_f$,  we obtain
    $$\mathrm{Im}(g\circ f)=(g\circ f)(\mathscr{D}_f)=g(f(\mathscr{D}_f))=g(\mathrm{Im}(f))\subseteq \mathrm{Im}(g).$$
   In the case where $\mathrm{Dom}(g)\subseteq \mathrm{Im}(f)$,  by proposition \ref{proposition3.3.1} and \ref{proposition3.3.2} we obtain 
   $$\mathrm{Im}(g)=g(\mathrm{Dom}(g))\subseteq g(\mathrm{Im}(f))=\mathrm{Im}(g\circ f).$$ 
\end{proofenv}


\section{Surjectivity}
\begin{definitionenv}
    Let $f$ be a correspondence. If $\mathscr{A}_f=\mathrm{Im}(f)$,  we say that $f$ is \textbf{surjective}. If $f^{-1}$ is surjective ,  or equivalently $\mathrm{Dom}(f)=\mathscr{D}_f$ ,  we say that $f$ is a \textbf{multivalued mapping}.
\end{definitionenv}
\begin{remark}
    multivalued mapping is not always a mapping
\end{remark}
\begin{propositionenv}\label{proposition3.5.1}
    Let $f$ be a correspondence.Assume that $f$ is surjective. Then , for any subset $B$ of $\mathscr{A}_f$,  one has $B\subseteq f(f^{-1}(B))$.
\end{propositionenv}
\begin{proofenv}
    Let $y$ be an element of $B$. Since $f$ is surjective there exists $x\in \mathscr{D}_f$ such that $(x, y)\in \Gamma_f$ .Therefore,  $x\in f^{-1}(B)$ and hence $y\in f(f^{-1}(B))$
\end{proofenv}
\begin{propositionenv}\label{proposition3.5.2}
    Let $f$ and $g$ be correspondences.
    \newline
    (1) If $g\circ f$ is surjective,  so is $g$.
    \newline
    (2) If $g\circ f$ is multivalued mapping,  so is $f$.
\end{propositionenv}
\begin{proofenv}
    One has 
    $$\mathrm{Im}(g\circ f)\subseteq \mathrm{Im}(g)\subseteq \mathscr{A}_g=\mathscr{A}_{g\circ f}.$$
    If $g\circ f $ is surjective,  namely $\mathrm{Im}(g\circ f )=\mathscr{A}_{g\circ f}$,  then we deduce $\mathrm{Im}(g)=\mathscr{A}_g$,  namely $g$ is surjective.
\end{proofenv}
\begin{propositionenv}\label{proposition3.5.3}
    Let $f$ and $g$ be correspondences.
    \newline
    (1) If $g$ is surjective and $\mathrm{Dom}(g)\subseteq \mathrm{Im}(f)$, then $g\circ f $ is also surjective.
    \newline
    (2) If $f$ is a multivalued mapping and $\mathrm{Im}(f)\subseteq \mathrm{Dom}(g)$,  then $g\circ f $ is a multivalued mapping.
\end{propositionenv}
\begin{proofenv}
    (1) Since $\mathrm{Dom}(g)\subseteq \mathrm{Im}(f)$, by proposition\ref{proposition3.4.4}, we obtain 
    $$\mathrm{Im}(g\circ f )=\mathrm{g}.$$
    Since $g$ is surjective, 
    $$\mathrm{Im}(g)=\mathscr{A}_g=\mathscr{A}_{g\circ f}.$$
    Hence $g\circ f $ is also surjective.
    \newline
    Applying (1) to $g^{-1}$ and $f^{-1}$ , we obtain (2).
\end{proofenv}

\section{injectivity}
\begin{definitionenv}
    Let $f$ be a correspondence. If each element of $\mathscr{D}_f$ has at most one image under $f$ ,  we say that $f$ is a \textbf{function}.If $f^{-1}$ is a function,  we say that $f$ is \textbf{injective} .
\end{definitionenv}
\begin{notationenv}
    Functions form a special case of correspondences.The definition feature of functions is that corresponding to each element in the domain of definition,  is a unique element in the arrival set of function.
    \newline
    Let $f$ be a function, and let $x\in \mathrm{Dom}(f)$. We denote the unique image of $x$ under $f$ as $f(x)$ , and we say that $f$ sends $x\in \mathrm{Dom}(f)$ to $f(x)$ or $f(x)$  is the \textbf{value} of $f$ at $x$ .we can also use the notation:
    $$x\mapsto f(x)$$
    to indicate the correspondence of $x$ to its image under $f$.
\end{notationenv}
\begin{propositionenv}\label{proposition3.6.1}
    Let $f$ be a correspondence.
    \newline
    (1) Assume that $f$ is injective.For any set $A$ one has $f^{-1}(f(A))\subseteq A$.
    \newline
    (2) Assume that $f$ is a function. For any set $B$ on has $f(f^{-1}(B))\subseteq B$. 
\end{propositionenv}
\begin{proofenv}
    Let $x$ be an element of $f^{-1}(f(A))$ , By definition, there exists $y\in f(A)$ such that $(x, y)\in \Gamma_f$. Since $y\in f(A) $ there exist $x'\in A$ such that $(x', y)\in \Gamma_f$.Since $y$ admits at most one preimage,  we obtain $x'=x$. Hence $x\in A$.
    \newline
    Applying (1) to $f^{-1}$ we obtain (2).
\end{proofenv}
\begin{propositionenv}\label{proposition3.6.2}
    Let \( f \) and \( g \) be correspondences.
\newline
    (1) If \( f \) and \( g \) are functions,  so is \( g \circ f \). Moreover,  for any \( x \in \text{Dom}(g \circ f) \),  one has \((g \circ f)(x) = g(f(x))\).
    \newline
    (2) If \( f \) and \( g \) are injective,  so is \( g \circ f \).

\end{propositionenv}
\begin{proofenv}
Let \( x \) be an element of \(\text{Dom}(g \circ f)\). Assume that \( z \) and \( z' \) are images of \( x \) under \( g \circ f \). Let \( y \) and \( y' \) be such that

\[
(x,  y) \in \Gamma_f,  \quad (y,  z) \in \Gamma_g,  \quad (x,  y') \in \Gamma_f,  \quad (y',  z') \in \Gamma_g.
\]

Since \( f \) is a function,  one has \( y = y' = f(x) \). Since \( g \) is a function,  we deduce that \( z = z' = g(f(x)) \). Therefore \( g \circ f \) is a function,  and the equality \((g \circ f)(x) = g(f(x))\) holds for any \( x \in \text{Dom}(g \circ f) \).

Applying (1) to \( g^{-1} \) and \( f^{-1} \),  we obtain (2).
\end{proofenv} 


\begin{propositionenv}\label{proposition3.6.3}
    Let $f$ and $g$ be correspondences.
    \newline
    (1) If \( g \circ f \) is injective and \(\mathrm{Im}(f) \subseteq \mathrm{Dom}(g)\),  then \( f \) is also injective.
\newline
(2) If \( g \circ f \) is a function and \(\mathrm{Dom}(g) \subseteq \mathrm{Im}(f)\),  then \( g \) is also a function.
\end{propositionenv}




\begin{proofenv}
    \quad 
    \newline
    (1)  Let \( y \) be an element of the image of \( f \). Let \( x \) and \( x' \) be preimages of \( y \) under \( f \). Since \(\mathrm{Im}(f) \subseteq \mathrm{Dom}(g)\),  one has \( y \in \mathrm{Dom}(g) \). 
    Hence there exists \( z \in \mathscr{A}_g \) such that \((y,  z) \in \Gamma_g \). 
    We then deduce that \((x,  z)\) and \((x',  z)\) are elements of \(\Gamma_{g \circ f}\). Since \( g \circ f \) is injective,  we obtain \( x = x' \). Therefore,  \( f \) is injective.
\newline
Applying (1) to \( g^{-1} \) and \( f^{-1} \),  we obtain (2).
\end{proofenv}



\begin{propositionenv}\label{proposition3.6.4}
     Let \( f \) be a correspondence,  and \( I \) be a non-empty set.
\newline
     (1) Suppose that \( f \) is a function. For any family \((B_i)_{i \in I}\) of sets parametrised by \( I \),  one has
    \[
    f^{-1}\left(\bigcap_{i \in I} B_i\right) = \bigcap_{i \in I} f^{-1}(B_i).
    \]
    \newline
    (2) Suppose that \( f \) is injective. For any family \((A_i)_{i \in I}\) of sets parametrised by \( I \),  one has
    \[
    f\left(\bigcap_{i \in I} A_i\right) = \bigcap_{i \in I} f(A_i).
    \]
\end{propositionenv}


\begin{proofenv}
    \quad
    \newline
(1) Let \( x \) be an element of \(\bigcap_{i \in I} f^{-1}(B_i)\). For any \( i \in I \),  one has \( f(x) \in B_i \). Hence \( x \in f^{-1}(\bigcap_{i \in I} B_i) \). Therefore we obtain
\[
f^{-1}\left(\bigcap_{i \in I} B_i\right) \supseteq \bigcap_{i \in I} f^{-1}(B_i).
\]
\newline
Combining with (2) of proposition\ref{proposition3.3.4} ,  we obtain the equality
\[
f^{-1}\left(\bigcap_{i \in I} B_i\right) = \bigcap_{i \in I} f^{-1}(B_i).
\]

Applying (1) to \( f^{-1} \),  we obtain (2).
\end{proofenv} 


\section{Mapping}
\begin{definitionenv}
   A correspondence \( f \) is said to be a \textbf{mapping} if any element of \( \mathscr{D}_f \) has a unique image,  or equivalently,  \( f \) is a function and \( \mathscr{D}_f = \operatorname{Dom}(f) \). Note that \( f \) is a mapping if and only if \( f^{-1} \) is both injective and surjective.

\end{definitionenv}





\begin{notationenv}\label{notation3.7.1}
    Let \( X \) and \( Y \) be sets. We denote by \( Y^X \) the set of all mappings from \( X \) to \( Y \). An element \( u \in Y^X \) is often written in the form of a family of elements of \( Y \) parametrised by \( X \) as follows
\[
(u(x))_{x \in X}.
\]
In the case where \( X = \{1,  \ldots,  n\} \),  where \( n \) is a positive integer,  the set \( Y^{\{1,  \ldots,  n\}} \) is also denoted as \( Y^n \). An element \( u \) of \( Y^n \) is often written as
\[
(u(1),  \ldots,  u(n)).
\]
\end{notationenv}


\begin{exampleenv}
    \quad
    \begin{enumerate}
    \item Let \( X \) be a set. The identity correspondence \( \operatorname{Id}_X \) is a mapping. It is also called the \textbf{identity mapping} of \( X \).
    \item Let \( X \) and \( Y \) be sets and \( y \) be an element of \( Y \). The mapping from \( X \) to \( Y \) sending any \( x \in X \) to \( y \) is called the \textbf{constant mapping with value \( y \)}.
    \item Let $X$ be a set and $A\subseteq X$,  we define $\mathbbm{1}_A:X\rightarrow \mathbb{R}$   
    $$\mathbbm{1}_A(x):=\begin{cases}
 1,  \text{if}\space x \in A \\0, \text{if}\space x \notin A.

\end{cases}$$ 
It is called \textbf{indicator function}
\end{enumerate}
\end{exampleenv}


\begin{remark}
   Let \( f: X \to Y \) be a mapping,  \( I \) be a set.
\begin{enumerate}
    \item By (1) of Proposition\ref{proposition3.3.4},  for any family of sets \( (A_i)_{i \in I} \),  one has
    \[
    f\left(\bigcup_{i \in I} A_i\right) = \bigcup_{i \in I} f(A_i).
    \]
    By (2) of Proposition \ref{proposition3.3.4},  for any family of sets \( (B_i)_{i \in I} \),  one has
    \[
    f^{-1}\left(\bigcup_{i \in I} B_i\right) = \bigcup_{i \in I} f^{-1}(B_i).
    \]
    \item Assume that \( I \) is not empty. By (1) of Proposition \ref{proposition3.3.4},  for any family of sets \( (A_i)_{i \in I} \),  one has
    \[
    f\left(\bigcap_{i \in I} A_i\right) \subseteq \bigcap_{i \in I} f(A_i).
    \]
    By (1) of Proposition \ref{proposition3.6.4},  for any family of sets \( (B_i)_{i \in I} \),  one has
    \[
    f^{-1}\left(\bigcap_{i \in I} B_i\right) = \bigcap_{i \in I} f^{-1}(B_i).
    \]
    \item By (2) of Proposition \ref{proposition3.6.1},  for any set \( B \),  one has \( f(f^{-1}(B)) \subseteq B \). Since \( f \) is a function and \( f^{-1} \) is injective,  by (1) of Proposition \ref{proposition3.6.1} and (2) of Proposition \ref{proposition3.5.1},  for any subset \( A \) of \( X \) one has \( f^{-1}(f(A)) = A \).
\end{enumerate}

 
\end{remark}
\begin{propositionenv}\label{proposition3.7.1}
    Let \( f \) and \( g \) be mappings. Suppose that \( \operatorname{Im}(f) \subseteq \mathscr{D}_g \). Then \( g \circ f \) is also a mapping. Moreover,  for any \( x \in \mathscr{D}_f = \mathscr{D}_{g \circ f} \) one has
\[
(g \circ f)(x) = g(f(x)).
\]

\end{propositionenv}

\begin{proofenv}
   Note that \( \mathscr{D}_g = \operatorname{Dom}(g) \) since \( g \) is a mapping. Hence the statement is a direct consequence of Propositions \ref{proposition3.6.2} and \ref{proposition3.5.3}
 
\end{proofenv}

\begin{remark}\label{remark3.7.2}
    Let \( f: X \to Y \) and \( g: Y \to Z \) be mappings.
\begin{enumerate}
    \item By Proposition \ref{proposition3.5.3},  if \( f \) and \( g \) are both surjective,  so is \( g \circ f \). By Proposition \ref{proposition3.5.2},  if \( g \circ f \) is surjective,  so is \( g \).
    \item By Proposition \ref{proposition3.6.2},  if \( f \) and \( g \) are both injective,  so is \( g \circ f \). By Proposition \ref{proposition3.6.3},  if \( g \circ f \) is injective,  so is \( f \).
\end{enumerate}

\end{remark}

\section{Bijection}

\begin{definitionenv}
    Let \( f \) be a mapping,  that is,  a correspondence such that \( f^{-1} \) is injective and surjective. If \( f \) is injective and surjective,  we say that \( f \) is a \textbf{bijection},  or a \textbf{one-to-one correspondence}. Note that a correspondence is a bijection if and only if its inverse is a bijection.

\end{definitionenv}

\begin{propositionenv}\label{proposition3.8.1}
    Let \( X \) and \( Y \) be sets,  \( f \) be a correspondence from \( X \) to \( Y \). If \( f \) is a bijection,  then \( f^{-1} \circ f = \operatorname{Id}_X \) and \( f \circ f^{-1} = \operatorname{Id}_Y \). Conversely,  if there exists a correspondence \( g \) such that \( g \circ f = \operatorname{Id}_X \) and \( f \circ g = \operatorname{Id}_Y \),  then \( f \) is a bijection and \( g = f^{-1} \).

\end{propositionenv}

\begin{proofenv}
    If \( f \) is a bijection,  then \( f \) and \( f^{-1} \) are both mappings. By Proposition \ref{proposition3.7.1},  one has
\[
\forall x \in X,  \quad (f^{-1} \circ f)(x) = f^{-1}(f(x)) = x, 
\]
\[
\forall y \in Y,  \quad (f \circ f^{-1})(y) = f(f^{-1}(y)) = y.
\]
Hence \( f^{-1} \circ f = \operatorname{Id}_X \) and \( f \circ f^{-1} = \operatorname{Id}_Y \).

Assume that \( g \) is a correspondence such that \( g \circ f = \operatorname{Id}_X \) and \( f \circ g = \operatorname{Id}_Y \). Since identity correspondences are surjective mappings,  by Proposition \ref{proposition3.5.2},  we deduce from the equality \( g \circ f = \operatorname{Id}_X \) that \( g \) is surjective and \( \operatorname{Dom}(f) = X = \operatorname{Im}(g) \). Similarly,  we deduce from the equality \( f \circ g = \operatorname{Id}_Y \) that \( f \) is surjective and \( \operatorname{Dom}(g) = Y = \operatorname{Im}(f) \).

Since identity correspondences are injective,  by Proposition 3.6.5,  we deduce from \( g \circ f = \operatorname{Id}_X \) that \( f \) is injective. Similarly,  we deduce from \( f \circ g = \operatorname{Id}_Y \) that \( f \) is a function. Therefore,  \( f \) is a mapping which is injective and surjective,  namely a bijection.

Finally,  by Propositions \ref{proposition3.4.3} and \ref{proposition3.4.2},  we obtain
\[
g = g \circ \operatorname{Id}_Y = g \circ (f \circ f^{-1}) = (g \circ f) \circ f^{-1} = \operatorname{Id}_X \circ f^{-1} = f^{-1}.
\]
\end{proofenv}



\begin{propositionenv}\label{proposition3.8.2}
    Let \( f: X \to Y \) and \( g: Y \to Z \) be bijections. Then the composite correspondence \( g \circ f \) is also a bijection.

\end{propositionenv}

\begin{proofenv}
    This is a direct consequence of Propositions \ref{proposition3.7.1},  \ref{proposition3.6.2} and \ref{proposition3.5.3}

\end{proofenv}

\begin{propositionenv}\label{proposition3.8.3}
    Let \( X \) and \( Y \) be sets,  \( f \) be a correspondence from \( X \) to \( Y \),  and \( g \) be a correspondence from \( Y \) to \( X \). If \( f \circ g \) and \( g \circ f \) are bijections,  then \( f \) and \( g \) are both bijections.

\end{propositionenv}

\begin{proofenv}
    By Proposition \ref{proposition3.5.2},  \( f \) and \( g \) are surjective and are multivalued mappings. In particular, 
\[
\operatorname{Dom}(f) = X,  \quad \operatorname{Im}(f) = Y,  \quad \operatorname{Dom}(g) = Y,  \quad \operatorname{Im}(g) = X.
\]
Therefore,  by Proposition \ref{proposition3.6.3},  we deduce that \( f \) and \( g \) are injective and are functions. Hence \( f \) and \( g \) are both bijections. 

\end{proofenv}

\section{Direct product}
\label{sec:direct-product}

\begin{definitionenv}
Let $I$ be a set and $(A_i)_{i \in I}$ be a family of sets parametrised by $I$. We denote by
\[
\prod_{i \in I} A_i
\]
the set of all mappings from $I$ to $\bigcup_{i \in I} A_i$ which send any $i \in I$ to an element of $A_i$. This set is called the \textbf{direct product} of $(A_i)_{i \in I}$. Using Notation \ref{notation3.7.1} we often write an element of the direct product in the form of a family $x := (x_i)_{i \in I}$ parametrised by $I$,  where each $x_i$ is an element of $A_i$,  called the $i$-th \emph{coordinate} of $x$. In the case where $I$ is the empty set,  the union $\bigcup_{i \in I} A_i$ is empty. Therefore,  the direct product contains a unique element (identity mapping of $\varnothing$).

For each $j \in I$,  we denote by
\[
\operatorname{pr}_j : \prod_{i \in I} A_i \longrightarrow A_j
\]
the mapping which sends each element $(a_i)_{i \in I}$ of the direct product to its $j$-th coordinate $a_j$. This mapping is called the \emph{projection to the $j$-th coordinate}.
\end{definitionenv}

\begin{notationenv}
Let $n$ be a non-zero natural number. If $(A_i)_{i \in \{1,  \ldots,  n\}}$ is a family of sets parametrised by $\{1,  \ldots,  n\}$,  then the set
\[
\prod_{i \in \{1,  \ldots,  n\}} A_i
\]
is often denoted as
\[
A_1 \times \cdots \times A_n.
\]
\end{notationenv}

\begin{axiomenv}[Axiom of choice]
In this book,  we adopt the following axiom. If $I$ is a non-empty set and if $(A_i)_{i \in I}$ is a family of non-empty sets,  then the direct product $\prod_{i \in I} A_i$ is not empty.
\end{axiomenv}
\begin{propositionenv}\label{3.9.3}
\label{prop:direct-product-bijection}
Let $I$ be a set and $(A_i)_{i \in I}$ be a family of sets parametrised by $I$. For any set $X$,  the mapping
\[
\left( \prod_{i \in I} A_i \right)^X \longrightarrow \prod_{i \in I} A_i^X, 
\]
which sends $f$ to $(\operatorname{pr}_i \circ f)_{i \in I}$,  is a bijection.



\begin{center}
    \begin{tikzcd}
X \arrow[r,  "f"] \arrow[dr,  swap,  "f_j"] &\displaystyle \prod_{i \in I} A_i \arrow[d,  "\mathrm{pr}_j"] \\
& A_j
\end{tikzcd}
\end{center}
\end{propositionenv}
\begin{proofenv}
Let $(f_i)_{i \in I}$ be an element of
\[
\prod_{i \in I} A_i^X, 
\]
where each $f_i$ is a mapping from $X$ to $A_i$. Let $f : X \to \prod_{i \in I} A_i$ be the mapping which sends $x \in X$ to $(f_i(x))_{i \in I}$. By definition,  for any $i \in I$ one has
\[
\forall x \in X,  \quad \operatorname{pr}_i(f(x)) = f_i(x).
\]
Therefore the mapping is surjective.

If $f$ and $g$ are two mappings from $X$ to $\prod_{i \in I} A_i$ such that $\operatorname{pr}_i \circ f = \operatorname{pr}_i \circ g$ for any $i \in I$,  then,  for any $x \in X$ one has
\[
\forall i \in I,  \quad \operatorname{pr}_i(f(x)) = \operatorname{pr}_i(g(x)).
\]
Hence $f(x) = g(x)$ for any $x \in X$,  namely $f = g$. Therefore the mapping is injective.
\end{proofenv}

\begin{notationenv}
\label{not:direct-product-mappings}
Let $I$ be a set,  $(A_i)_{i \in I}$ be a family of sets parametrised by $I$.

Let $X$ be a set. For any $i \in I$,  let $f_i : X \to A_i$ be a mapping from $X$ to $A_i$. By Proposition~\ref{prop:direct-product-bijection} there exists a unique mapping $f : X \to \prod_{i \in I} A_i$ such that $\operatorname{pr}_i \circ f = f_i$ for any $i \in I$. By abuse of notation,  we denote by $(f_i)_{i \in I}$ this mapping.

Let $(B_i)_{i \in I}$ be a family of sets parametrised by $I$. For any $i \in I$,  let $g_i : B_i \to A_i$ be a mapping from $B_i$ to $A_i$. We denote by
\[
\prod_{i \in I} g_i : \prod_{i \in I} B_i \longrightarrow \prod_{i \in I} A_i
\]
the mapping which sends $(b_i)_{i \in I}$ to $(g_i(b_i))_{i \in I}$. In the case where $I = \{1,  \ldots,  n\}$,  where $n$ is a non-zero natural number,  the mapping $\prod_{i \in \{1,  \ldots,  n\}} g_i$ is also denoted as
\[
g_1 \times \cdots \times g_n.
\]
\end{notationenv}

\begin{propositionenv}
\label{prop:direct-product-factorization}
Let $f : X \to Y$ be a mapping.
\begin{enumerate}
    \item[(1)] If $f$ is surjective,  then there exists an injective mapping $g : Y \to X$ such that $f \circ g = \operatorname{Id}_Y$.
    \item[(2)] If $f$ is injective and $X$ is not empty,  then there exists a surjective mapping $h : Y \to X$ such that $h \circ f = \operatorname{Id}_X$.
\end{enumerate}
\end{propositionenv}

\begin{proofenv}
(1) The case where $Y = \varnothing$ is trivial since in this case $X = \varnothing$ and $f$ is the identity mapping of $\varnothing$. In the following,  we assume that $Y$ is not empty. Since $f$ is surjective,  for any $y \in Y$,  the set $f^{-1}(\{y\})$ is not empty. Hence the direct product
\[
\prod_{y \in Y} f^{-1}(\{y\})
\]
is not empty. In other words,  there exists a mapping $g$ from $Y$ to $X$ such that $f(g(y)) = y$ for any $y \in Y$,  that is $f \circ g = \operatorname{Id}_Y$. By (2) of Remark \ref{remark3.7.2} $g$ is injective.

(2) Let $x_0$ be an element of $X$. We define a mapping $h : Y \to X$ as follows:
\[
h(y) := 
\begin{cases}
f^{-1}(y),  & \text{if } y \in \operatorname{Im}(f),  \\
x_0,  & \text{else}.
\end{cases}
\]
Then,  by construction one has $h \circ f = \operatorname{Id}_X$. 
\newline
By (1) of Remark \ref{remark3.7.2} $h$ is surjective.
\end{proofenv}
\section{Restriction and Extension}
\begin{definitionenv}
    Let $f$ and $g$ be correspondence. If $\Gamma _f\subseteq \Gamma _g$,  we say that $f$ is a \textbf{restriction} of $g$ and that $g$ is an \textbf{extension} of $f$
    \newline
    Let $X$ anf $Y$ be sets,  $h$ be a correspondence from $X$ to $Y$ , and $A$ be a subset of $X$.Denote by $h|_A$ the correspondence from $A$ to $Y$ such that$$\Gamma_{h|_A}=\Gamma_h\bigcap (A\times Y).$$ We call it the \textbf{restriction of $h$ to $A$}
\end{definitionenv}
\chapter{Binary Relations }
$\dagger$This chapter was first written in pre-course,  then added some sections in make-up session, which titled "Ordering".Some sections have the same knowledge.It's a bit mess.
\section{Generalities}
\begin{definitionenv}
    Let $X$ be a set , we call \textbf{binary relation} on $X$ any correspondence from $X$ to $X$ .If $R$ is a binary relation on $X$ , for any $(x, y)\in X\times X $ we denote by $x R y $ the statement $(x, y)\in \Gamma_R$.
\end{definitionenv}
\begin{exampleenv}
    We denote by" $=$" the correspondence $\mathrm{Id}_X$.
\end{exampleenv}
\begin{definitionenv}
    If $R$ is a binary relation on $X$,  we denote by $\cancel{R} $ the binary relation such that $$x \cancel{R}  y \Leftrightarrow (x, y)\notin \Gamma_R.$$
\end{definitionenv}
\section{Equivalent Relation}\label{4.2}
\textit{Section \ref{5.5}:Quotient,  will use this concept.}
\begin{definitionenv}
    Let $X$ be a set and $R$ a binary relation on $X$.
    \newline
    (1) If $\forall x \in X , xRx$,  we say that $R$ is \textbf{reflexive}.
    \newline
    (2) If $\forall (x, y) \in X\times X, xRy\Rightarrow yRx$, we say that $R$ is \textbf{symmetric}.
    \newline
    (3) If for all $x, y, z$ of $X$,  $xRy\wedge yRz\Rightarrow xRz$, we say that $R$ is \textbf{transitive}.
    \newline
    (4) If $R$ is reflexive, symmetric and transitive,  we say that $R$ is an \textbf{equivalent relation}.

\end{definitionenv}
\begin{definitionenv}
    Let $\thicksim$ be an equivalent relation on $X$.For any $x\in X$,  we call the set $$[x]:=\{y\in X|y\thicksim x\}$$ the equivalent class of $x$ under $\thicksim$, we denote by $X/\thicksim$ the set $\{[x]|x\in X\}$ of all equivalent class.It is a subset of $\wp (X)$.Moreover,  since $\forall x\in X, x\in [x] $,  one has $$X=\bigcup_{A\in X/\thicksim}A.$$
\end{definitionenv}
\begin{propositionenv}
    $\forall (x, y)\in X\times X$,  either $[x]=[y]$ or $[x]\cap [y]=\varnothing$.
\end{propositionenv}
\begin{definitionenv}
    The mapping $\pi :X\rightarrow X/\thicksim$ is called the \textbf{projection mapping} of $\thicksim$.
\end{definitionenv}
\begin{propositionenv}[Theorem \ref{5.5.5}]\label{4.2.5}
    $f:X\rightarrow Y$ be a mapping, if $\forall (x, y)\in X\times X, x\thicksim y\Rightarrow f(x)=f(y)$, then there exists a unique mapping$$\tilde{f}:X/\thicksim\rightarrow Y,  [x]\mapsto f(x), $$ such that $$\tilde{f}\circ \pi =f.$$


\begin{center}
\begin{tikzcd}
    X\arrow[d, swap, "\pi"]\arrow[r, "f"]& Y\\
    X/\sim \arrow[ur, swap, "\tilde{f}"]
\end{tikzcd}
\end{center}
\end{propositionenv}

\section{Partial Order}
\begin{definitionenv}
    If 
    \newline
    (1) $R$ is reflexive.
    \newline
    (2) $R$ is antisymmetric $\forall (x, y)\in X^2, xRy $ and $yRx$ then $x=y$.
    \newline
    (3) $R$ is transitive.
    \newline
    then we say that $R$ is a \textbf{partial order} on $X$ and $(X, R) $ is a \textbf{partially ordered set}.If in addition , $\forall (x, y)\in X , xRy$ or $yRx$,  we say that $R$ is a \textbf{total order} and $(X, R)$ is totally ordered set. 
\end{definitionenv}
\begin{exampleenv}
    $(\RR , \le)$ is a totally ordered set.$(\NN , |)$ is a partially ordered set.  
\end{exampleenv}
\begin{definitionenv}
    Let $(X, \underline{R})$ be a partially ordered set. We denote by $R$ the binary relation on $X$ defined as:$$xRy \Leftrightarrow x\underline{R}y \wedge x\not=y, $$ we call $R$ the \textbf{strict partial order}(not a partial order) associated with $\underline{R}$.
\end{definitionenv}
\begin{exampleenv}
    \quad
    \newline
    (1) $<$ on $\RR$.
    \newline
    (2) $\subset $on$ \wp (X)$.
\end{exampleenv}
\begin{propositionenv}
    $R$ is the strict partial order associated with some partial order iff.the following condition are satisfied:
    \newline
    (1) Irreflexivity $\forall x\in X, x \cancel{R}x$.
    \newline
    (2) Asymmetry.$\forall (x, y)\in X^2, xRy\Rightarrow y\cancel{R}x$.
    \newline
    (3) Transitivity.
\end{propositionenv}
\begin{proofenv}
    "$\Rightarrow$": easy.
    \newline
    "$\Leftarrow$":Suppose that $R$ is a binary relation satisfying $(1)\thicksim(3)$. Define another binary relation $\underline{R}$ on $X$ as:
    $$x\underline{R}y\Leftrightarrow xRy \vee x=y.$$
    We claim that $xRy\Leftrightarrow x\underline{R}y \wedge x\not= y$:
    \newline
    Suppose that $xRy$,  then by definition,  $x\underline{R}y$. By the irreflexivity,  $x\not=y$.
    \newline
    Conversely,  if $x\underline{R}y\wedge x\not=y$, then $xRy$ should be true.
\end{proofenv}
\section{Monotonic Functions}
\begin{definitionenv}
    Let$(I, \le)$ and $(X, \le)$ be partially ordered sets, and $f$ be a function from $I$
 to $X$.
 \newline
 (1) If $\forall(x, y)\in \mathrm{Dom}(f)^2, x<y \Rightarrow f(x)\le f(y)$ we say that f is increasing.
 \newline
 (2) If $\forall (x, y)\in \mathrm{Dom}(f)^2, x<y \Rightarrow f(x)< f(y)$,  we say that $f$ is strictly increasing.
 \newline
  (3) If $\forall (x, y)\in \mathrm{Dom}(f)^2, x<y \Rightarrow f(x)\geq  f(y)$,  we say that $f$ is decreasing.
\newline
 (4) If $\forall (x, y)\in \mathrm{Dom}(f)^2, x<y \Rightarrow f(x)> f(y)$,  we say that $f$ is strictly decreasing.
 \newline
increasing and decreasing functions are called \textbf{monotonic function},  strictly increasing and decreasing functions are called \textbf{strictly monotonic function}.
\end{definitionenv}
\begin{propositionenv}
    Let $f, g$ be functions between partially ordered sets.
    \newline
    (1) If both $f$ and $g$ are increasing or both $f$ and $g$ are decreasing, then $g\circ f $ is increasing.
    \newline
    (2) If one function between $f$ and $g$ is increasing while the order is decreasing, then $g\circ f $ is decreasing.
\end{propositionenv}
\begin{propositionenv}
    Let $f$ be a function between partially ordered set. If $f$ is monotonic and injective,  then $f$ is strictly monotonic.
\end{propositionenv}
\begin{propositionenv}
    Let $I$ be a totally ordered set, $X$ be a partially ordered set , and $f$ be a function from $I$ to $X$. If $f$ is strictly monotonic,  then $f$ is injective.
\end{propositionenv}
\begin{proofenv}
    Let $(x, y)\in \mathrm{Dom}(f)^2$, such that $f(x)=f(y)$.Since $I$ is totally ordered,  then $x<y$ or $x>y$ or $x=y$. Suppose that $f$ is strictly increasing. If $x<y $, then $f(x)<f(y)$,  contradiction. If $x>y $, then $f(x)>f(y)$,  contradiction.
\end{proofenv}
\begin{propositionenv}
    Let $X$ be a totally ordered set,  $Y$ be an partially ordered set,  $f$ be an injective function from $X$ to $Y$. If $f$ is monotonic,  then $f^{-1}$ is also monotonic,  and they have the same monotonic direction.
\end{propositionenv}
\begin{proofenv}
    We may suppose that $f$ is increasing.
    Let $(a, b)\in \mathrm{Dom}(f^{-1})^2=\mathrm{Im}(f)^2, a<b$. Since $f^{-1}$ is a injective function,  $f^{-1}(a)\not =f^{-1}(b)$,  so either $f^{-1}(a)<f^{-1}(b)$ or $f^{-1}(a)>f^{-1}(b)$. If $$f^{-1}(a)>f^{-1}(b), a=f(f^{-1}(a))>f^{-1}(b)=b, $$ contradiction. Therefore,  $f^{-1}(a)<f^{-1}(b)$. Hence $f^{-1}$ is strictly increasing.
\end{proofenv}
\section{Bounds}
\begin{definitionenv}
    Let $(X, \le)$ be a partially ordered set , let $A$ be a subset of $X$.
    \newline
    (1) Let $M\in X$. If $\forall a \in A , a\le M$,  we say that $M$ is an upper bound of $A$. 
    \newline
    (2) Let $m\in X$. If $\forall a \in A , m\le a$,  we say that $m$ is an lower bound of $A$. 
    \newline
    Denote by $A^u$ the set of upper bounds of $A$ in $(X, \le)$.
    \newline
     Denote by $A^l$ the set of lower bounds of $A$ in $(X, \le)$.

\end{definitionenv}
\begin{exampleenv}
    $\Omega=\{1, 2, 3\}, X=\wp (\Omega).(X, \subseteq)$ forms a partially ordered set.Let $A=\{\{1\}, \{2\}, \{1, 2\}\}, A^u=\{\{1, 2\}, \{1, 2, 3\}\}, A^l=\{\varnothing\}$.
\end{exampleenv}
\begin{definitionenv}
    Let $(X, \le)$ be a partially ordered set , let $A$ be a subset of $X$.
    \newline
    (1)If $M \in A$ is an upper bound of $A$,  we say that $M$ is the \textbf{greatest element} of $A$,  denote as $\mathrm{max}_\le A$.
    \newline
    (2)If $m \in A$ is an lower bound of $A$,  we say that $m$ is the \textbf{least element} of $A$,  denote as $\mathrm{min}_\le A$.
    \newline
    If there is not ambiguity on $\le$, we can also write as $\mathrm{max}A, \mathrm{min}A$.
\end{definitionenv}
\begin{definitionenv}
    $A\subseteq Y\subseteq X$, let $A_Y^\mathrm{u}:=\{y\in Y|\forall a\in A, a\le y\}$ be the set of upper bounds of $A$ in $Y$.If $A_Y^\mathrm{u}$ has a least element , we call it the \textbf{supremum} of $A$ in $Y$,  denoted as $\mathrm{sup}_{(Y, \le)}A$,  if there's no ambiguity on $\le$ we can also write as $\mathrm{sup}_{Y}A$. Resp. \textbf{infimum}.
\end{definitionenv}
\begin{notationenv}\label{notation4.5.1}
    
         Let $(X, \le)$ be a partially ordered set , $f:I\rightarrow X$ be a function.$$\mathrm{max}f(I), \mathrm{min}f(I), \mathrm{sup}f(I), \mathrm{inf}f(I)$$ are written as $$\mathrm{max}f, \mathrm{min}f, \mathrm{sup}f, \mathrm{inf}f.$$
         Let $(X, \le)$ be a partially ordered set , and $(x_i)_{i\in I}\in X^I$, $$\mathrm{max}\{x_i|i\in I\}, \mathrm{min}\{x_i|i\in I\}, \mathrm{sup}\{x_i|i\in I\}, \mathrm{inf}\{x_i|i\in I\}$$ are denoted as $$\max _{i\in I}x_i, \min _{i\in I}x_i, \sup _{i\in I}x_i, \inf _{i\in I}x_i.$$
    
\end{notationenv}
\begin{propositionenv}\label{proposition4.5.1}
    \quad
    \newline
    Let $(X, \le)$ be a partially ordered set $(A, Z, Y)\in \wp (X)^3, A\subseteq Z\subseteq Y$.
    \newline
   (1) If $\max A$ exists, then it is also the supremum of $A$ in $(Y, \le)$.So as infimum
    \newline
    (2) If $\sup_{(Y, \le)}A$ exists and belongs to $Z$ , then it is also the supremum of $A$ in $(Z, \le)$. So as infimum.
\end{propositionenv}
\begin{proofenv}
    \quad \newline
   (1) By definition, $\max A $ is an upper bound of $A$. Since $A\subseteq Y,  \max A \in Y$, Hence $\max A\in A_Y^\mathrm{u}$.Let $M\in A_Y^\mathrm{u} $.Since $M$ is upper bound of $A$ and $\max A\in A, \max A\le M$ .Then $\max A=\min A_Y^\mathrm{u}$.
\newline
(2) Since $Z\subseteq Y, A_Z^\mathrm{u}\subseteq A_Y^\mathrm{u}$.For any $M\in A_Z^\mathrm{u}$, one has $\sup _{Y, \le}A\le M$.If $\sup_{(Y, \le)}A\in Z$, then $\sup_{(Y, \le)}A\in A_Z^\mathrm{u}$.Hence $\sup _{(Y, \le)}A=\min A_Z^\mathrm{u}$.
\end{proofenv}
\begin{propositionenv}\label{proposition4.5.2}
    \quad
    \newline
    Let $(X, \le )$ be a partially ordered set , $(A, B, Y)\in \wp (X)^3, A\subseteq B\subseteq Y$
    \newline
    (1) If $\sup_{(Y, \le)}A$ and $\sup_{(Y, \le)}B$ exist,  then $$\sup_{(Y, \le)}A \le \sup_{(Y, \le)}B.$$
    \newline
    (2)If $\inf_{(Y, \le)}A$ and $\inf_{(Y, \le)}B$ exist,  then $$\inf_{(Y, \le)}B \le \inf_{(Y, \le)}A.$$
    
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) $\forall x\in A$, since $A\subseteq B, x\in B \le \sup B$, by definition,  $\sup B$ is an upper bound of $A$, $\sup B\in A_Y^\mathrm{u}$.$\sup A$ is the least in $A_Y^\mathrm{u}$. Hence, $\sup_{(Y, \le)}A \le \sup_{(Y, \le)}B$.
\end{proofenv}
\begin{propositionenv}\label{proposition4.5.3}
    Let $(X, \le )$ be a partially ordered set , $f, g$ be elements of $X^I$ where $I$ is a set .Suppose that , $\forall i\in  I , f(i)\le g(i)$
 \newline
 (1) If $\sup f, \sup g$ exist , then $\sup f\le\sup g$.
 \newline
 (2) So as infimum.
\end{propositionenv}
\begin{proofenv}
    $\forall t\in I, f(t)\le g(t)\le \sup g$, hence $\sup g$ is an upper bound of $f$.Since $\sup f $ is a the least upper bound of $f(i)$, $\sup f\le\sup g$.
\end{proofenv}
\begin{propositionenv}
    Let $I$ be a totally ordered set $J\subseteq I $, and $f:I\rightarrow X $ be a mapping. Assume that $J$ does not have any upper bound in $I$.
    \newline 
    (1) If $f$ is increasing,  then $f(I)^\mathrm{u}=f(J)^\mathrm{u}$.
    \newline
    (2) If $f$ is decreasing,  then $f(I)^\mathrm{l}=f(J)^\mathrm{l}$.
\end{propositionenv}
\begin{proofenv}
    \quad 
    \newline
    (1) $f(J)\subseteq f(I)$ Any upper bound of $f(I)$ is also an upper bound of $f(J)$, hence $f(I)^\mathrm{u}\subseteq f(J)^\mathrm{u}$.Let $M\in f(J)^\mathrm{u}$, for any $i\in I, \exists j\in J, i<j$. Hence $f(i)\le f(j)\le M$ .So $M\in f(I)^\mathrm{u}$, $f(J)^\mathrm{u}\subseteq f(I)^\mathrm{u}$.Therefore, $f(I)^\mathrm{u}=f(J)^\mathrm{u}$.
\end{proofenv}
\begin{propositionenv}
    Let $(X, \le)$ be a partially ordered set , $Y\subseteq X, I$ be a set,  and $(A_i)_{i\in I}\in \wp (Y)^I$ .Let $A=\bigcup _{i\in I}A_i$
    \newline
    (1) Suppose that , $\forall i \in I, A_i$ has a supremum $y_i$ in $(Y, \le)$ and $\{y_i|i\in I\}$ has a supremum in $(Y, \le)$. Then $A$ has a supremum in $(Y, \le )$ and $$\sup_{(Y, \le)}A=\sup_{(Y, \le)}\{y_i|i\in I\}.$$
    \newline
    (2) Resp. inf.
\end{propositionenv}
\begin{proofenv}
    Let $y=\sup_{(Y, \le)}\{y_i|i\in I\}, \forall a\in A , \exists i\in I, a\in A_i$. Hence $a\le y_i\le y$. Thus $y $ is an upper bound of $A$ in $Y$.Let $M\in A_Y^\mathrm{u}, \forall i \in I, M\in (A_i)_Y^\mathrm{u}$,  So $y_i \le M $ We then deduce that $y\le M$.
\end{proofenv}
\begin{propositionenv}
    Let $(X, \le)$ be a partially ordered set , $Y\subseteq X$.$$\varnothing_Y^\mathrm{u}=\varnothing_Y^\mathrm{l}=Y.$$
\end{propositionenv}


\section{Intervals}

\begin{definitionenv}
    Let $(X, \le)$ be a partially ordered set.$\forall (a, b)\in X^2$,  let 
    $$[a, b]:=\{x\in X|a\le x\le b \}, $$
     $$\interval[open right]{a}{b}:=\{x\in X|a\le x <b\}.$$ 
     We say that a subset is a \textbf{interval} if $\forall (a, b)\in I^2 , [a, b]\subseteq I$.
\end{definitionenv}
\begin{propositionenv}
    Let $(X, \le)$ be a partially ordered set,  let $\Lambda$ be a non-empty set and $(I_\lambda)_{\lambda\in \Lambda}$ be a family of interval in $X$,  then 
    \newline
    (1) $\displaystyle I:=\bigcap _{\lambda\in \Lambda}I_\lambda$ is an intervals.
    \newline
    (2) If $\displaystyle \bigcap _{\lambda\in \Lambda}I_\lambda\not=\varnothing $, then $\displaystyle J:=\bigcup_{\lambda\in \Lambda}I_\lambda$ is an interval.
\end{propositionenv}
\begin{proofenv}
    \quad 
    \newline
    (2):Let $x\in I=\bigcap _{\lambda\in \Lambda}I_\lambda$, let $(a, b)\in J^2, \exists (\alpha, \beta )\in \Lambda^2, \alpha\in I_\alpha, \beta\in I_\beta$.We will show that $[a, b]\subseteq I_\alpha\cup I_\beta$. If $a\not\le b $,  then $[a, b]\not =\varnothing\subseteq I_\alpha\cup I_\beta $. We may assume $a\le b $.
    \newline 
    If $b\le x$,  then $[a, b]\subseteq [a, x]\subseteq I_\alpha$,  if $x\le a $,  then $[a, b]\subseteq [x, b]\subseteq I_\beta$. Suppose that $a<x<b$,  one has $[a, b]=[a, x]\cup[x, b]$ and so on ,  $[a, b]=[a, x]\cup[x, b]\subseteq I_\alpha\cup I_\beta \subseteq J$.

\end{proofenv}
\begin{definitionenv}
    Let $(X, \le )$ be a partially ordered set and $I $ be a non-empty interval in $X$. 
    \newline
    If $\sup I$ exists,  we call it the right endpoint of $I$.
    \newline
    If $\inf I$ exists,  we call it the left endpoint of $I$.
\end{definitionenv}
\begin{propositionenv}
   Let $(X, \le )$ be a totally ordered set and $I $ be a interval in $X$ 
   \newline
   (1) Suppose that $I$ has a supremum $b$ in $X, \forall x\in I, \interval[open right]{x}{b}\subseteq I$.
   \newline
   (2) Suppose that $I$ has a infimum $b$ in $X, \forall x\in I, \interval[open left]{b}{x}\subseteq I$.
\end{propositionenv}
\begin{remark}
    totally ordered set condition is used to prove (2)
\end{remark}
\begin{propositionenv}
   Let $(X, \le )$ be a totally ordered set and $I $ be a  non-empty interval in $X$  .Assume that $I$ has an infimum $a$ and a supremum $b$ in $X$. Then $I$ is one of the following sets:$[a, b], \interval[open right]{a}{b}, \interval[open left]{a}{b}, \interval[open]{a}{b}$.
\end{propositionenv}
\begin{proofenv}
    $\forall x\in I , a\le x\le b $, hence $I\subseteq [a, b]$.
    \newline
    (i) if $\{a, b\}\in I$,  then $I=[a, b]$.
    \newline
    (ii) if $a\in I,  b\notin I, I\subseteq\interval[open right]{a}{b}=[a, b]\backslash\{b\}$. Let $x\in \interval[open right]{a}{b}$,  since $x<b$,  $x$ is not an upper bound of $I$. Hence $\exists y\in I,  x<y$. Note that $[a, y]\subseteq I$,  hence $x\in I$,  therefore $\interval[open right]{a}{b}\subseteq I$. Similarly ,  is $b\in I, a\notin I$,  then $\interval[open left]{a}{b}=I$.
    \newline
    (iii) if $\{a, b\}\cap I=\varnothing$,  then $I\subseteq\interval[open]{a}{b} .\forall x\in \interval[open]{a}{b}, \exists s, t\in I, s<x<t$ Hence $x\in [s, t]\subseteq I$. Therefore $\interval[open]{a}{b}=I$.

\end{proofenv}
\begin{definitionenv}[Dense]
    Let $(X, \le )$ be a totally ordered set,  if $\forall (x, z)\in X^2, x<z\Rightarrow \interval[open]{x}{z}\not=\varnothing$ then we say that $(X, \le)$ is \textbf{dense}.
\end{definitionenv}
\begin{propositionenv}
    Let $(X, \le )$ be a totally ordered set that is dense,  $(a, b)\in X^2, a<b$. If $I$ is one of the intervals $[a, b], \interval[open right]{a}{b}\dots$,  then $a=\inf I, b=\sup I$.
\end{propositionenv}
\begin{proofenv}
    By definition,  $b$ is an upper bound of $I$,  since $(X, \le)$ is a totally ordered set, if $b$ is not the supremum of $I$ ,  $\exists M\in I^\mathrm{u}$ such that $M<b$.
    Let $x\in I$, one has $x\le M <b$.Since $\interval[open right]{x}{b}\subseteq I, M\in I$,  hence $M=\max I$. Since $X$ is dense , pick $M'\in \interval[open]{M}{b}$.Since $M\in I, b=\sup I, \interval[open right]{M}{b}\subseteq I$. Hence $M'\in I, M'\le M$.This contradicts $M<M'$.
\end{proofenv}


\section{Well-ordered Set}
\begin{definitionenv}
    Let $(X, \le)$ be a partially ordered set.If $\forall A\in \wp(X), A\not=\varnothing\Rightarrow A$ has a least element, we say that $(X, \le)$ is a \textbf{well-ordered set}.
\end{definitionenv}
\begin{axiomenv}
    $(\NN, \le)$ is a well-ordered set.
\end{axiomenv}
\begin{propositionenv}
    If $(X, \le)$ is a well-ordered set,  then it is a totally ordered set.
\end{propositionenv}
\begin{propositionenv}
    $(X, \le)$ is a well-ordered set, $Y\subseteq X$,  then $(Y, \le)$ is a well-ordered set. 
\end{propositionenv}
\begin{theoremenv}
    Let $(X, \le)$ be a well-ordered set .Let $P(\cdot)$ be a condition on $X$. If 
    $$\forall x\in X, (\forall y\in X_{<x} , P(y))\Rightarrow P(x), $$
    then $\forall x\in X, P(x)$.
\end{theoremenv}
\begin{remark}
    Suppose that $X\not=\varnothing$, There is a least element $m$ of $X$.The statement
    $$\forall x\in X, (\forall y\in X_{<m} , P(m))\Rightarrow P(x) \text{ and } P(m) \text{ have the same truth value.}$$
\end{remark}
\begin{proofenv}
    Let $A=\{x\in X|\neg P(x)\}$. If $A\not=\varnothing, \exists x\in A$ which is the least element of $A$. By definition,  $(\forall y\in X_{<x} , P(y))$ is true. It contradicts to .
\end{proofenv}
\begin{remark}
    We add a formal element $+\infty$ to $\mathbb{N}$ and require $\forall n\in \NN,  n<+\infty$
\end{remark}
Fact:$\NN \cup \{+\infty\}$ is a well-ordered set. Let $P(\cdot)$ be a condition on $\NN \cup \{+\infty\}$. We need to check:
\begin{enumerate}
    \item $P(0)$.
    \item $\forall n\in \NN_{\leq 1}, P(0)\wedge\dots\wedge P(n-1)\Rightarrow P(n)$.
    \item $(\forall n \in \NN, P(n))\Rightarrow P(+\infty)$.
\end{enumerate}
\section{Order-completeness}
\begin{definitionenv}
    Let $(X, \le)$ be a partially ordered set .If any subset of $X$ has a supremum in $X$,  we say that  $(X,  \le)$ is \textbf{order-complete}. Note that an order-complete
partially ordered set is never empty.
\end{definitionenv}
\begin{axiomenv}
    Let $\overline{\mathbb{R}}:=\RR\cup\{-\infty, +\infty\}$,  where $-\infty, +\infty$ are distinct formal elements that do not belongs to $\RR$.If we equip $\overline{\RR}$ with the total order extending that of $\RR$ such that 
    $$\forall x\in \RR, -\infty<x<+\infty, $$ 
    then $(\overline{\RR}, \le)$ is order complete.
\end{axiomenv}
\begin{exampleenv}
    Let $\Omega$ be a set , $X=\wp(\Omega)$. Then $(X, \subseteq)$ is  order complete.
\end{exampleenv}
    \begin{proofenv}
        Let $Y\subseteq X$.Then 
        $$Y^{\mathrm{u}}=\{B\in\wp(\Omega)|\forall A\in Y, A\subseteq B\}.$$
        $\displaystyle \bigcup_{A\in Y}A$ is the least upper bound of $Y$ in $X$. So $\sup (Y)=\bigcup_{A\in Y}A$.
    \end{proofenv}
\begin{propositionenv}
    Let $(X, \le)$ be an order complete partially ordered set. Any subset of $X$ has an infimum in $X$.
\end{propositionenv}
\begin{proofenv}
    Let $A\subseteq X, m:=\sup A^\mathrm{l}$. We prove that $m\in A^\mathrm{l}$.
    \newline
    Let $x\in A, \forall y \in A^\mathrm{l}, y\le x $,  so $x\in (A^\mathrm{l})^\mathrm{u}$. Hence $m\le x$.
\end{proofenv}
Here Huayi gave a notation which have been given in Notation\ref{notation4.5.1}, then came to Proposition\ref{proposition4.5.1} and the following.
\begin{definitionenv}
    Let $X$ be a set and $f:X\rightarrow X$ be a mapping. If $x\in X $ is such that $f(x)=x$,  then we say that $x$ is a fixed point of $f$. 
\end{definitionenv}
\begin{theoremenv}[Knaster-Tarski fixed point]
    \quad
    \newline
    Let $(X, \le)$ be an order complete partially ordered set , $f:X\rightarrow X$ be an increasing mapping.Let 
    $$F=\{x\in X|f(x)=x\}, $$
    then $(F, \le)$ is order complete. In particular $F\not=\varnothing$.
    
\end{theoremenv}
\begin{proofenv}
    Let $A$ be a subset of $F$. We consider 
    $$S_A:=\{y\in A^\mathrm{u}|f(y)\le y\}.$$
    Let $m:=\inf S_A, \forall a\in A, a$ is a lower bound of $S_A$.So $a\le m$.So $m\in A^\mathrm{u}, \sup A\le m$. For any $y\in S_A$, one has $m\le y$. Since $f$ is increasing,  $f(m)\le f(y)\le y$. So $f(m)$ is a lower bound of $S_A$,  which leads to $f(m)\le m$. That means $m\in S_A$. Hence $m=\min S_A$. For any $x\in A, x=f(x)\le f(m)$. So $f(m)\in A^\mathrm{u}$. Moreover,  since $f(m)\le m , f(f(m))\le f(m)$. So $f(m)$ is an element of $S_A$,  which leads to $m\le f(m)$. Hence $m\in F$. Therefore,  $m=\sup_{(F, \le)}A$.
\end{proofenv}
\begin{definitionenv}
    Let $X, Y$ be sets.If there exists a bijection from $X$ to $Y$,  we say that $X$ and $Y$ are \textbf{equipotent}.
\end{definitionenv}



\begin{figure}[htbp]
    \begin{center}
        

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt, y=0.75pt, yscale=-0.7, xscale=0.7]
%uncomment if require: \path (0, 300); %set diagram left start at 0,  and has height of 300

%Shape: Free Drawing [id:dp6757315742756932] 
\draw  [line width=3] [line join = round][line cap = round] (432.84, 94.51) .. controls (423.91, 85.57) and (417.94, 75.44) .. (407.84, 67.51) .. controls (389.3, 52.94) and (354.43, 70.01) .. (348.84, 90.51) .. controls (346.27, 99.93) and (347.79, 123.7) .. (347.84, 125.51) .. controls (348, 130.68) and (352.28, 138.07) .. (347.84, 142.51) .. controls (341.76, 148.59) and (334.14, 152.95) .. (326.84, 157.51) .. controls (304.57, 171.42) and (288.63, 164.83) .. (285.84, 195.51) .. controls (285.19, 202.64) and (284.78, 209.79) .. (290.84, 214.51) .. controls (326.9, 242.55) and (362.17, 210.37) .. (399.84, 204.51) .. controls (410.54, 202.84) and (420.13, 203.46) .. (429.84, 207.51) .. controls (440.63, 212) and (449.78, 218.68) .. (461.84, 213.51) .. controls (471.26, 209.47) and (480.29, 197.67) .. (485.84, 190.51) .. controls (501.13, 170.76) and (511.94, 148.7) .. (509.84, 123.51) .. controls (509.11, 114.78) and (507.62, 105.16) .. (499.84, 99.51) .. controls (481.9, 86.46) and (463.43, 95.99) .. (445.84, 99.51) .. controls (438.37, 101) and (433.82, 93.51) .. (432.84, 93.51) ;
%Shape: Free Drawing [id:dp6453905503802386] 
\draw  [line width=3] [line join = round][line cap = round] (350.84, 136.51) .. controls (383.1, 136.51) and (418.59, 137.55) .. (443.84, 160.51) .. controls (450.93, 166.95) and (452.72, 176.06) .. (457.84, 183.51) .. controls (464.34, 192.96) and (469.88, 195.54) .. (474.84, 200.51) .. controls (475.08, 200.74) and (474.84, 201.17) .. (474.84, 201.51) ;
%Shape: Free Drawing [id:dp051216671495937005] 
\draw  [line width=3] [line join = round][line cap = round] (53, 95) .. controls (33.78, 95) and (22.79, 107.25) .. (25, 126) .. controls (25.84, 133.12) and (32.95, 139.65) .. (34, 147) .. controls (36.15, 162.06) and (24.12, 174.09) .. (40, 186) .. controls (66.02, 205.52) and (85.93, 181.84) .. (111, 175) .. controls (132.29, 169.19) and (157.62, 177.86) .. (179, 180) .. controls (215.66, 183.67) and (260.62, 151.15) .. (267, 115) .. controls (270.77, 93.64) and (246.36, 84.44) .. (230, 87) .. controls (207.33, 90.54) and (198.2, 98.1) .. (174, 97) .. controls (164.41, 96.56) and (156.03, 90.25) .. (147, 87) .. controls (135.47, 82.85) and (118.31, 83.57) .. (107, 84) .. controls (89, 84.69) and (73.05, 95) .. (52, 95) ;
%Shape: Free Drawing [id:dp8803974418610332] 
\draw  [line width=3] [line join = round][line cap = round] (117.26, 84.32) .. controls (117.26, 103.03) and (141.03, 112.34) .. (152.26, 122.32) .. controls (165.02, 133.66) and (168.29, 145.45) .. (172.26, 159.32) .. controls (174.86, 168.42) and (181.26, 168.66) .. (181.26, 178.32) ;
%Curve Lines [id:da948169565487457] 
\draw    (207, 114) .. controls (246.6, 84.3) and (400.4, 66) .. (410.98, 120.64) ;
\draw [shift={(411.26, 122.32)},  rotate = 262.22] [color={rgb,  255:red,  0; green,  0; blue,  0 }  ][line width=0.75]    (10.93, -3.29) .. controls (6.95, -1.4) and (3.31, -0.3) .. (0, 0) .. controls (3.31, 0.3) and (6.95, 1.4) .. (10.93, 3.29)   ;
%Curve Lines [id:da6547582412802545] 
\draw    (358.26, 201.32) .. controls (278.66, 284.9) and (173.32, 233.83) .. (108.97, 158.14) ;
\draw [shift={(108, 157)},  rotate = 49.9] [color={rgb,  255:red,  0; green,  0; blue,  0 }  ][line width=0.75]    (10.93, -3.29) .. controls (6.95, -1.4) and (3.31, -0.3) .. (0, 0) .. controls (3.31, 0.3) and (6.95, 1.4) .. (10.93, 3.29)   ;

% Text Node
\draw (200, 124.4) node [anchor=north west][inner sep=0.75pt]    {$C$};
% Text Node
\draw (314, 61.4) node [anchor=north west][inner sep=0.75pt]    {$f$};
% Text Node
\draw (438, 123.4) node [anchor=north west][inner sep=0.75pt]    {$f( C)$};
% Text Node
\draw (75, 124.4) node [anchor=north west][inner sep=0.75pt]    {$X\backslash C$};
% Text Node
\draw (350, 168.4) node [anchor=north west][inner sep=0.75pt]    {$Y\backslash f( C)$};
% Text Node
\draw (197, 236.4) node [anchor=north west][inner sep=0.75pt]    {$g$};


\end{tikzpicture}
    \end{center}
\end{figure}



\begin{theoremenv}[Cantor-Bernstein]
    Let $X$ and $Y$ be sets. Assume that there exists injective mappings $f:X\rightarrow Y$ and $g:Y\rightarrow X$. Then $X$ and $Y$ are equipotent.
\end{theoremenv}
\begin{proofenv}
    Consider $\Phi:\wp(X)\rightarrow\wp(X), A\mapsto X\backslash g(Y\backslash f(A))$. If $(A, B)\in \wp(X)^2$ such that $A\subseteq B$,  then $f(A)\subseteq f(B), Y\backslash f(A)\supseteq  Y\backslash f(B), g(Y\backslash f(A))\supseteq g(Y\backslash f(A)) , \Phi(A)\subseteq\Phi(B)$. So $\Phi$ is increasing. By Knaster-Tarski theorem,  $\exists C\in \wp(X), C=\Phi(C)$. Then $h:X\rightarrow Y, h(x):=\left\{\begin{matrix}
 f(x), x\in C\\
g^{-1}(x), x\in X\backslash C

\end{matrix}\right.$
is a bijection.
\end{proofenv}

\begin{lemmaenv}
    Let $(X, \le)$ is a partially ordered set.
    \newline
    (1) Let $(A, B)\in \wp(X)^2$,  if $A\subseteq B$,  then $B^\mathrm{u}\subseteq A^\mathrm{u}, B^\mathrm{l}\subseteq A^\mathrm{l}$.
    \newline
    (2) $\forall A\in \wp(X), A\subseteq (A^\mathrm{u})^\mathrm{l}\cap (A^\mathrm{l})^\mathrm{u}$.
\end{lemmaenv}

\begin{theoremenv}[Dedekind-MacNeille]
    \quad
    \newline
    Let $(X, \le )$ be a partially ordered set.Let $\hat{X}:=\{A\in \wp (X)|(A^\mathrm{u})^\mathrm{l}=A\}$
\newline
(1) $(\hat{X}, \subseteq)$ is order complete.
\newline
(2) $\forall A\in \wp(X), A^\mathrm{l}\in \hat{X}$.
\newline
(3) $X\rightarrow \hat{X}, x\mapsto \{x\}^\mathrm{l}$ is strictly increasing.
\newline
(4) $\forall A\in \hat{X}$ one has $A=\bigcup_{x\in A}\{x\}^\mathrm{l}=\bigcup_{x\in A}\hat{x}$.In particular,  
$$A=\sup_{(\hat{X}, \subseteq)}\{\hat{x}|x\in A\}.$$
(5) Let $A\in \hat{X}$.If $A^\mathrm{u}=\varnothing$,  then $A=X$. If $A^\mathrm{u}\not=\varnothing$, then 
$$A=\bigcap_{x\in A^\mathrm{u}}\hat{x}=\inf_{(\hat{X}, \subseteq)}\{\hat{x}|x\in A^\mathrm{u}\}, $$
$$A=\bigcup_{x\in A}\hat{x}=\sup_{(\wp(X), \subseteq)}\{\hat{x}|x\in A\}=\sup_{(\hat{X}, \subseteq)}\{\hat{x}|x\in A\}.$$
\end{theoremenv}
\begin{remark}
    We've know that $(\wp(X), \subseteq)$ is order complete. So for the sets not order complete,  we can build a relation between them to make it become order complete. And this theorem tell us how to do.
\end{remark}
\begin{proofenv}
    \quad
    \newline
    (1) Consider $\Phi :\wp(X)\rightarrow\wp(X), A\mapsto(A^\mathrm{u})^\mathrm{l}$.By the lemma,  $\Phi $  is increasing.Since $\wp(X)$ is complete, and $\hat{X}$ is the set of fixed point of $\Phi$. By Knaster-Tarski fixed point theorem, $(\hat{X}, \subseteq)$ is order complete.
    \newline
    (2) Let $A\in \wp(X)$,  we prove that $A^\mathrm{l}=((A^\mathrm{l})^\mathrm{u})^\mathrm{l}$. Since $A\subseteq (A^\mathrm{l})^\mathrm{u}$ (by the lemma),  $((A^\mathrm{l})^\mathrm{u})^\mathrm{l}\subseteq A^\mathrm{l}$,  by (2) of the lemma applied to $A^\mathrm{l}$. Hence $A^\mathrm{l}=((A^\mathrm{l})^\mathrm{u})^\mathrm{l}$
    \newline
    (3) Let $x$ and $y$ be element of $X$ such that $x< y$ then $\{x\}^\mathrm{l}\subseteq\{y\}^\mathrm{l}$. In fact,  if $z\in \{x\}^\mathrm{l}, z\le x$. Since $x<y, z<y$. Moreover,  $y\in \{y\}^\mathrm{l}$,  but $y\notin \{x\}^\mathrm{l}$.
    \newline
    (4) $\forall x \in A, x\in \{x\}^\mathrm{l}=\hat{x}$. So $A\subseteq\bigcup_{x\in A}\hat{x}$.
    Conversely,  $\forall x\in A,  x=\min (\{x\}^\mathrm{u})$. Hence $\{x\}^\mathrm{l}=(\{x\}^\mathrm{u})^\mathrm{l}\subseteq (A^\mathrm{u})^\mathrm{l}=A$. Therefore $\bigcup_{x\in A}\{x\}^\mathrm{l}\subseteq A$. Finally we get $\bigcup_{x\in A}\hat{x}=A\in \hat{X}$.
    \newline
    (5) If $A^\mathrm{u}=\varnothing$ then $A=(A^\mathrm{u})^\mathrm{l}=\varnothing^\mathrm{l}=X$. We assume that $A^\mathrm{u}\not=\varnothing$.
    $$\inf _{(\wp(X), \subseteq)}\{\hat{x}|x\in A^\mathrm{u}\}=\bigcap_{x\in A^\mathrm{u}}\hat{x}=\bigcap_{x\in A^\mathrm{u}}\{x\}^\mathrm{l}=(A^\mathrm{u})^\mathrm{l}=A.$$
    So it is equal to $\inf_{(\hat{X}, \subseteq)}\{\hat{x}|x\in A ^\mathrm{u}\}$.
\end{proofenv}
\begin{remark}
    $\forall A\in \hat{X}, A=\{x\in X|\hat{x}\subseteq A\}, A^\mathrm{u}=\{x\in X|A\subseteq \hat{x}\}$.
\end{remark}
\begin{definitionenv}
    $\hat{X}$ is called the Dedekind-MacNeille order completion of $(X, \le)$.
\end{definitionenv}
\section{Recursive Construction}
\begin{definitionenv}
    Let $(X, \le)$ be a partially ordered set.Let $I\subseteq X$. If $\forall a\in I , X_{<a}\subseteq I$,  we say that $I$ is an initial segment of $X$. 
\end{definitionenv}
\begin{propositionenv}
    Let $(X, \le)$ be a totally ordered set,  $I, J$ be initial segments of $X$. Either $I\subseteq J$ or $J\subseteq I$.
\end{propositionenv}
\begin{proofenv}
    Assume that $I\backslash J\not=\varnothing$.take $x\in I\backslash J, \forall y\in J$,  if $y\not\le x $, then $x<y$ and hence $x\in X_{<y}\subseteq J$, contradiction.Therefore $y\le x$.Then $y=x\in I$ or $y\in X_{<x}\subseteq I$.
\end{proofenv}
\begin{propositionenv}
    Let $(X, \le)$ be a well-ordered set. $I$ be an initial segment of $X$,  such that $I\not=X$.There is a unique $a\in X$ such that $I=X_{<a}$.
\end{propositionenv}
\begin{proofenv}
    $X\backslash I\not=\varnothing$ Let $a=\min (X\backslash I)$.By definition, $I\subseteq X_{<a}$. In fact,  $\forall y\in I$ if $y\not< a $,  then $a\le y$.
    Since $I$ is an initial segment $a\in I$,  contradiction.
    \newline
    Conversely,  if $x\in X_{<a}$,  then $x\notin X\backslash I$. Since otherwise $a\le x$. Therefore $x\in I$. Uniqueness, $\forall a\in X, a=\min(X\backslash X_{<a})=\min(X_\leq a)$. Hence $X_{<a}=X_{<b}\Rightarrow a=b$.
\end{proofenv}
\begin{propositionenv}
    Let $(X, \le)$ be a partially ordered set,  $\Lambda$ be a non-empty set,  and $(I_\lambda)_{\lambda\in \Lambda}$ be a family of initial segments of $X$. Then 
    $$I:=\bigcap_{\lambda\in \Lambda}I_\lambda, \space J:=\bigcup_{\lambda\in \Lambda}I_\lambda$$ are initial segments of $X$.
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    Let $a\in I.\forall \lambda\in \Lambda, a\in I_\lambda$ and hence $X_{<a}\subseteq I_\lambda$. Therefore, $X_{<a}\subseteq\bigcap_{\lambda\in\Lambda}I_\lambda=I$.
    \newline
    Let $b\in J$.Then $\exists \lambda_0\in \Lambda$ such that $b\in I_{\lambda_0}$. So $X_{<b}\subseteq I_{\lambda_0}\subseteq \bigcup_{\lambda\in \Lambda}I_\lambda=J$. 
\end{proofenv}
\begin{theoremenv}[Recursive construction]
    Let $(X, \le)$ be a well ordered set,  and $Y$ be a set. For any $x\in X$ and any mapping $h:X_{<x}\rightarrow Y$,  we fix an element $\Phi(h)\in Y$. Then,  there exists a unique mapping $f:X\rightarrow Y$  such that 
    $$\forall x\in X,  f(x)=\Phi(f\mid_{X_{<x}}).$$
\end{theoremenv}
\begin{exampleenv}
    For any $(a_0, \dots, a_{n-1})\in \RR^n$,  we fix an element $a_{n-1}+\varepsilon\in \RR$,  where $\varepsilon$ is a real number. There exists a unique mapping $(n\in \NN)\mapsto f(n)$ such that $f(n)=f(n-1)+\varepsilon$.($f(n):=n\varepsilon$).
\end{exampleenv}
\begin{proofenv}
    \quad
    \newline
    "Uniqueness": 
    \newline
    Let $f, g$ be mappings from $X$ to $Y$ such that 
    $$\forall x\in X, f(x)=\Phi(f\mid_{X_{<x}}), g(x)=\Phi(g\mid_{X_{<x}}).$$
    Then: $\forall x\in X$,  we have 
    $$(\forall y\in X_{<x}, f(y)=g(y))\Rightarrow f(x)=g(x).$$
    So by inclusion $\forall x \in X,  f(x)=g(x)$,  namely,  $f=g$.
    \newline
    "Existence": 
    \newline
    Let $\mathscr{S}$  be the set of initial segments $S$ of $X$ such that $\exists f_S:S\rightarrow Y$ satisfying 
    \begin{equation*}
        \forall x\in S, f_S(x)=\Phi(f_S\mid_{X_{<x}}). \tag{$*$}
    \end{equation*}
    Let $X_0=\bigcup_{S\in\mathscr{S} }S$. It is also an initial segment of $X$. For any $x\in X_0$ there exists $ S $ such that $x\in S$. If $S_1$ and $S_2$ are two elements of $\mathscr{S}$,  then $S_1\cap S_2$ is also an initial segment. Moreover $f_{S_1}\mid_{S_1\cap S_2}$ and $f_{S_2}\mid_{S_1\cap S_2}$ satisfy $(*)$ So $f_{S_1}\mid_{S_1\cap S_2}=f_{S_2}\mid_{S_1\cap S_2}$. Thus $f_S(x)$ does not depend on the choice of $S\in \mathscr{S}$ containing $x$. We denote it as $f(x)$. $f:X_0\rightarrow Y$ satisfying $(*)$. So $X_0\in \mathscr{S}$.
    If $X_0\not=X.\exists a\in X$ such that $ X_0=X_{<a}$. We extend $f$ to $X_0\cup\{a\}$ by letting $f(a)=\Phi(f)$. Then we get $X_0\cup\{a\}\in \mathscr{S}$.Contradiction.Therefore $X_0=X$  and we get the existence of $f$. 
\end{proofenv}

\begin{definitionenv}
    Let $A$ be a set. If there exists an injective mapping $A\rightarrow \NN$,  then we say that $A$ is \textbf{countable}.
    If  there exists an injective mapping $f:A\rightarrow \NN$ such that $f(A)$ is bounded from above (having an upper bound in $\NN$),  then we say that $A$ is \textbf{finite}.
\end{definitionenv}
\begin{lemmaenv}\label{4.9.8}
    \quad
    \newline
    (1) Let $n\in \NN$ and $x_0, \dots , x_n$ be elements of $\NN$ such that $x_0<\dots<x_n$,  then $\forall i\in \NN_{\leq n}, i\leq x_i$.
    \newline
    (2) Let $(x_n)_{n\in \NN}$ be a family of elements in $\NN$ such that $\forall n\in \NN, x_n<x_{n+1}$,  then $\forall i\in \NN, i\le x_i$.
\end{lemmaenv}
\begin{proofenv}
    If $j\le x_j$ for $j\in \{0, \dots, i-1\}$. Then,  in the case where $i=0, 0\le x_0$ holds since $0=\min_\le \NN$. In the case where $i>0$,  one has $i-1\le x_{i-1}<x_i$. So $x_i\ge x_{i-1}+1\ge i-1+1=i$.
\end{proofenv}
\begin{propositionenv}
    Let $f:A\rightarrow B$ be a mapping.
    \newline
    (1) If $f$ is injective and if $B$ is finite,  then $A$ is finite.(resp. countable)
    \newline
    (2) If $f$ is surjective and $A$ is finite,  then $B$ is finite.(resp countable)
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) Let $g:B\rightarrow \NN$ injective and bounded from above. Then $g\circ f$ is injective and $\mathrm{Im}(g\circ f)\subseteq \mathrm{Im}(g)$.
    \newline
    (2) $\exists$ injective mapping $B\rightarrow A$ by the axiom of choice. $f:A\rightarrow B$ For any $b\in B$,  pick $h(b)\in f^{-1}(\{b\})\subseteq A$, $h:B\rightarrow A$. If $h(b)=h(b')$,  then $f(h(b))=f(h(b'))=b'$.
\end{proofenv}
\begin{propositionenv}
    Let $X, Y$ be sets.
    \newline
    (1) If $X$ and $Y$ are finite,  then $X\cup Y$ is finite.(resp. countable)
    \newline
    (2) If $X$ is infinite and $Y$ is finite,  then $X\backslash Y$ is infinite.(resp. uncountable)
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) Let $f:X\rightarrow \NN$ and $g:Y\rightarrow \NN$ be injective mappings. We construct $h:X\cup Y\rightarrow \NN$ such that 
    $$h(x)=\left\{\begin{matrix}
     2f(x)\,  &x\in X\\
    2g(x)+1\, &x\in Y\backslash X

    \end{matrix}\right.$$
    $h$ is then injective,  and $h$ is bounded if $f$ and $g$ are bounded.
    \newline
    In fact,  if $(x, y)\in (X\cup Y)^2$, 
    \newline
    either $(x, y)\in X^2$ and $h(x)=2f(x)=h(y)=2f(y)$ if and only if $x=y$.
    \newline
    or $(x, y)\in (Y\backslash X)^2$ and $h(x)=h(y)\Rightarrow x=y$.
    \newline
    or $x\in X, y\in Y\backslash X.h(x)\not=h(y)$ (So $h(x)=h(y)\Rightarrow x=y$).
    \newline
    or $y\in X, x\in Y\backslash X , h(x)\not=h(y)$.
    \newline
    (2)  Assume that $X\backslash Y$ is finite,  then $X=(X\backslash Y)\cup Y$ is also finite.
\end{proofenv}
\begin{notationenv}
    If $f:X\rightarrow X$  is a mapping. Then $f^0$ denotes $\mathrm{Id}_X$. For $n\in\NN_{\ge 1}$, $f^n$ denotes $\underset{n}{\underbrace{f\circ f\circ \dots \circ f}}$. 
\end{notationenv}
\begin{theoremenv}\label{4.9.12}
    $\NN\times\NN$ and $\NN$ are equipotent.
\end{theoremenv}
\begin{proofenv}
    Let $f:\NN\times\NN\rightarrow \NN, (a, b)\mapsto2^a(2b+1)$. It is an injective mapping since $2^a(2b+1)=2^{a'}(2b'+1)$.So $a=a', b=b'$.Moreover $x\mapsto(0, x)$ is injective.
\end{proofenv}
\begin{corollaryenv}
    $\forall n\in \NN,  n\ge 1, \NN^n$ and $\NN$ are equipotent.
\end{corollaryenv}
\begin{proofenv}
    Induction on $n$.
    \newline
    For $n=1$,  easy. We assume that $\NN^n$ is equipotent to $\NN$ and $f:\NN^n\rightarrow\NN$ be a bijection. Then the mapping
    $$f':\NN^n\times \NN\rightarrow \NN\times\NN, \, \left(x_1, \dots, x_n;x_{n+1}\right)\mapsto\left(f(x_1, \dots, x_n), x_{n+1}\right)$$ 
    is a bijection. By Theorem \ref{4.9.12},  there exists a bijection $g:\NN^2\rightarrow\NN$.Therefore, 
    $$g\circ f':\NN^{n+1}\rightarrow\NN$$
    is a bijection,  which leads to $\NN^{n+1}$ and $\NN$ are equipotent.
\end{proofenv}

\textbf{Motivation}: Let $X$ be a set. A sequence in $X$ is by definition a family $(x_i)_{i\in I}$,  where $I$ is an infinite subset of $\NN$,  and each $x_i$ is an element of $X$.
\begin{exampleenv}
    $(a+bn)_{n\in \NN};\left(\frac{1}{n}\right)_{n\in \NN_{\ge 1}}$.
\end{exampleenv}
\begin{propositionenv}
    Let $I\subseteq \NN$.
    \newline
    (1) $\mathrm{Id}_I:I\rightarrow I$ is the only increasing mapping bijection from $I$ to $I$.
    \newline
    (2) If $I$ is bounded from above,  then $\mathrm{Id}_I$ is the only strictly increasing mapping from $I$ to $I$.
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) Let $f:I\rightarrow I$ be an increasing bijection. We want to prove:
    $$A:=\{x\in I\mid f(x)\not=x\}=\varnothing.$$
    If this set is non-empty,  it has a least element $n_0$. By definition,  $f(n_0)\not=n_0$. So either $n_0<f(n_0)$ or $n_0>f(n_0)$.
    \newline
    If $f(n_0)<n_0$,  then $f(n_0)\notin A$,  and hence $f(n_0)=f(f(n_0))<f(n_0)$,  contradiction. So $n_0<f(n_0)$. For any $n\in I$,  if $n_0\le n$ then $f(n_0)\le f(n)$. If $n_0>n$,  then $n\notin A$ and $f(n)=n<n_0$.$(*)$Hence $f(n)\not=n_0$ for any $n\in I$. This contradicts the assumption that $f$ is bijective.
    \newline
    (2) Suppose that $I$ is bounded from above,  and $f:I\rightarrow I$ is strictly increasing. We follow the same reasoning until $(*)$. $n_0<f(n_0)$ implies that $\forall k \in \NN,  f^k(n_0)<f^{k+1}(n_0)$,  that means 
    $$n_0<f(n_0)<\dots <f^{k+1}(n_0).$$
    So by the lemma \ref{4.9.8},  $k\le f^k(n_0)$,  this contradicts the assumption that $I$ is bounded from above.
\end{proofenv}
\begin{corollaryenv}
    Let $I\subseteq \NN$ bounded from above,  and $J\subseteq I$.If $J\not=I$,  there does not exist a strictly increasing mapping from $I$ to $J$.
\end{corollaryenv}
\begin{proofenv}
    Suppose that $f:I\rightarrow J$ is a strictly increasing mapping. Let $g:J\rightarrow I, x\mapsto x$ be a inclusion mapping. So $g\circ f:I\rightarrow I$ is strictly increasing and hence $g\circ f=\mathrm{Id}_I$. However $\mathrm{Im}(g\circ f)\subseteq\mathrm{Im}(g)=J\not=I$. Contradiction.
\end{proofenv}
\begin{propositionenv}
    Let $I\subseteq \NN$ non-empty.
    \newline
    (1) If $I$ is bounded from above,  then there exists a unique pair $(N, f)$,  where $N\in \NN$ and $f:\{0, 1, \dots, N\}\rightarrow I$ is an increasing bijection. (We say that the cardinality of $I$ is $N+1$.)
    \newline
    (2) If $I$ is NOT bounded from above,  there exists an increasing bijection from $\NN$ to $I$. (We say that the cardinality of $I$ is $\aleph_0$) 
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    We construct in a recursive way a family of elements in $I$. Let $x_0=\min(I)$. If $x_0, \dots , x_n$ are chosen (with $x_0<\dots<x_n$) we pick $x_{n+1}=\min(I\backslash\{x_0, \dots, x_n\})$. We slop at $N$ if $\{x_0, \dots, x_N\}=I$. Thus we obtain the increasing bijection needed by the proposition.
    \newline
    "Uniqueness" for (2): If $f:\NN\rightarrow I, g:\NN\rightarrow I$ are increasing bijections,  then $f^{-1}\circ g:\NN\rightarrow\NN$ and $g^{-1}\circ f:\NN\rightarrow\NN$ are increasing bijections. So $f^{-1}\circ g=\mathrm{Id}_{\NN}$. Hence $f=g$.
    \newline
    "Uniqueness" for (1): Let $f:\{0, 1, \dots, N\}\rightarrow I$ and $g:\{0, 1, \dots, M\}\rightarrow I$ be increasing bijections. $g^{-1}\circ f:\{0, 1, \dots, N\}\rightarrow \{0, 1, \dots,  M\}$ and $g:\{0, 1, \dots, M\}\rightarrow I$ be increasing bijections. $f^{-1}\circ g:\{0, 1, \dots, M\}\rightarrow \{0, 1, \dots,  N\}$ are increasing bijection. So $N\le M$ and $M\le N$,  which leads to $N=M, g=f$. 
\end{proofenv}
\begin{corollaryenv}
    A non-empty set $X$ is finite if and only if it can be written as $\{x_0, \dots, x_N\}$ where $N\in \NN$,  and $x_0, \dots, x_N$ are distinct elements of $X$.
\end{corollaryenv}
\begin{proofenv}
    Let $f:X\rightarrow \NN$ be an injective mapping with $f(x)$ bounded from above. Then there exists $(N, g)$ where $N\in \NN$ and $g:\{0, \dots , N\}\rightarrow f(x)$ is an increasing bijection. Then $f^{-1}\circ g:\{0, \dots, N\}\rightarrow X$ is a bijection. We take $x_i$ to be $(f^{-1}\circ g)(i)$ (Note that $N$ is unique $N+1$ is called the cardinality of $X$).
\end{proofenv}
\begin{propositionenv}
    Let $X$ be a set. The following condition are equipotent:
    \newline
    (1) $X$ is infinite.
    \newline
    (2) $\exists \NN\rightarrow X$ injective.
    \newline
    (3) $\exists$ injective mapping $f:X\rightarrow X$ such that $f(X)\not=X$.
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1)$\Rightarrow$(2) We construct a sequence $(x_n)_{n\in \NN}$ in $X$ as follows. $X\not=\varnothing$. We pick arbitrary $x_0\in X$. Suppose that distinct elements $x_0, \dots,  x_n$ of $X$ are chosen. The set $X\backslash\{x_0, \dots, x_n\}\not=\varnothing$ since otherwise $X=\{x_0, \dots, x_n\}$ is finite. We pick $x_{n+1}\in X\backslash\{x_0, \dots, x_n\}, \,  x_0, \dots, x_{n+1}$ are distinct. The mapping $\NN\rightarrow X,  n\mapsto x_n$ is injective.
    \newline
    (2)$\Rightarrow$(3) Let $f:\NN\rightarrow X$ be injective. We define $g:X\rightarrow X$.
    $$g(x):=
\left\{\begin{matrix}
 f(n+1)&,  &x=f(n) \\
x&, &x\notin f(\NN).
\end{matrix}\right. $$
$g(X)\not=X$ since $f(0)\notin g(X)$ If $x\notin f(\NN), g(x)=x\notin f(\NN)$,  so $g(x)\not=f(0)$. If $x=f(n),  g(x)=f(n+1)\not=f(0)$ since $f$ is injective.
\newline
(3)$\Rightarrow$(2) Let $g:X\rightarrow X$ be injective with $g(X)\not=X$. We pick $x_0\in X\backslash g(X)$. We define a sequence $(x_n)_{n\in\NN}$ by letting $x_{n+1}:=g(x_n)$. Since $g$ is injective,  $x_n\in g^n(X)\backslash g^{n+1}(X)$. otherwise $\exists y\in X$,  such that $x_n=g^{n}(x_0)=g^{n+1}(y)$. Hence $x_0=g(y)\in g(x)$ contradiction. Then $x_0, x_1, \dots, $ are distinct,  which defines an injective mapping $\NN\rightarrow X, n\mapsto x_n$.
\newline
(2)$\Rightarrow$(1) If $X$ is finite,  $\exists g:X\rightarrow\NN$ injective with $g(x)$ bounded. Then $\NN\rightarrow X\overset{g}{\rightarrow}\NN$ is injective with $h(\NN)$ bounded from above.
\end{proofenv}

\chapter{Groups}

\section{Composition Law}
\begin{definitionenv}
    Let $X$ be a set.
    \begin{enumerate}[ (i)]
        \item A \textbf{compositon law} on $X$ is a mapping
        $$*:X\times X\rightarrow X, (x, y)\mapsto x * y$$
        \item Let $Y\subseteq X$ be a set ,  $Y$ is \textbf{close under } $*$ if $\forall x, y \in Y,  x*y\in Y$
        \item $*$ is \textbf{communitative} if $\forall (x, y)\in X^2, x*y=y*x$
        \item $*$ is \textbf{associative} if $\forall (x, y, z)\in X^3, (x*y)*z=x*(y*z)$.
        If $*$ is associative,  then we can define
    $$x_1*x_2*\dots *x_n=(x_1*x_2*\dots *x_{n-1})*x_n$$
    \item Let $G$ be a set ,  $*$ is a composition law on $G$. If $*$ is associative, then we say $(G, *)$ is a \textbf{semigroup}
    \end{enumerate}
    
    

\end{definitionenv}
\begin{exampleenv}
    \quad
    \newline
    (1) Let $(X, *)$ be a composition law .We define $(X, \hat{*})$ satisfies:
    $$\hat{*}:X\times X\rightarrow X,  (x, y)\mapsto y*x$$
    By definition, $x=\hat{x}\Leftrightarrow *$ is communitative.If $*$ is associative,  then so does $\hat{*}$.
    Let $\mathfrak{M} _X$ the set of all mapping from $X$ to $X$.On $\mathfrak{M} _X$, the composition of mapping defines a composition law:
    $$\begin{matrix}
\mathfrak{M} _X\times \mathfrak{M} _X\rightarrow\mathfrak{M} _X \\
(f, g)\mapsto f\circ g

\end{matrix}$$
It is associative but not communitative:
\newline
Let $f_a:x\mapsto a , f_b:x\mapsto b , \forall x\in X$ Then,  $f_a\circ f_b=f_a, f_b\circ f_a=f_b$
\end{exampleenv}
\begin{propositionenv}
    Let $(X, *)$ be an associative composition law on a set $X$.If $n\in \NN_>0, x_1, \dots, x_n\in X$,  then ,  $\forall 1\le i\le n-1$,  we have 
    $$x_1*\dots *x_n=x_1*\dots *(x_i*x_{i+1})*\dots *x_n$$
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    $i=1$: By definition, $x_1*\dots *x_n=(x_1*x_2)*\dots *x_n$.We suppose $i\geq 2 $,  by the associativity of $*$,  we have
    $$x_1*\dots *x_{i+1}=(x_1*\dots *x_{i-1})*x_i*x_{i+1}=x_1*\dots *x_{i-1}*(x_i*x_{i+1})$$ 
\end{proofenv}
\begin{definitionenv}
    Let $(G, *)$ be a set equipped with a composition law ,  $g\in G$
    \newline
    If $\forall (x, y)\in G^2, g*x=g*y\Rightarrow x=y$, we say that $g$ is \textbf{left cancellative}.
    \newline
    If $\forall (x, y)\in G^2, x*g=y*g\Rightarrow x=y$, we say that $g$ is \textbf{right cancellative}.
    \newline
    If $*$ is communitative,  left cancellative $\Leftrightarrow$ right cancellative.
\end{definitionenv}
\begin{exampleenv}
    \quad
    \newline
    In $(\NN, +)$,  any element is cancellative.
    \newline
    In $(\NN, *)$,  any positive natural number is cancellative.

\end{exampleenv}
\section{Neutral Element \& Invertible Element}
\begin{definitionenv}
    $(X, *), e\in X$ is called a \textbf{neutral element} if 
    $$\forall x\in X, \,  e*x=x=x*e.$$
\end{definitionenv}
\begin{propositionenv}
    Assume $(X, *)$ admits a neutral element,  then its neutral element is unique.
\end{propositionenv}
\begin{proofenv}
    Let $e, e'\in X$ be neutral elements.Then 
    $$e=e*e'=e'.$$
\end{proofenv}
\begin{definitionenv}
    Let $(G, *)$ be a semigroup. If $(G, *)$ has a neutral element ,  then we say $(G, *)$ is \textbf{monoid}.
\end{definitionenv}
\begin{exampleenv}
    \quad
    \newline
    (1) $X$ is a set,  $(\mathfrak{M} _x, \circ)$ is a monoid with the neutral element $\mathrm{Id}_X$.
    \newline
    (2) $d\in \NN_{>0}$, $(d\NN, +)$ with neutral $0$, $(\NN, \times)$ with neutral $1$.
\end{exampleenv}
\begin{definitionenv}
    Let $(G, *)$ be a monoid with the neutral element $e$. For any $ (x, y)\in G^2$,  if $x*y=e$ then we say $x$ is a \textbf{left inverse} of $y$,  and $y$ is the \textbf{right inverse} of $x$.
\end{definitionenv}
\begin{remark}
    We say $x$ is \textbf{left invertible} if $x$ has a left inverse.(resp. right invertible)
\end{remark}
\begin{remark}
    $x$ is left invertible in $(G, *)\Leftrightarrow x $ is right invertible in $(G, \hat{*})$.
\end{remark}
\begin{propositionenv}
    Let $(G, *)$ be a monoid,  $g\in G$. If $g$ is both left invertible and right invertible,  then $g$ has a unique left inverse and a unique right inverse, which actually coincide.
\end{propositionenv}
\begin{proofenv}
    Let $x$ (resp. $y$) be a left (resp. right) inverse of $g$. Then ,  by the associativity law,  we have 
    $$x=x*e=x*(g*y)=(x*g)*y=y.$$
    Hence any left inverse is equal to $y$,  hence it is unique. Similarly for the right.
\end{proofenv}
\begin{definitionenv}
    Let $(G, *)$ be a monoid. If $g\in G$ is both left invertible and right invertible,  then we say $g$ is \textbf{invertible}.
    If $g$ is invertible,  the left inverse is equal to right inverse,  hence we called it the inverse of $g$,  denote by $\iota(g)$.
\end{definitionenv}
\begin{propositionenv}
    Let $(G, *)$ be a monoid,  $g\in G$. If $g$ is right (resp. left) invertible,  then it is right (resp. left) cancellative.
\end{propositionenv}
\begin{proofenv}
    Let $h$ be the right inverse of $g$. If $x*g=y*g$,  then 
    $$x=x*e=x*(g*h)=(x*g)*h=(y*g)*h=y*(g*h)=y*e=y.$$
\end{proofenv}
\begin{notationenv}
    For a monoid $(G, *)$.
    \newline
    If $*$ is written multiplicatively,  we usually denote $x*y$ as $x\cdot y$ or $xy$. If no ambiguity,  neutral element as $1$, inverse of $x$ as $x^{-1}$.
    \newline
    If $*$ is written additively,  $x*y$ as $x+y$,  neutral element as $0$,  inverse of $x$ as $-x$.
\end{notationenv}
\begin{propositionenv}\label{proposition5.2.4}
    Let $(G, *)$ be a monoid.
    \newline
    (1) If $x\in G$ is an invertible element ,  then $\iota (x)$ is also invertible,  and $\iota(\iota(x))=x$.
    \newline
    (2) If $x, y\in G$ are invertible,  so does $x*y$ and $\iota(x*y)=\iota(y)*\iota(x)$.
\end{propositionenv}
\begin{proofenv}
    \quad\newline
    (1) $$x*\iota(x)=\iota(x)*x=e.$$
    (2) $$(xy)(\iota(y)\iota(x))=xy\iota(y)\iota(x)=xe\iota(x)=x\iota(x)=e.$$
    $$(\iota(y)\iota(x))(xy)=\iota(y)\iota(x)xy=\iota(y)ey=\iota(y)y=e.$$
\end{proofenv}
\begin{definitionenv}
    Let $(G, *)$ be a monoid. If any element of $G$ is invertible,  then we say $G$ with the composition law is a \textbf{group}. A communitative group is also called \textbf{abelian group}.
\end{definitionenv}
\begin{box2}
   Now we have : 
   \newline
   {\color{mlv} (binary operations on $X$ )$\supseteq$(semigroup)$\supseteq$(monoids)$\supseteq$(group)$\supseteq$(abelian group)}

\end{box2}
\begin{exampleenv}
    \quad
    \newline
    (1) $(\ZZ, +)$ is an abelian group.
    \newline
    (2) Let $X$ be a set and $\mathfrak{S}_X$ be the set of bijections from $X$ to $X$.$(\mathfrak{S}_X , \circ)$ is a monoid with the neutral element $\mathrm{Id}_X$.Since $f\in \mathfrak{S}_X $ is bijective,  hence there exists a unique inverse $f^{-1}\in \mathfrak{S} _X$.So $(\mathfrak{S} _x, \circ)$ is a group (but not abelian in general), called the symmetric group of $X$.
    \newline
    Let $\mathfrak{S} _n$ be the symmetric group of the set $\NN_{\le n}$,  its element $f$ can be denoted as a table:
    $$\begin{pmatrix}
  1&2  &\dots &n \\
  f(1)& f(2) &\dots   &f(n)
\end{pmatrix}.$$
\end{exampleenv}
\section{Substructure}
\begin{definitionenv}
    Let $(G, *)$ be a semigroup,  $H$ be a subset of $G$. If $H$ is close under $*$,  then we say $H$ is a \textbf{subsemigroup} of $(G, *)$. Note that $H$ equipped with the restriction of $*$ forms a semigroup.
    Let $(G, *)$ be a monoid. If a sub-semigroup $H$ of $(G, *)$ contains the neutral element of $(G, *)$,  then we say $H$ is a \textbf{submonoid} of $(G, *)$.
\end{definitionenv}
\begin{exampleenv}
   \quad
   \newline
   (1) Let $d\in \NN^*$,  then $d\NN$ forms a submonoid of $(\NN, +)$.
   $d\NN$ is a subsemigroup of $(\NN, \cdot)$.
    \newline
    (2) $\mathfrak{S} _X$ is submonoid of $(\mathfrak{M} _X, \circ)$.
\end{exampleenv}
\begin{propositionenv}\label{proposition5.3.1}
    Let $(M, *)$ be a monoid,  $H\subseteq M$ be a non-empty subset. Suppose that any element of $H$ is invertible in $M$ ,  and ($\forall x, y \in H, (x, y)\mapsto x*\iota(y)$),  if $\forall x, y\in H,  x*\iota(y)\in H$,  then $H$ is a submonoid of $M$. Moreover,  $H$ equipped with the restriction of $*$ forms a group $(H, *|_H)$. 
\end{propositionenv}
\begin{proofenv}
    Let $e$ be the neutral element of $(M, *)$. Let $a\in H$,  then $e=a\circ\iota(a)\in H$. For any $y\in H$,  one has $\iota(y)=e*\iota(y)\in H$. For any $(x, y)\in H^2$,  $x*y=x*\iota(\iota(y))\in H$. Hence $H$ is closed under $*$ and it contains the neutral element. Also,  $\forall y\in H,  \iota(y)\in H$,  hence $H$ is group. 
\end{proofenv}
\begin{corollaryenv}
    Let $(M, *)$ be a monoid,  $G$ be the set of all invertible element in $M$. Then $G$ is a submonoid. Moreover,  $G$ equipped with the restriction of $*$ forms a group.
\end{corollaryenv}
\begin{proofenv}
    By definition,  any element in $G$ is invertible in $M$. By Proposition \ref{proposition5.2.4},  $\forall x, y\in G,  x*\iota(y)\in G$. Therefore,  Proposition \ref{proposition5.3.1} implies the claim.
\end{proofenv}
\begin{notationenv}
    Let $M$ be a monoid,  we often use $M^\times$ to denote the submonoid of $M$ consisting of all invertible element if the composition law on $M$ is not written additively.
\end{notationenv}
\begin{exampleenv}
    Let $X$ be a set,  $\mathfrak{M} _X^\times=\mathfrak{S} _X$.
\end{exampleenv}
\begin{definitionenv}
    Let $(G, *)$ be a group,  $H\subseteq G$ be a submonoid. If $\forall x\in H$,  one has $\iota(x)\in H$,  then we say $H$ is \textbf{subgroup} of $G$.
\end{definitionenv}
\begin{propositionenv}
    Let $(M, *)$ be a monoid,  $\varnothing \not=H\subseteq M^\times$ be a subset such that $\forall x, y \in H$, 
    $$x*\iota(y)\in H.$$
    Then $H$ is a subgroup of $M^\times$.
\end{propositionenv}
\begin{proofenv}
    Let $e$ be the neutral element of $(M, *)$.By Proposition\ref{proposition5.3.1},  we obtain that $H$ forms a submonoid of $M^\times$.Moreover,  $\forall x \in H$,  one has $\iota(x)=e*\iota(x)\in H$.So $H$ is a subgroup of $M^\times$.
\end{proofenv}
\begin{propositionenv}
    Let $(G, *)$ be a semigroup (resp. monoid, group),  $(H_i)_{i\in I}$ be a family of subsemigroups (resp. submonoids, subgroups), where $I$ is a non-empty set. Then 
    $$H:=\bigcap_{i\in I}H_i$$
    is a subsemigroup (resp.submonoid, subgroup) of $G$.
\end{propositionenv}
\begin{proofenv}
    \quad\newline
    For semigroup case, let $x, y\in H$ then $x, y\in H_i, \forall i \in I$.Then $x*y\in H_i, \forall i \in I $,  thus 
    $$x*y\in \cap_{i\in I}H_i=H.$$
    For monoid case, the neutral element $e$ of $G$ satisfies
    $$e \in H_i, \forall i \in I\Rightarrow e\in \bigcap_{i\in I}H_i=H.$$
    For group case, to check $x*\iota(y)\in H$ like above.
\end{proofenv}
\section{Homomorphism}
\begin{definitionenv}
    Let $(M, *)$ and $(N, \star)$ be  semigroups,  $f:M\rightarrow N$ be a mapping of sets.
    \newline
    (1) $f$ is called a \textbf{semigroup homomorphism} from $(M, *)$ to $(N, \star)$ if
    $$f(a*b)=f(a)\star f(b), \forall a, b\in M.$$
    (2) If moreover,  $(M, *)$ and $(N, \star)$ are both monoids with neutral elements $e_M, e_N$,  $f$ is called a \textbf{monoid homomorphism} if 
     $$f(a*b)=f(a)\star f(b), \forall a, b\in M, $$ 
     $$f(e_M)=e_N.$$
    (3) If moreover,  $(M, *)$ and $(N, \star)$ are both groups,  $f$ is called a \textbf{group homomorphism} if 
    $$f(a*b)=f(a)\star f(b), \forall a, b\in M, $$
    $$f(e_M)=e_N, $$
    $$f(\iota(a))=\iota(f(a)), \forall a\in M.$$
    (They are not independent.)
\end{definitionenv}
\begin{remark}
    Let $(M,*),(N,\star)$ be groups, we claim that if $\forall a,b\in M,f(a*b)=f(a)\star f(b)$, then $f(e_M)=e_N$ and $f(\iota(a))=\iota(f(a))$. 
    Let $b=e_M$, then 
    {\small$$ f(e_M)=\left(\iota(f(a))\star f(a)\right)\star f(e_M)=\iota(f(a))\star\left( f(a)\star f(e_M)\right)=\iota(f(a))\star f(a)=e_N.$$
    $$\iota(f(a))=\iota(f(a))\star e_N=\iota(f(a))\star\left(f(a)\star f(\iota(a))\right)=\iota(f(a))\star f(a)\star f(\iota(a))=f(\iota(a)).$$}
    But for monoid,  we need $f(e_M)=e_N$.
\end{remark}
\begin{propositionenv}
    \quad 
    \newline
    Let $f:(M, *)\rightarrow (N, \star)$ be a semigroup (resp. monoid, group) homomorphism. If $M_1$ is a subsemigroup (resp. submonoid, subgroup) of $M$,  then the image $f(M_1)$ is a subsemigroup (resp. submonoid, subgroup).
\end{propositionenv}
\begin{proofenv}
    The semigroup case.Let $x, y\in f(M_1)$,  we may write $x=f(a), y=f(b), a, b\in M_1$
    $$x\star y=f(a)\star f(b)=f(a*b)\in f(M_1).$$
    The monoid case.We denote $e_M, e_N$ be the neutral elements of $M, N$
    $$e_M\in M_1, e_N=f(e_M)\in f(M_1).$$
    The group case.We have to check that $x, y\in f(M_1), x\star \iota(y)\in f(M_1)$
    $$\forall a\in M,  f(a)\star f(\iota(a))=f(a*\iota(a))=f(e_M)=e_N.$$
    We may write $x=f(a), y=f(b), a, b\in M_1$
    $$x\star \iota(y)=f(a)\star\iota(f(b))=f(a)\star f(\iota(b))=f(a*\iota(b))\in f(M_1).$$
\end{proofenv}
\begin{remark}
    \quad
    \newline
    (1) The semigroup homomorphism
    $$f:(\NN, \times)\rightarrow (\NN, \times), n\mapsto 0$$
    of two monoids, but is not a monoid homomorphism, and its image is $\{0\}$,  which is not a submonoid of $(\NN, \times)$.
    \newline
    (2) Let $M$ be a semigroup (resp. monoid, group) and let $N$ be a subsemigroup (resp. submonoid, subgroup).Then the inclusion mapping $\jmath :N\rightarrow M$ is a semigroup (resp. monoid,  group) homomorphism.
\end{remark}
\begin{propositionenv}\label{propositions5.4.2}
    Let $(X, *)\overset{f}{\rightarrow} (Y, \star)\overset{g}{\rightarrow}(Z, \diamond ) $ be semigroup (resp. monoid,  group) homomorphisms.Then so does the composite mapping $g\circ f$.
\end{propositionenv}
\begin{proofenv}
    The semigroup case.
        \begin{align*}
            (g\circ f)(x_1*x_2) & = g(f(x_1*x_2))  = g(f(x_1)\star f(x_2)) \\
            & = g(f(x_1))\diamond g(f(x_2)), \forall x_1, x_2\in X.
        \end{align*}
    The monoid case :
    $$(g\circ f)(e_X)=g(f(e_X))=g(e_Y)=e_Z.$$
    The group case:
    $$(g\circ f)(\iota(x))=g(f(\iota(x)))=g(\iota(f(x)))=\iota((g\circ f)(x)).$$
\end{proofenv}
\begin{propositionenv}
    Let $f:(X, *)\rightarrow (Y, \star)$ be a semigroup (resp.monoid,  group) homomorphism between semigroups (resp.monoids groups).If $f$ is bijective,  then its inverse mapping $f^{-1}:Y\rightarrow X$ is also a semigroup homomorphism (resp.monoid, group) 
\end{propositionenv}
\begin{proofenv}
    The semigroup case: Let $y_1, y_2\in Y$ and let $x_i=f^{-1}(y_i), \, i=1, 2$. Then 
    $$y_1\star y_2=f(x_1)\star f(x_2)=f(x_1*x_2), $$
    $$f^{-1}(y_1\star y_2)=x_1*x_2=f^{-1}(y_1)*f^{-1}(y_2).$$
    The monoid case:
    $$f(e_X)=e_Y\Rightarrow f^{-1}(e_Y)=e_X.$$
    The group case:
    $$f^{-1}(\iota(y))\overset{y=f(x)}{=}f^{-1}(\iota(f(x)))=(f^{-1}\circ f)(\iota(x))=\iota(f^{-1}(y)).$$
\end{proofenv}
\begin{definitionenv}
    A semigroup (resp. monoid,  group) homomorphism $f:X\rightarrow Y$ is called a \textbf{semigroup (resp.monoid, group) isomorphism} if there exists a semigroup (resp.monoid, group) homomorphism $g:Y\rightarrow X$,  such that 
    $$g\circ f=\mathrm{Id}_X, f\circ g=\mathrm{Id}_Y.$$
\end{definitionenv}
By Proposition \ref{propositions5.4.2},  a semigroup (resp.monoid group) homomorphism is a semigroup (resp.monoid , group) isomorphism if and only if $f$ is a bijection.
\begin{propositionenv}
    Let $(G, *)$ be a group.The inversion mapping $\imath :(G, *)\rightarrow(G, \hat{*})$ is a group isomorphism.
\end{propositionenv}
\section{Quotient}\label{5.5}
\begin{definitionenv}
    Let $X$ be a set and $\sim$ be a binary relation on $X$. (We write $x\sim y$ the condition $(x, y)\in \Gamma_\sim$)
    \newline
    (1) If $\forall x\in X,  x\sim x$.
    \newline
    (2) $\forall (x, y)\in X^2,  x\sim y\Rightarrow y\sim x$.
    \newline
    (3) $\forall (x, y, z)\in X^3,  (x\sim y \text{ and } y\sim z )\Rightarrow x\sim z$.
    \newline
    We say that $\sim $ is a \textbf{equivalence relation}.
\end{definitionenv}
\textit{Check Section~\ref{4.2}:Equivalent Relation,  to get more information about it.}
\begin{propositionenv}
    Let $(X_i)_{i\in i}$ be a family of sets. For any $i\in I$,  let $\sim_i$ be an equivalence relation on $X_i$. Let $X=\prod_{i\in I}X_i$. We define a binary relation $\sim$ on $X$ as follows:
    $$(x_i)_{i\in I}\sim (y_i)_{i\in I}\Leftrightarrow \forall i\in I,  x_i\sim_i y_i.$$
    Then,  $\sim $ is an equivalence relation,  and the mapping
    $$X/\sim \overset{\Phi}{\longrightarrow }\prod_{i\in I}X_i/\sim_i, $$
    $$ [(x_i)_{i\in I}]\longmapsto  ([x_i])_{i\in I}$$
    is a bijection.
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) Let $(x_i)_{i\in I}\in X. \forall i\in I, x_i\sim x_i$,  so $(x_i)_{i\in I}\sim(x_i)_{i\in I}$.
    \newline
    (2) Let $x=(x_i)_{i\in I}, y=(y_i)_{i\in I},  x_i\sim_i y_i$,  so $y_i\sim x_i$. Therefore,  $y\sim x$.
    \newline 
    (3) Let $x=(x_i)_{i\in I}, y=(y_i)_{i\in I}, z=(z_i)_{i\in I}$ in $X$. If $x\sim y$ and $y\sim z$,  then $\forall i \in I, x_i\sim_i y_i$ and $y_i\sim_i z_i$. Hence $\forall i \in I,  x_i\sim_i z_i$. So $x\sim z$.
    \newline
    We check that $\Phi$ is well defined. Let $x=(x_i)_{i\in I}$ and $y=(y_i)_{i\in I}$ be elements of $X$. If $[x]=[y]$,  then $x\sim y$ and hence $\forall i\in I,  x_i\sim_i y_i$,  that means 
    $$([x_i])_{i\in I}=([y_i])_{i\in I}.$$
    By definition,  $\Phi$ is surjective. If $\Phi([(x_i)_{i\in I}])=\Phi([(y_i)_{i\in I}])$,  then $\forall i\in I,  [x_i]=[y_i]$,  namely $x_i\sim_i y_i$. Therefore,  $([(x_i)_{i\in I}])=([(y_i)_{i\in I}])$.
\end{proofenv}
\begin{notationenv}
    Let $X$ be a set ,  $\sim $ be an equivalence relation on $X$. Then $X/\sim$ is called the \textbf{quotient} of $X$ by $\sim$. The mapping 
    $$\pi:X\longrightarrow X/\sim , $$
    $$x\longmapsto [x]$$
is called the \textbf{quotient mapping}.
\end{notationenv}
\begin{definitionenv}
    Let $X$ be a set,  $f:X\rightarrow Y$ be a mapping and $\sim $ an equivalence relation on $X$. If $\forall(x, y)\in X^2,  x\sim y\Rightarrow f(x)=f(y)$ we say that $\sim$ is \textbf{compatible} with $f$.
\end{definitionenv}
\begin{theoremenv}[Proposition\ref{4.2.5}]\label{5.5.5}
    Let $f:X\rightarrow Y$ be a mapping and $\sim $ be an equivalence relation on $X$ which is compatible with $f$. Then there exists a unique mapping 
    $$\tilde{f}:X/\thicksim\rightarrow Y,  [x]\mapsto f(x), $$such that $$\tilde{f}\circ \pi =f.$$
    \begin{center}
\begin{tikzcd}
    X\arrow[d, swap, "\pi"]\arrow[r, "f"]& Y\\
    X/\sim \arrow[ur, swap, "\tilde{f}"]
\end{tikzcd}
\end{center}
\end{theoremenv}
\begin{proofenv}
    If such $\tilde{f}$ exists. For an $x\in X$. 
    $$\tilde{f}([x])=\tilde{f}(\pi(x))=f(x)$$
    So $\tilde{f}$ is unique. To prove the existence,  it suffices to check that $\tilde{f}:X/\sim\rightarrow Y$ is well defined. If $[x]=[y]$,  then $[x]\mapsto f(x),  x\sim y$ and hence $f(x)=f(y)$. So $\tilde{f}$ is well defined.
\end{proofenv}
\begin{definitionenv}
    We call $\tilde{f}$ the \textbf{mapping induced by $f$ by passing to quotient}.
\end{definitionenv}
\begin{exampleenv}
    Let $X$ be a set and $*$ a composition law on $X$. We say that an equivalence relation $\sim$ on $X$ is compatible to $*$ if $\forall (x_1, y_1), (x_2, y_2)\in X^2$
    $$(x_1\sim x_2 \text{ and } y_1\sim y_2)\Leftrightarrow x_1*y_1\sim x_2*y_2$$
    Or equivalently,  the equivalence relation $R$ on $X\times X$ defined by 
    $$(x_1, y_1)R(x_2, y_2)\Leftrightarrow x_1\sim x_2\text{ and }y_1\sim y_2$$
    is compatible with the mapping:
    $$X\times X\longrightarrow X/\sim$$
    $$(x, y)\longmapsto [x*y]$$
    By the theorem,  $*$ induces by passing to quotient a mapping
    $$(X/\sim)\times (X/\sim) \longrightarrow (X\times X)/R\longrightarrow X/\sim$$
    $$([x], [y])\longmapsto[(x, y)]\longmapsto[x*y]$$
    The compatible mapping
    $$(X/\sim)\times (X/\sim) \longrightarrow X/\sim$$
    $$([x], [y])\longmapsto[x*y]$$
    defines a composition law on $X/\sim$,  which is often denoted as $*$ by abuse of notation,  called the composition law on $X/\sim$ induced by the composition law $*$ on $X$ by passing to quotient.
\end{exampleenv}
\begin{exampleenv}
    $N_n$ on $\ZZ$. 
    \newline
    If $n\mid (x_1-x_2)\ n\mid (y_1-y_2)$,  then $n\mid(x_1+y_1)-(x_2+y_2)$.
    \newline
    Since $x_1y_1-x_2y_2=x_1(y_1-y_2)+(x_1-x_2)y_2$,  $n\mid x_1y_1-x_2y_2$.
    \newline
    Hence $+$ and $\cdot$ on $\ZZ$ induces by passing to equivalent composition law on $\ZZ/\sim_n$.
\end{exampleenv}
\begin{propositionenv}
    \quad\newline
    (1) If $*:X\times X\rightarrow X$ is associative (resp.communitative) then so is $$*:(X/\sim)\times(X/\sim)\rightarrow X/\sim$$
    (2) If $e$ is a neutral element of $(X, *)$,  then $[e]$ is a neutral element of $(X/\sim, *)$.
    \newline
    (3) If $(X, *)$ is a semigroup (resp. monoid),  then the projection $$\pi:X\rightarrow X/\sim,  x\mapsto[x]$$ is a homomorphism of  semigroup (resp.monoid).
    \newline
    (4) If $(X, *)$ is a monoid,  $x\in X$ is invertible,  then $[x]$ is invertible in $(X/\sim, *)$.
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1)associative:
     $[x]*([y]*[z])=[x]*[y*z]=[x*(y*z)]=[(x*y)*z]=[x*y]*[z]=([x]*[y])*[z]$.
    \newline
    communitative:
    $[x]*[y]=[x*y]=[y*x]=[y]*[x]$
    \newline
    (2) $[e]*[x]=[e*x]=[x], [x]*[e]=[x*e]=[x]$.
    \newline
    (3) $$\pi(x*y)=[x*y]=[x]*[y]=\pi(x)*\pi(y)$$
    $\pi(e)=[e]$ is the neutral element of $(X/\sim, *)$.
    \newline
    (4) By (3),  $\pi$ is a homomorphism of monoid,  $\forall x \in X^\times,  \pi(x)=[x]\in (X/\sim )^\times $ and $\iota([x])=[\iota(x)]$.
\end{proofenv}
\begin{remark}
    If $(X, *)$ is a group,  so is $(X/\sim, *)$.
\end{remark}
\begin{definitionenv}
    \quad
    \newline
    If $(X, *)$ is a semigroup (resp. monoid,  group),  then $(X/\sim,  *)$ is called the \textbf{quotient semigroup} (resp. quotient monoid,  quotient group) of $(X, *)$ by $\sim$.
\end{definitionenv}
\begin{definitionenv}
    Let $(X, *), (Y, \star)$ be groups and $f:X\rightarrow Y$ be a homomorphism of groups. We define the \textbf{kernel} of $f$ as 
    $$\ker(f):=\{x\in X\mid f(x)=e_Y\}$$
    where $e_Y$ is the neutral element of $Y$.
\end{definitionenv}
\begin{propositionenv}
    Let $(X, *)$ be a monoid,  $(Y, \star)$ be a semigroup,  $f:X\rightarrow Y$ be a homomorphism of semigroups. If $f$ is surjective,  then $(Y, \star)$ is a monoid,  and $f$ is a homomorphism of monoid.
\end{propositionenv}
\begin{proofenv}
    We check that $f(e_X)$ is the neutral element of $Y$. $\forall y\in Y, \exists x\in X, f(x)=y$. So $f(e_X)\star y=f(e_X)\star f(x)=f(e_X*x)=f(x)=y$. Also $y\star f(e_X)=f(x)\star f(e_X)=f(x*e_X)=f(x)=y$.
\end{proofenv}
\begin{propositionenv}
    Let $(X, *)$ be a monoid and $(Y, \star)$ be a group. If $f:X\rightarrow Y$ is a homomorphism of semigroups,  then it is homomorphism of monoids.
\end{propositionenv}
\begin{proofenv}
    Let $e_X$ and $e_Y$ be neutral elements of $X$ and $Y$. One has $e_X=e_X*e_X$,  so $f(e_X)=f(e_X)\star f(e_X)$,  so $e_Y=f(e_X)$.
\end{proofenv}
\begin{propositionenv}
    \quad
    \newline
    (1) $\ker(f)$ is a subgroup of $X$.
    \newline
    (2) $\forall (a, x)\in X\times \ker(f) $,  there exists $y\in \ker(f)$ such that $a*x=y*a$.
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) The neutral element $e_x$ of $(X, *)$ belongs to $\ker(f)$. If $x, y$ are elements of $\ker(f)$,  then 
    $$f(x*\iota(y))=f(x)\star f(\iota(y))=f(x)\star \iota(f(y))=e_Y\star\iota(e_Y)=e_Y, $$
    so,  $x*\iota(y)\in \ker(f)$.
    \newline
    (2) We should take $y:=(a*x)*\iota(a)$. It remains to check that $y\in \ker(f)$. One has $f(y)=f(a*x*\iota(a))=f(a)\star f(x)\star \iota(f(a))=f(a)\star \iota(f(a))=e_Y$.
\end{proofenv}
\begin{definitionenv}
    Let $(G, *)$ be a group and $H$ be a subgroup of $G$. If $\forall (a, x)\in G\times H$,  $a*x*a^{-1}\in H$,  we say that $H$ is a \textbf{normal subgroup}.
\end{definitionenv}
\begin{propositionenv}
    Let $(G, *)$ be a group and $H$ be a normal subgroup of $G$.
    \newline
    (1) The binary relation $\sim_H$ on $G$ defined as 
    $$x\sim_H y\Leftrightarrow x*\iota(y)\in H$$
    is an equivalence relation on $G$. Moreover,  
    $$\forall x\in G,  [x]=H*x:=\{y*x\mid y\in H\}.$$
    (2) If $H$ is normal,  then
    $$\forall x\in G, x*H=H*x.$$
    Moreover,  $\sim_H$ is compatible with $*$.
    \newline
    (3) The kernel of $\pi :G\rightarrow G/\sim_H$ is equal to $H$.
\end{propositionenv}
\begin{proofenv}
    \quad\newline
    (1) If $x\sim_H y$,  then $x*\iota(y)\in H$,  so $y*\iota(x)=\iota(x*\iota(y))\in H$,  so $y\sim_H x$. If $x\sim_H y$ and $y\sim_H z$,  then $x*\iota(y)\in H,  y*\iota(z)\in H$,  so $x*\iota(z)=x*\iota(y)*y*\iota(z)\in H$. Hence $x\sim_H z$. By definition,  $[x]:=\{y\in G\mid  x*\iota(y)\in H\}$. If $y\in [x]$,  then $y*\iota(x)\in H$. Hence $y=(y*\iota(x))*x\in H*x$. Conversely,  if $y=h*x\in H*x \quad (h\in H)$,  then $y*\iota(x)=h*x*\iota(x)\in H$. So $y\in [x]$,  $[x]=H*x$.
    \newline
    We denote by $G/H $ the set 
    $$G/H:=\{x*H\mid x\in G\}.$$ 
    We denote by $H\backslash G$ the set 
    $$H\backslash G:=\{H*x\mid x\in G\}.$$
    (2) Suppose that $H$ is normal.$\forall (x, y)\in G\times H$,  one has $x*y*\iota(x)\in H$. So $\forall y\in H, \exists z(=x*y*\iota(x))\in H$ such that $x*y=z*x$. So $x*H\subseteq H*x$. Conversely,  $H*x\subseteq x*H$. Let $x_1, x_2, y_1, y_2$ be elements of $G$,  such that $x_1\sim_H x_2, y_1\sim_H y_2$.
    \begin{align*}
&(x_1*y_1)*\iota (x_2*y_2)\\
=&x_1*y_1*\iota (y_2)*\iota (x_2)\\
=&x_1*(y_1*\iota (y_2))*\iota (x_1)*x_1*\iota (x_2)\in H.
\end{align*}
    (3)$$\ker(\pi)=[e_G]=H*e_G=H.$$
\end{proofenv}
\begin{notationenv}
    If $H$ is a normal subgroup of $G$,  we denote by $G/H$ the quotient group $G/\sim_H$.
\end{notationenv}
\begin{theoremenv}
    Let $f:(X, *)\rightarrow (Y, \star)$ be a homomorphism of groups,  and $K=\ker(f)$. Then $\sim_K$ is compatible with $f$,  and $f$ induces by passing to quotient a mapping
    $$\tilde{f}:X/K\longrightarrow Y, $$
    which is actually an injective homomorphism of groups,  with $\tilde{f}(X/K)=f(X)$. In particular,  $X/K$ is isomorphism to $f(X)$.
        \begin{center}
\begin{tikzcd}
    X\arrow[d, swap, "\pi"]\arrow[r, "f"]& f(X)\subseteq Y\\
    X/\ker(f) \arrow[ur, swap, "\tilde{f}"]
\end{tikzcd}
\end{center}
\end{theoremenv}
\begin{proofenv}
    Let $x$ and $y$ be elements of $X$. $x\sim_K y\Leftrightarrow x*\iota(y)\in K$. Hence $f(x)\star \iota(f(y))=f(x*\iota(y))=e_Y$. So $f(x)=f(y)$. $\tilde{f}([x]*[y])=\tilde{f}([x*y])=f(x*y)=f(x)\star f(y)=\tilde{f}([x])\star \tilde{f}([y])$.
\end{proofenv}

\section{Universal Homomorphisms}
\begin{propositionenv}
    Let $(M, *)$ be a monoid,  $x\in M$. Then there exists a unique homomorphism of monoid $f:(\NN, +)\rightarrow (M, *)$ such that $f(1)=x$.
\end{propositionenv}
\begin{proofenv}
    We construct a mapping $f:\NN\rightarrow M$ in a recursive way as follows: $f(0)=e_M$. For any $n\in \NN$,  we let $f(n+1)=f(n)*x$. We will prove that $f$ is a homomorphism of monoids,  that is 
    $$\forall (n, m)\in \NN\times \NN, f(n+m)=f(n)*f(m).$$
    We reason by induction on $m$. If $m=0$,  $f(n)=f(n)*e_M$. Suppose that $f(n+m)=f(n)*f(m)$. One has $$f(n+m+1)=f(n+1)*f(m)=f(n)*f(1)*f(m)=f(n)*f(m+1).$$
    If $g:\NN\rightarrow M$ is a homomorphism of monoid,  such that $g(1)=x$. Since $g(n+1)=g(n)*g(1)=g(n)*x$,  we have $g(n)=f(n)$. By induction,  $\forall n\in \NN, g(n)=f(n)$. So $f$ is unique.
\end{proofenv}
\begin{notationenv}
    Let $(M, *)$ be a monoid,  $x\in M$,  $f:(\NN, +)\rightarrow (M, *)$ be the unique homomorphism of monoid,  such that $f(1)=x$. For any $n\in \NN$,  we denote by $x^{*n}$ the element $f(n)\in M$,  $x^{*0}=e_M$,  $x^{*(n+m)}=x^{*n}*x^{*m}$.
    \newline
    Two exceptions: If $*=\cdot$ is written multiplicatively,  $x^{*n}$ is written as $x^n$. If $*=+$,  then $x^{*n}$ is written as $nx$. 
\end{notationenv}
\begin{propositionenv}
    Let $(M, *)$ be a monoid,  $x\in M$. There exists a unique homomorphism of monoids $f:(\ZZ, +)\rightarrow (M, *)$ such that $f(1)=x$. Note that  $f(\ZZ)\subseteq M^\times$. So $f$ defines a homomorphism of groups $f:(\ZZ^\times, +)\rightarrow (M^\times, *)$.
\end{propositionenv}
\begin{proofenv}
    We define $f$ as 
    $$f(n):=\left\{\begin{matrix}
        x^{*n}, &n\ge 0\\
        \iota(x^{*(-n)}), &n<0
    \end{matrix}\right. .$$
    Let $n, m$ be two elements of $\ZZ$.
    \newline
    (1) If $n, m>0$. Then $f(n+m)=x^{*n}*x^{*m}=f(n+m)$.
    \newline
    (2) If $n, m<0$. Then $f(n+m)=\iota(x^{*(-n-m)})=\iota(x^{*(-m)}*x^{*(-n)})=\iota(x^{*(-n)})*\iota(x^{*(-m)})=f(n)*f(m)$.
    \newline
    (3) If $n>0, m<0$ and $n+m>0$. Then $$f(n+m)=x^{*(n-(-m))}=x^{*n}*\iota(x^{*(-m)})=f(n)*f(m).$$
    (4) If $n>0, m<0$ and $n+m<0$. Then $$f(n+m)=\iota(x^{*(-n-m)})=\iota(\iota(x^{*n})*x^{*(-m)})=\iota(x^{*(-m)})*x^{*n}=f(m)*f(n).$$
\end{proofenv}
\begin{notationenv}
    If $x\in M^\times$,  for any $n\in \ZZ$,  let $x^{*n}$ be the image of $n$ by this unique homomorphism of monoids $(\ZZ, +)\rightarrow (M, *), \ 1\mapsto x$. $x^{\cdot n}$ is denoted as $x^n$,  $x^{+n}$ is denoted as $nx$.
\end{notationenv}
\begin{propositionenv}
    Let $(M, *)$ be a monoid,  $x, y\in M$.
    \newline
    (1) If $x*y=y*x$,  then for any $(n, m)\in \NN^2$, 
     $$x^{*n}*y^{*m}=y^{*m}*x^{*n}.$$ 
     $$(x*y)^{*n}=x^{*n}*y^{*n}.$$
    \newline
    (2) If $x\in M$,  $\iota(x^{*n})=\iota(x)^{*n}$ and for any $(n, m)\in \NN^2$,  with $n\ge m$, 
    $$x^{*(n-m)}=x^{*n}*\iota(x)^{*m}, $$
    $$\iota(x^{*(n-m)})=\iota(x)^{*n}*x^{*m}.$$
\end{propositionenv}
\begin{proofenv}
   \quad
   \newline
    (1) We prove by induction on $n$ such that $x^{*n}*y=y*x^{*n}$.
    If $n=0$,  $x^{*n}=e_M$,  so $y*e_M=y=e_M*y$. If $x^{*n}*y=y*x^{*n}$,  we have $x^{*n+1}*y=x^{*n}*y*x=y*x^{*n}*x=y*x^{*(n+1)}$. We apply this  statement in replacing $n$ by $m$,  $x$ by $y$,  and $y$ by $x^{*n}$. From $x^{*n}*y=y*x^{*n}$,  we deduce that $y^{*m}*x^{*n}=x^{*n}*y^{*m}$. We prove $(x*y)^{*n}=x^{*n}*y^{*n}$ by induction on $n$. If $n=0, e_M=e_M*e_M$. If $n=1, x*y=x*y$. If $(x*y)^{*n}=x^{*n}*y^{*n}$,  then $$(x*y)^{*(n+1)}=(x*y)^{*n}*x*y=x^{*n}*y^{*n}*x*y=x^{*n}*x*y^{*n}*y=x^{*(n+1)}*y^{*(n+1)}.$$
    (2) $x^{*n}*\iota(x)^{*n}=(x*\iota(x))^{*n}=e_{M}^{*n} = e_{M}$,  since $(\NN, +)\rightarrow (M, *),  n\mapsto e_M$ is a homomorphism of monoids. $\iota(x)^n* x^{*n} = (\iota(x) * x)^{*n} = e_{M}$. If $n \geq m$ 
$$
x^{*n} * \iota(x)^{*m} = x^{*(n-m)} * x^{*m} * \iota(x)^{*m} = x^{*(n-m)}
.$$
$$
\iota(x)^{*n} * x^{*m} = \iota(x)^{*(n-m)} * \iota(x)^{*m} * x^{*m}=\iota(x)^{*(n-m)}=\iota(x^{*(n-m)})
.$$
\end{proofenv}
\begin{definitionenv}
    Let $I$ be a set. For any $i\in I$,  let $(M_i, *_i)$ be a set equipped with a composition law. Let 
    $$M=\prod_{i\in I}M_i=\{(x_i)_{i\in I}\mid \forall i\in I, x_i\in M_i\}.$$ 
    We define a composition law on $M$ such that 
    $$(x_i)_{i\in I}*(y_i)_{i\in I}=(x_i*y_i)_{i\in I}.$$
    For any $j\in I$,  let $\pi_j:M\rightarrow M_j, \ (x_i)_{i\in I}\mapsto x_j$.
\end{definitionenv}
\begin{propositionenv}
    \quad 
    \newline
    (1) If $\forall i\in I,  *_i$ is commutative,  then $*$ is communitative.
    \newline
    (2) If $\forall i\in I,  *_i$ is associative,  then $*$ is associative. Moreover,  $\pi_j:(M, *)\rightarrow (M_j, *_j)$ is a homomorphism of semigroups.
    \newline
    (3) If $\forall i\in I$,  $e_i$ is a neutral element of $(M_i, *_i)$,  then $e:=(e_i)_{i\in I}$ is a neutral element of $(M, *)$. Moreover,  if each $(M_i, *_i)$ is a monoid,  then $\pi_j:(M, *)\rightarrow (M_j, *_j)$ is a homomorphism of monoids.
    \newline
    (4) Assume that each $(M_i, *_i)$ is a monoid. Then $M^\times=\prod_{i\in I}M_i^\times$. In particular,  if each $(M_i, *_i)$ is a group,  then $M^\times=\prod_{i\in I}M_i^\times$ is also a group.
\end{propositionenv}
\begin{proofenv}
    If $(x_i)_{i\in I},  (y_i)_{i\in I}\in M$,  then $\pi_j(x*y)=\pi_j((x_i*y_i)_{i\in I})=x_j*_jy_j=\pi_j(x)*\pi_j(y)$.
    \newline
    proof of (4): Assume that $x=(x_i)_{i\in I}\in M^\times$. Then $\exists y=(y_i)_{i\in I}\in M^\times$ such that $x*y=e:=(e_i)_{i\in I}$,  where $e_i$ is the neutral element of $(M_i, *_i)$. $x*y=(x_i*y_i)_{i\in I}=(e_i)_{i\in I}=$. So $x_i*_iy_i=e_i$ for all $i\in I$. Therefore,  $x_i\in M_i^\times$ for all $i\in I$. Hence $M^\times\subseteq\prod_{i\in I}M_i^\times$.
    Now let $x=(x_i)_{i\in I}\in \prod_{i\in I}M_i^\times$. We claim that $(\iota(x_i))_{i\in I}$ is the inverse of $x$. In fact $(x_i)_{i\in I}*(\iota(x_i))_{i\in I}=e$. So $x\in M^\times$.
\end{proofenv}
\begin{theoremenv}
    Suppose that each $(M_i, *_i)$ is a semigroup. Let $(N, \star)$ be a semigroup (resp. monoid,  group). For any $i\in I$,  let $f_i:N\rightarrow M_i$ be a homomorphism of semigroups (resp. monoid,  group). Then there is a unique homomorphism of semigroups (resp. monoid,  group) $f:M\rightarrow N$ such that $\forall i\in I,  \pi_i\circ f=f_i$.
    \newline
    $(M, *)$ is called the \textbf{product} of $(M_i, *_i)$.
\end{theoremenv}
\begin{proofenv}
    By Proposition~\ref{prop:direct-product-factorization},  there exists a unique mapping $f:N\rightarrow M$ such that $\forall i\in I,  \pi_i\circ f=f_i $. We check that $f$ is a homomorphism. 
    \newline
    Recall that $\forall y\in N,  f(y)=(f_i(y))_{i\in I}$. If $(y, z)\in N\times N$,  then $f(y*z)=(f_i(y)*f_i(z))_{i\in I}=(f_i(y))_{i\in I}*(f_i(z))_{i\in I}=f(y)*f(z)$. If each $(M_i, *_i)$ is a monoid with neutral element $e_i$,  and $e_N$ is the neutral element of $N$,  in the case where each $f_i$ is a homomorphism of monoids $(f_i(e_N)=e_i)$. One has $f(e_n)=(f_i(e_N))_{i\in I}=(e_i)_{i\in I}$ is the neutral element of $M$.
\end{proofenv}
\begin{notationenv}
    Let $M$ be a communitative monoid,  $(x_i)_{i\in I}$ be a family of elements in $M$. We suppose that $I_0=\{i\in I\mid x_i\not=e\}$ is finite. We pick a natural number $n$ and a bijection $\sigma:\{1, 2, \dots, n\}\rightarrow I_0$. If the composition law of $M$ is written as $+$,  then 
    $$\sum_{i\in I}x_i \text{ denotes }(x_{\sigma(1)}+x_{\sigma(2)}+\cdots+x_{\sigma(n)}), $$
    it denotes the neutral element $0$ of $M$ when $I_0=\varnothing$. If the composition law of $M$ is written as $\cdot$,  then 
    $$\prod_{i\in I}x_i \text{ denotes }(x_{\sigma(1)}\cdot x_{\sigma(2)}\cdots x_{\sigma(n)}), $$
    it denotes the neutral element $1$ of $M$ when $I_0=\varnothing$.
    \newline
    Let $(M_i)_{i\in I}$ be a family of communitative monoids. (The composition law of $M_i$ is written additively,  the neutral element of $M_i$ is written $0$)
\end{notationenv}
\begin{notationenv}
    Let $(M_i)_{i\in I}$ be a family of communitative monoids. For any $i\in I$,  let $e_i$ be a neutral element of $M_i$. 
    We denote by 
    $$\bigoplus_{i\in I}M_i$$ 
    the set of $(x_i)_{i\in I}\in \prod_{i\in I}M_i$ such that $\{i\in I\mid x_i\not=e_i\}$ is finite. 
\end{notationenv}
\begin{propositionenv}
    $\displaystyle\bigoplus_{i\in I}M_i$ is a submonoid of $\displaystyle\prod_{i\in I}M_i$.
\end{propositionenv}
\begin{proofenv}
    First,  $e:=(e_i)_{i\in I}\in \bigoplus_{i\in I}M_i$. Let $*_i$ be the composition law of $M_i$,  $*$ be the direct product of $(*_i)_{i\in I}$. If $x=(x_i)_{i\in I}$ and $y=(y_i)_{i\in I}$ are in $\bigoplus_{i\in I}M_i$,  then $x*y=(x_i*y_i)_{i\in I}$. If $I_x=\{i\in I\mid x_i\not=e_i\}$ and $I_y=\{i\in I\mid y_i\not=e_i\}$ are finite,  then $\{i\in I\mid x_i*y_i\not=e\}\subseteq I_x\cup I_y$. So $x\in \bigoplus_{i\in I}M_i$ and $y\in \bigoplus_{i\in I}M_i$ imply that $x*y\in\bigoplus_{i\in I}M_i$.
\end{proofenv}
\begin{definitionenv}[Direct sum]
    $\bigoplus_{i\in I}M_i$ is called the \textbf{direct sum} of $(M_i)_{i\in I}$. For any $j\in I$,  the homomorphisms 
    $$\begin{matrix}
M_j\overset{\mathrm{Id}_{M_j}}{\longrightarrow }M_j & \\
M_j\longrightarrow M_i, & (i\not=j)\\
x_j\longmapsto e_i &

\end{matrix}$$
induce:$$\begin{matrix}
M_j\longrightarrow \prod_{i\in I}M_i \\
x_j\longmapsto  (y_i)_{i\in I}

\end{matrix}$$
with
$$y_i=\left\{\begin{matrix}
 x_j, \ j=i\\
e_i, \ i\not=j
\end{matrix}\right. \ .$$
Claim: This homomorphism takes value in $\bigoplus_{i\in I}M_i$. We denote by 
$$\lambda_j:M_j\longrightarrow \bigoplus_{i\in I}M_i$$ 
this homomorphism.
$$\lambda_j(x_j)_i= \left\{\begin{matrix}
x_j, \ i=j\\
e_i, \ i\not=j
\end{matrix}\right. , $$
$$\lambda_j(x_j)_i=(\lambda_j(x_j)_i)_{i\in I}.$$
\end{definitionenv}
\begin{theoremenv}
    Let $(N, \star)$ be a communitative monoid. Then for any $i\in I$,  let $\psi_i:M_i\rightarrow N$ be a homomorphism of monoids. Then there is a unique homomorphism of monoids $\displaystyle\psi:\bigoplus_{i\in I}M_i\rightarrow N$ such that for any $ j\in I, \psi\circ \lambda_j=\psi_j$.
       \begin{center}
\begin{tikzcd}
    \displaystyle\bigoplus_{i\in I}M_i\arrow[r, "\psi"]& N\\
    M_j \arrow[u, "\lambda_j"]\arrow[ur, swap, "\psi_j"]
\end{tikzcd}
\end{center}
\end{theoremenv}
\begin{proofenv}
    For simplicity,  we write all composition laws as $+$,  and all neutral element as $0$. We should define $\displaystyle\psi:\bigoplus_{i\in I}M_i\rightarrow N, \ (x_i)_{i\in I}\mapsto \sum_{i\in I}\psi_i(x_i)$. $\psi\left((0)_{i\in I}\right)=\sum_{i\in I}0=0$. $\psi\left((x_i)_{i\in I}+(y_i)_{i\in I}\right)=\psi\left((x_i+y_i)_{i\in I}\right)=\sum_{i\in I}\psi_i(x_i+y_i)=\sum_{i\in I}\left[\psi_i(x_i)+\psi_i(y_i)\right]=\sum_{i\in I}\psi_i(x_i)+\sum_{i\in I}\psi_i(y_i)$. (The last equality holds because the composition law of $N$ is commutative.)
\end{proofenv}






\chapter{Rings and Modules}
\section{Unitary Rings}
\begin{definitionenv}
    Let $A$ be a set,  and $+$ and $*$ be composition laws. If 
    \newline
    (1) $(A, +)$ forms a communitative group.
    \newline
    (2) $(A, *)$ forms a monoid.
    \newline
    (3) For any $(a, b, c)\in A^3$,  $a*(b+c)=(a*b)+(a*c)$ and $(b+c)*a=(b*a)+(c*a)$.
    \newline
    (4)$^\dagger$ If in addition,  $*$ is communitative,  then we say that the unitary ring $(A, +, *)$ is communitative.
\end{definitionenv}
\begin{exampleenv}
    $(\ZZ, +, \cdot)$ is a unitary ring.
\end{exampleenv}
Note that,  if we denote by $\hat{*}$ the composition law 
$$A\times A\longrightarrow A, $$
$$(a, b)\longmapsto b*a.$$
Then $(A, +, \hat{*})$ forms a unitary ring. We call it the opposite unitary ring of $(A, +, *)$.
\begin{notationenv}
    Usually,  we denote by $+$ the first composition law,  of a unitary ring $A$ and call it the \textbf{addition}. We denote by $0$ the neutral element of $+$,  and call it the \textbf{zero element} of $A$. Usually we denote by $\cdot$ the second composition law of $A$ and call it the \textbf{multiplication}. We denote by $1$ the neutral element with respect to $\cdot$,  and call it the \textbf{unity element} of $A$.
\end{notationenv}
\begin{definitionenv}
    Let $A$ be a unitary ring and $B$ be a subset of $A$. If $B$ is a subgroup of $(A, +)$ and a submonoid of $(A, \cdot)$,  then we call $B$ a \textbf{unitary subring} of $A$.
\end{definitionenv}
\begin{exampleenv}
    Let $\{0\}$ be the set of $1$ element. Let $+$ and $\cdot$ be both the composition law $\{0\}\times\{0\}\rightarrow\{0\}, (0, 0)\mapsto0$. Then $(\{0\}, +, *)$ is a unitary ring. We call it the \textbf{zero ring}.
\end{exampleenv}
\begin{definitionenv}
    Let $A$ and $B$ be unitary rings and $f:A\rightarrow B$ be a mapping. If $f$ is a group homomorphism from $(A, +)$ to $(B, +)$,  and is a monoid homomorphism from $(A, \cdot)$ to $(B, \cdot)$,  then we call $f$ a \textbf{unitary ring homomorphism}. 
\end{definitionenv}
\begin{propositionenv}
    For any unitary ring $A$,  there exists a unitary ring homomorphism $A\rightarrow\{0\}$.
\end{propositionenv}
\begin{lemmaenv}
    Let $A$ be a unitary ring.
    \newline
    (1) $\forall a\in A,  0a=a0=0.$
    \newline
    (2) $\forall a, b\in A,  -(ab)=(-a)b=a(-b).$
\end{lemmaenv}
\begin{proofenv}
    \quad\newline
    (1) $0+0=0$,  so $0+0a=0a=(0+0)a=0a+0a$. Hence $0a=0$.
    \newline
    (2) $ab+(-a)b=(a+(-a))b=0b=0, ab+a(-b)=a(b+(-b))=a0=0.$
\end{proofenv}
\begin{propositionenv}
    For any unitary ring $A$,  there exists a unitary ring homomorphism from $\ZZ$ to $A$.
\end{propositionenv}
\begin{proofenv}
    If $f:\ZZ \rightarrow A$ is a unitary ring homomorphism,  then $f(1)=1_A$. So $f$ is identifies with the unitary group homomorphism.
    $$(\ZZ, +)\longrightarrow(A, +), $$
    $$n\longmapsto n1_A.$$
    It remains to check that for any $(n, m)\in \ZZ^2, f(nm)=f(n)f(m)$. Note that ,  if $(n, m)\in \NN\times\NN$,  then 
    $$f(n)=\underset{n \text{ copies}}{\underbrace{1_A+\dots+1_A}}, \ f(m)=\underset{m \text{ copies}}{\underbrace{1_A+\dots+1_A}}.$$
    So $f(n)f(m)=nm1_A1_A=nm1_A=f(nm)$.
    $f(-n)f(m)=(-f(n))f(m)=-f(n)f(m)=-f(nm)=f(-nm)$.
    $f(-n)f(-m)=\dots$
\end{proofenv}
\begin{definitionenv}
    Let $K$ be a unitary ring. We denote by $K^\times$ the invertible elements of $(K, \cdot)$. If $K^\times=K\backslash\{0\}$ then we say that $K$ is a division ring. If in addition,  $K$ is commutative,  then we say that $K$ is a \textbf{field}.
\end{definitionenv}
\begin{exampleenv}
    $\QQ, \RR, \CC$ are fields.
\end{exampleenv}
\section{Action of Monoids}
\begin{definitionenv}
    Let $(G, *)$ be a monoid,  the neutral element of which is denoted as $e$. Let $X$ be a set. We call \textbf{left action} of $G$ on $X$ any mapping 
    $$\phi:G\times X\rightarrow X, $$
    such that 
    \newline
    (1) $\phi(e, x)=x$,  for any $x\in X$.
    \newline
    (2) $\forall (a, b)\in G\times G, \forall x\in X$, 
    $$\phi(a*b, x)=\phi(a, \phi(b, x)).$$
    (Resp. right action)
\end{definitionenv}
\begin{remark}
    A left action of $(G, *)$ on $X$ is a right action of $(G, \hat{*})$ on $X$.
\end{remark}
\begin{notationenv}
    If $*=\cdot$,  a left action is usually denoted as 
    $$G\times X\longrightarrow X, $$
    $$(a, x) \longmapsto ax.$$
    Condition (1) becomes $ex=x$,  (2) becomes $(ab)x=a(bx)$.
\end{notationenv}
\begin{exampleenv}
    Let $G$ be a group,  $H$ be a subgroup of $G$. Then 
    $$H\times G\longrightarrow G, $$
    $$(h, g)\longmapsto hg.$$
    is a left action of $H$ on $G$. (Resp. right action.)
\end{exampleenv}
\begin{propositionenv}
    Let $G$ be a monoid,  $X$ be a set and $\phi:G\times X\longrightarrow X$ be a left action of $G$ on $X$. We define a binary relation $\sim_{\phi}$ on $X$ as follows:
    $$x\sim_{\phi}y\Leftrightarrow \exists g\in G, \phi(g, x)=y.$$
Then $\sim_{\phi}$ is reflexive and transitive. It is an equivalence relation if $G$ is a group.
\end{propositionenv}
\begin{proofenv}
    \quad\newline
    Reflexivity: Let $e$ be the neutral element of $G$,  then $x=ex$,  so $x\sim_{\phi}x$.
    \newline
    Transitivity: If $y=ax$ and $z=by$,  then $z=b(ax)=(ba)x$,  so $x\sim_{\phi}y \wedge y\sim_{\phi}z \Rightarrow x\sim_{\phi}z$.
    \newline
    Assume that $G$ is a group. If $y=ax$,  then $\iota(a)y=\iota(a)(ax)=(\iota(a)a)x=ex=x$,  so $x\sim_{\phi}y$ implies $y\sim_{\phi}x$.
\end{proofenv}
\begin{definitionenv}
    Let $G$ be a group,  $X$ be a set and $\phi:G\times X\longrightarrow X$ be a left action. For any $x\in X$,  the equivalence class of $x$ under the equivalence relation $\sim_{\phi}$ is called the \textbf{orbit} of $x$ under the action $\phi$,  denoted as $\mathrm{orb}_\phi(x)$. We denote by $G\backslash X$ the set of all orbits of $X$ under the action $\phi$. (Resp. right action and $X/G$.)
\end{definitionenv}
\begin{remark}
    If $X$ is finite,  then 
    $$\mathrm{card}(X)=\sum_{A\in G\backslash X}\mathrm{card}(A).$$
    In particular,  if $(G, *)$ is a finite group,  and $H$ is a subgroup of $G$,  then $\mathrm{card}(G)=\mathrm{card}(H)\mathrm{card}(H\backslash G).$ In fact,  $H\backslash G=\{H*x\mid x\in G\}, H*x:=\{h*x\mid h\in H\}.$ 
\end{remark}


\section{Vector Space}
\begin{definitionenv}
    Let $K$ be a unitary ring. Let $(V, +)$ be an abelian group. (Neutral element of $(V, +)$ is denote as $0$.) We call a \textbf{left K-module structure} any left action of $(K, \cdot)$ on $V$. 
    $$\phi:K\times V\longrightarrow V$$
    (1) $\forall (a, b)\in K\times K, \forall x\in V$, $$\phi(a+b, x)=\phi(a, x)+\phi(b, x).$$
    (2) $\forall a\in K$,  $\forall (x, y)\in V\times V$, $$\phi(a, x+y)=\phi(a, x)+\phi(a, y).$$
    The abelian group $(V, +)$ equipped with a left $K$-module structure is called a \textbf{left K-module}. If $K$ is communitative,  left and right $K$-modules structures have the same axioms. So we just call them $K$-module structures. Left and right $K$-modules structures are called $K$-modules.
    If $K$ is a field,  a $K$-module is called a \textbf{vector space} over $K$.
\end{definitionenv}
\begin{exampleenv}
    $(\{0\}, +)$ is a left $K$-module. Action 
    $$\phi:K\times \{0\}\longrightarrow \{0\}, $$
    $$\phi(a, 0)=0.$$
    It is called the zero K-module.
\end{exampleenv}
\begin{exampleenv}
    Consider the action 
    $$\phi:K\times K\longrightarrow K, $$
    $$\phi(a, x)=ax.$$
    $\phi$ defines a left $K$-module structure on $K$.
\end{exampleenv}
\begin{definitionenv}
    Let $I$ be a set and $(V_i)_{i\in I}$ be a family of left $K$-modules.
    $$V=\prod_{i\in I}V_i.$$
    The action 
    $$\phi:(K\times V)\longrightarrow V, $$
    $$\ (a, (x_i)_{i\in I})\longmapsto (a*x_i)_{i\in I}$$
    defines a left $K$-module structure on $V$.
\end{definitionenv}
\section{Submodules}
\begin{definitionenv}
    Let $V$ be a left $K$-module,  we call \textbf{left sub-K-module} of $V$ any subgroup $W$ of $(V, +)$ such that for any $(a, x)\in K\times W, ax\in W$. (resp. right.)
\end{definitionenv}
\begin{exampleenv}
    $\{0\}$ and $V$ itself is a left sub-$K$-modules of $V$.
\end{exampleenv}
\begin{definitionenv}
    Let $E$ and $F$ be left-K-modules. We call \textbf{homomorphism of left K-modules from $E$ to $F$} any mapping $f:E\rightarrow F$,  such that 
    \newline
    (1) $f$ is a homomorphism of groups from $(E, +)$ to $(F, +)$.
    \newline
    (2) For any $(a, x)\in K\times E, f(ax)=af(x)$.
    \newline
    If $K$ is communitative,  a homomorphism of $K$-module is also called a \textbf{$K$-linear mapping}.
\end{definitionenv}
\begin{lemmaenv}
    Let $V$ be a left K-module.
    \newline
    (1) $\forall a\in K, a0_V=0_V$.
    \newline
    (2) $\forall x\in V, 0x=0_V$.
\end{lemmaenv}
\begin{proofenv}
    \quad
    \newline
    (1) $a0_V=a(0_V+0_V)=a0_V+a0_V\Rightarrow 0_V=a0_V$.
    \newline
    (2) $0x=(0+0)x=0x+0x\Rightarrow 0x=0_V$.
\end{proofenv}
\begin{theoremenv}
    Let $f:E\rightarrow F$ be a homomorphism of left-K-modules.
    \newline 
    (1) $\ker(f)$ is a left sub-K-module of $E$.
    \newline
    (2) $\mathrm{Im}(f)$ is a left sub-K-module of $F$. 
\end{theoremenv}
\begin{proofenv}
    First,  $\ker(f)$ is a subgroup of $E$,  $\mathrm{Im}(f)$ is a subgroup of $F$.
    \newline
    (1) Let $a\in K,  x\in \ker(f),  f(ax)=af(x)=a0_V=0_V$. So $ax\in \ker(f)$.
    \newline
    (2) Let $y\in \mathrm{Im}(f)$,  there exists $x\in E$ such that $f(x)=y$. For any $a\in K, ay=af(x)=f(ax)\in \mathrm{Im}(f)$
\end{proofenv}
\begin{propositionenv}
    Let $V$ be a left K-module. For any $x\in V,  -x=(-1)x$.
\end{propositionenv}
\begin{proofenv}
    $$(-1)x+x=(-1+1)x=0x=0_V.$$
\end{proofenv}
\begin{exampleenv}
    Let $(V_i)_{i\in I}$ be a family of left K-modules. We denote by 
    $$\bigoplus_{i\in I}V_i\text{ the set of }(x_i)_{i\in I}\in \prod_{i\in I}V_i, $$
    such that $\{i\in I\mid x_i\not=0_{V_i}\}$. This is a subgroup of $\prod_{i\in I}V_i$. For any $a\in K, $ and $(x_i)_{i\in I}\in \bigoplus_{i\in I}V_i$, 
    $$\{i\in I\mid ax_i\not=0_{V_i}\}\subseteq\{i\in I\mid x_i\not=0_{V_i}\}.$$
    So $\displaystyle a(x_i)_{i\in I}=(ax_i)_{i\in I}\in \bigoplus_{i\in i}V_i$,  which means that $\displaystyle\bigoplus_{i\in I}V_i$ is a left sub-K-module of $\displaystyle\prod_{i\in I}V_i$. $\displaystyle\bigoplus_{i\in I}V_i$ is called the direct sum of $(V_i)_{i\in I}$. We denote by 
    $$K^{\oplus I}$$
    the left sub-K-module of $K^I$.
\end{exampleenv}
\begin{propositionenv}
    Let $E$ and $F$ be left K-modules, $f:E\rightarrow F$ be a mapping.
    \newline
    (1) If $f$ is a homomorphism of left K-modules,  for any $n\in \NN_{\ge1}$,  any $(a_1, a_2, \dots, a_n)\in K$,  and $(x_1, x_2, \dots, x_n)\in E^n$, 
    $$f(a_1x_1+\dots+a_nx_n)=a_1f(x_1)+\dots+a_nf(x_n).$$
    (2) Suppose that for any $a\in K, (x, y)\in E^2$, 
    $$f(x+ay)=f(x)+af(y).$$
    Then $f$ is a homomorphism of left K-modules.
\end{propositionenv}
\begin{proofenv}
    (1) Induction on $n$.
    \newline
    (2) Take $a=1$,  for any $(x, y)\in E, f(x+y)=f(x)+f(y)$. 
    
    \ \ \ \ \ \ Take $x=0_E, f(ay)=0_F+af(y)=af(y)$.
\end{proofenv}
\begin{definitionenv}
    If a left K-module homomorphism is a bijection we say that it is a \textbf{left K-module isomorphism}.
\end{definitionenv}
\section{Universal Property}
\begin{propositionenv}
    Let $(V, +)$ be a communitative group. Then 
    $$\begin{matrix}
        \ZZ\times V\longrightarrow V\\
        (n, x)\longmapsto nx
    \end{matrix}$$
    defines a $\ZZ$-module substructure on $V$.
\end{propositionenv}
\begin{proofenv}
    First,  $nx$ is the image of $n$ by the unique homomorphism of groups $\phi_x:\ZZ\rightarrow V, 1\mapsto x$.
    $$(n+m)x=\phi_x(n+m)=\phi_x(n)+\phi_x(m)=nx+mx.$$
     Let $(x, y)\in V^2$, 
    \begin{align*}
        \phi_x+\phi_y:&\ZZ \longrightarrow V, \\
        &n\longmapsto \phi_x(n)+\phi_y(n)=nx+ny
    \end{align*}
    is a homomorphism of groups,  since for any $(n, m)\in \ZZ^2$
    \begin{align*}
        &(\phi_x+\phi_y)(n+m)\\
        =&\phi_x(n+m)+\phi_y(n+m)=\phi_x(n)+\phi_x(m)+\phi_y(n)+\phi_y(m)\\
        =&(\phi_x(n)+\phi_y(n))+(\phi_x(m)+\phi_y(m)).
    \end{align*}
    Since $(\phi_x+\phi_y)(1)=x+y=\phi_{x+y}, \phi_{x+y}=\phi_x+\phi_y.$ So $n(x+y)=nx+ny,  \forall n\in\ZZ.$ $1x=\phi_x(1)=x.$
    If $n\in \NN$, 
    $$(nm)x=\phi_x(nm)=\phi_x(\underset{n \text{ copies}}{\underbrace{m+\dots+m}})=n\phi_x(m)=n(mx).$$
    If $-n\in \NN$, 
    $$\phi_x(nm)=-\phi_x((-n)m)=-(-n)\phi(m)=n\phi_x(m).$$
\end{proofenv}
\begin{propositionenv}
    Let $V$ be a left K-module,  $x\in V$. There exists a unique homomorphism of left K-modules 
    $\phi_x:K\longrightarrow V, $
    such that $\phi_x(1)=x$.
\end{propositionenv}
\begin{proofenv}
    If $\phi_x$ exists,  then it should satisfy 
    $$\forall a\in K, \phi_x(a)=a\phi_x(1)=ax.$$
    It suffices to check that $\phi_x:K\rightarrow V, a\mapsto ax$ is a homomorphism.
    $$\phi_x(a+b)=(a+b)x=ax+bx=\phi_x(a)+\phi_x(b), $$
    $$\phi_x(\lambda a)=(\lambda a)x=\lambda(ax)=\lambda\phi_x(a).$$
\end{proofenv}
\begin{propositionenv}
    Let $(V_i)_{i\in I}$ be a family of left K-modules.
    \newline
    (1) Let $W$ be a left K-module. For any $i\in I$,  let $f_i:W\rightarrow V_i$ be aa homomorphism. Then there exists a unique homomorphism 
    $$f:W\longrightarrow \prod_{i\in I}V_i, $$ 
    such that 
    $$\forall i\in I, \pi_i\circ f=f_i, $$
    where $\pi_i$ sends $(x_j)_{j\in I}\in \prod_{j\in I}V_j$ to $x_i$.
    \newline
    (2) Let $W$ be a left K-module,  for any $i\in I$,  let $g_i:V_i\rightarrow W$ be a homomorphism of left K-modules. There exists a unique homomorphism
    $$g:\bigoplus_{i\in I}V_i\longrightarrow W$$
    such that 
    $$\forall i\in I,  g\circ\lambda_i=g_i, $$
    where
    $$\lambda_j:V_j\longrightarrow\bigoplus_{i\in I}V_i, $$
    $$x_j\longrightarrow (y_i)_{i\in I} \text{ with } y_i=\left\{\begin{matrix}
        x_j, i=j\\0, i\not=j
    \end{matrix}\right. .$$
\end{propositionenv}
\begin{proofenv}
    \quad
    \newline
    (1) There exists a unique mapping $f:W\rightarrow \prod_{i\in I}V_i$,  such that 
    $$\forall i\in I, \pi_i\circ f=f_i, $$
    $$\forall z\in W, f(z)=(f_i(z))_{i\in I}.$$
    We have proved that $f$ is a homomorphism of groups.
    $$\forall a\in K, z\in W. f(az)=(f_i(az))_{i\in I}=(af_i(z))_{i\in I}=af(z).$$
    (2) We have prove that there exists a unique $g:\bigoplus_{i\in I}V_i\rightarrow W$ homomorphism of group such that $\forall i\in I,  g\circ \lambda_i=g_i.$ $g\left((x_i)_{i\in I}\right)=\sum_{i\in I}g_i(x_i)$.
    \begin{align*}
        \forall a\in K, g\left(a(x_i)_{i\in I}\right)=&g\left((ax_i)_{i\in I}\right)\\
        =&\sum_{i\in I}g_i(ax_i)=\sum_{i\in I}ag_i(x_i)
        =a\sum_{i\in I}g_i(x_i)=ag(x).
    \end{align*}
\end{proofenv}
\begin{applicationenv}
Let $V$ be a left K-module. Let $I$ be a set and $(x_i)_{i\in I}\in V^{I}$. For any $i\in I$,  let 
$$\phi_{x_i}:K\longrightarrow V, a\mapsto ax_i.$$ 
So the family $(\phi_{x_i})_{i\in I}$ determines a homomorphism of left K-modules 
$$\Phi:K^{\oplus I}\longrightarrow V, $$
$$(a_i)_{i\in I}\longmapsto \sum_{i\in I}\phi_{x_i}(a_i)=\sum_{i\in I}a_ix_i.$$
\end{applicationenv}

\section{Matrices}
\begin{definitionenv}
    Let $n\in \NN$. Let $V$ be a \underline{\textbf{left}} K-module. For any $(x_1, \dots, x_n)\in V^n$,  we denote by 
    $$\begin{pmatrix}
x_1 \\
 \vdots \\
x_n
\end{pmatrix}
:
K^n\longrightarrow V, $$
$$(a_1, \dots, a_n)\longmapsto a_1x_1+\dots+a_nx_n. $$
This is a homomorphism of left K-modules.
\end{definitionenv}
\begin{exampleenv}
    Consider the case where $V=K^p$ with $p\in \NN$. Each $x_i$ is of the form $(b_{i, 1}, \dots, b_{i, p})$.
    $$\text{So } \begin{pmatrix}
x_1 \\
 \vdots \\
x_n
\end{pmatrix}
\text{ becomes}
\begin{pmatrix}
  b_{1, 1}& \dots  &b_{1, p} \\
  \vdots & \ddots  & \vdots \\
  b_{n, 1}& \dots &b_{n, p}
\end{pmatrix} .$$
\end{exampleenv}
\begin{definitionenv}
    We call $n$ by $p$ matrix with coefficients in $K$ any homomorphism of left K-module from $K^n$ to $K^p$.
\end{definitionenv}
\begin{definitionenv}
    Let $n$ and $p$ be natural numbers,and $V$ be a left K-module. Let $A:K^n\rightarrow K^p$, and $\varphi:K^p\rightarrow V$ be homomorphism of left K-modules. We denote by 
    $$A\varphi:K^n\longrightarrow V $$
    be the mapping $\varphi\circ A.$
\end{definitionenv}
\begin{propositionenv}
    Let $E,F$ and $G$ be left K-modules. Let $\varphi:E\rightarrow F$ and $\psi:F\rightarrow G$ be homomorphism of left K-modules. Then $(\varphi\circ \psi):E\rightarrow G$ is a homomorphism of left K-modules.
\end{propositionenv}
\begin{proofenv}
    Let $(x,y)\in E^2, a\in K. (\psi\circ\phi)(x+ay)=\psi(\varphi(x+ay))=\psi(\varphi(x)+a\varphi(y))=\psi(\varphi(x))+a\psi(\varphi(y))$.
\end{proofenv}


\begin{box2}
\textbf{Computation}\quad Suppose that 
$$A=\begin{pmatrix}  
  a_{1,1} & \cdots & a_{1,p} \\  
  \vdots & \ddots & \vdots \\  
  a_{n,1} & \cdots & a_{n,p}  
\end{pmatrix} ,\ \varphi =\begin{pmatrix}
 x_1\\
 \vdots\\
x_p

\end{pmatrix}.$$
For $t=(t_1, \dots, t_n)\in K^n$, 
$$t\overset{A}{\longmapsto}\left(\sum_{i=1}^{n}t_ia_{i,1},\dots,\sum_{i=1}^{n}t_ia_{i,p} \right)\overset{\varphi}{\longmapsto}\sum_{j=1}^{p}\sum_{i=1}^{n}t_ia_{i,j}x_j.$$
So, 
$$A\varphi=\begin{pmatrix}
    a_{1,1}x_1+\dots+a_{1,p}x_p\\
    \vdots\\
    a_{n,1}x_1+\dots+a_{n,p}x_p.
\end{pmatrix}$$
\end{box2}
\begin{box2}
    \textbf{Question}\quad Let 
    $$A=\begin{pmatrix}  
  a_{1,1} & \cdots & a_{1,p} \\  
  \vdots & \ddots & \vdots \\  
  a_{n,1} & \cdots & a_{n,p}  
\end{pmatrix} ,B=\begin{pmatrix}  
  b_{1,1} & \cdots & b_{1,q} \\  
  \vdots & \ddots & \vdots \\  
  b_{p,1} & \cdots & b_{p,q}  
\end{pmatrix} . \ AB=? $$
    We have
    $$AB=\begin{pmatrix}  
  a_{1,1}b_{1,1}+\cdots+a_{1,p}b_{p,1} & \cdots & a_{1,1}b_{p,1}+\cdots+a_{1,p}b_{p,q} \\  
  \vdots & \ddots & \vdots \\  
  a_{n,1}b_{1,1}+\cdots+a_{n,p}b_{p,1} & \cdots & a_{n,1}b_{p,1}+\cdots+a_{n,p}b_{p,q}  
\end{pmatrix} .$$
\end{box2}
\begin{exampleenv}
    Let $(a_1,a_2,\dots,a_n)\in K^n$, we denote by 
    \begin{align*}
        \mathrm{diag}(a_1,\dots,a_n):K^n&\longrightarrow K^n\\
        (t_1,\dots,t_n)&\longmapsto(t_1a_1,\dots,t_na_n).
    \end{align*}
    $\mathrm{diag}(a_1,\dots,a_n)$ is called a \textbf{diagonal matrix}.
\end{exampleenv}
\begin{exampleenv}
     $\mathrm{Id}_{K^n}:K^n\longrightarrow K^n,\ t\mapsto t$ is also written as $I_n=\begin{pmatrix}
  1&  & \\
  & \ddots  & \\
  &  &1
\end{pmatrix}$

Let $V$ be a left K-module, $(x_1,\dots,x_n)\in V^n, (a_1,\dots, a_n)\in K^n$.
$$\mathrm{diag}(a_1,\dots,a_n)\begin{pmatrix}
    x_1\\
    \vdots\\
    x_n
\end{pmatrix}=\begin{pmatrix}
    a_1x_1\\
    \vdots\\
    a_nx_n
\end{pmatrix}.$$
$$\mathrm{diag}(a_1,\dots,a_n)\mathrm{diag}(b_1,\dots,b_n)=\mathrm{diag}(a_1b_1,\dots,a_nb_n).$$
\end{exampleenv}





\section{Linear Equations}
{\large\textit{We fix a unitary ring.}}
\begin{definitionenv}
    Let $p\in \NN$. For $(a_1,\dots,a_p)\in K^p$, let $j(a_1,\dots,a_p)$ be the least index $i\in\{1,\dots,p\}$ such that $a_i\not=0$. By convention,
    $$j(0,\dots,0)=p+1.$$
    Let $V$ be a left K-module, $A\in M_{n,p}(K)$. Let $(b_1,\dots , b_n)\in V^n$. We consider
\begin{equation*}
A\begin{pmatrix}
 x_1\\
 \vdots\\
x_p
\end{pmatrix}=\begin{pmatrix}
 b_1\\
 \vdots\\
b_n
\end{pmatrix}\tag{$*$}
\end{equation*}
We write $A$ into the form
$$\begin{pmatrix}
 \vec{a}^{(1)} \\
 \vdots\\
 \vec{a}^{(n)}
\end{pmatrix}, \vec{a}^{(i)}=(a_{i,1},\dots,a_{i,p}).$$
\end{definitionenv}
\begin{definitionenv}
    We say that the matrix is of row echelon form if 
    $$j\left(\vec{a}^{(1)}\right)\le j\left(\vec{a}^{(2)}\right)\le \dots\le j\left(\vec{a}^{(n)}\right),$$
    and the strict inequality holds once $$j\left(\vec{a}^{(i)}\right)\le p.$$
    If in addition $a_{i,j\left(\vec{a}^{(i)}\right)}=1$,  and $a_{k,j\left(\vec{a}^{(i)}\right)}=0$ for any $k\not=i$ once $\vec{a}^{(i)}\not=(0,\dots,0)$. We say that $A$ is of \textbf{reduced row echelon form}. 
\end{definitionenv}
\begin{exampleenv}
    $$
    \begin{pmatrix}
        1&0\\
        0&1
    \end{pmatrix},\ 
\begin{pmatrix}
  1&2  &3  &4  &5 \\
  0& 0 &2  &1  &0 \\
  0& 0 &0  & 0 &0 \\
  0& 0 &  0&  0&0
\end{pmatrix}$$ are of row echelon form.
\end{exampleenv}
\begin{theoremenv}
    Suppose that $A$ is of reduced echelon form. Let 
    $$I(A)=\{i\in \{1,\dots,n\}\mid \vec{a}^{(i)}\not=(0,\dots,0)\},$$
    $$J_0(A)=\{1,\dots,p\}\backslash\{j\left(\vec{a}^{(i)}\right)\mid i\in I(A)\}.$$
    (1) If there exists $i\in \{1,\dots,n\}\backslash I(A),b_i\not=0$ the equation $A\begin{pmatrix}
 x_1\\
 \vdots\\
x_p
\end{pmatrix}=
\begin{pmatrix}
 b_1\\
 \vdots\\
b_n
\end{pmatrix}$ has no solution.
\newline
(2) If $\forall i\in\{1,\dots ,n\}\backslash I(A),b_i=0$. The solution set of the equation is the image of the following mapping:
$$\Phi:V^{I(A)}\longrightarrow V^p \text{ with}$$
$$(z_l)_{l\in J_0(A)}\longmapsto (x_1,\dots,x_p),$$
$$x_k=\left\{ \begin{matrix}
    z_k &,\text{if } k\in J_0(A)\\
    b_i-\sum_{l\in J_0(A)}a_{i,l}z_l &,\text{if } k=j\left(\vec{a}^{(i)}\right) .
\end{matrix}\right.$$

\end{theoremenv}
\begin{propositionenv}
    Let $m,n,p$ be natural numbers. $S\in M_{m,n}(K), A\in M_{n,p}$. If $(x_1,\dots,x_p)$ is a solution of the equation
    \begin{equation*}
        A\begin{pmatrix}
        x_1\\
        \vdots\\
        x_p
        \end{pmatrix}=\begin{pmatrix}
        b_1\\
        \vdots\\
        b_n
        \end{pmatrix}.\tag{$*$}
    \end{equation*}
    Then it is also a solution of the equation
    \begin{equation*}(SA)\begin{pmatrix}
        x_1\\
        \vdots\\
        x_p
        \end{pmatrix}=S\begin{pmatrix}
        b_1\\
        \vdots\\
        b_n
        \end{pmatrix}.\tag{$*_S$}
        \end{equation*}
    Moreover, if $S$ is left invertible ( namely there exists $T\in M_{n,m}(K)$ such that $TS=I_n$), then ($*$) and ($*_S$) have the same solution set.
\end{propositionenv}
\begin{proofenv}
     \begin{equation*}
        (SA)\begin{pmatrix}
        x_1\\
        \vdots\\
        x_p
        \end{pmatrix}=S\begin{pmatrix}
        b_1\\
        \vdots\\
        b_n
        \end{pmatrix}.
    \end{equation*}
    So 
     \begin{equation*}
        TSA\begin{pmatrix}
        x_1\\
        \vdots\\
        x_p
        \end{pmatrix}=TS\begin{pmatrix}
        b_1\\
        \vdots\\
        b_n
        \end{pmatrix}
        \Rightarrow
        A\begin{pmatrix}
        x_1\\        
        \vdots\\
        x_p
        \end{pmatrix}=\begin{pmatrix}
        b_1\\
        \vdots\\
        b_n
        \end{pmatrix}.
    \end{equation*}
\end{proofenv}
\begin{definitionenv}
    Let $n\in\NN$ and $\sigma:\{1,\dots,n\}\rightarrow\{1,\dots,n\}$ be a bijection. Denote by 
    $$P_\sigma:K^n\longrightarrow K^n,$$
    $$P(t_1,\dots,t_n):=\left(t_{\sigma^{-1}(1)},\dots,t_{\sigma^{-1}(n)}\right).$$
    $P_\sigma$ is a homomorphism of left K-modules.
\end{definitionenv}
\begin{box2}
    $$P_\sigma P_{\sigma^{-1}}=P_{\sigma^{-1}}P_\sigma=I_{n}.$$
    Let $V$ be a left K-module, $(x_1,\dots,x_n)\in V$,
    $$P_\sigma\begin{pmatrix}
 x_1\\
 \vdots\\
x_n
\end{pmatrix}
:K^n\longrightarrow V,$$
$$(t_1,\dots,t_n)\overset{P_\sigma}{\longmapsto}(t_{\sigma^{-1}(1)},\dots,t_{\sigma^{-1}(n)})\overset{\begin{pmatrix}
 x_1\\
 \vdots\\
x_n
\end{pmatrix}}{\longmapsto}\sum_{i=1}^{n}t_{\sigma^{-1}(i)}x_i=\sum_{j=1}^{n}t_jx_{\sigma(j)}.$$
$$P_\sigma\begin{pmatrix}
    x_1\\
    \vdots\\
    x_n
\end{pmatrix}=\begin{pmatrix}
    x_{\sigma(1)}\\
    \vdots\\
    x_{\sigma(n)}
\end{pmatrix},\ P_\sigma\begin{pmatrix}
    e_1\\
    \vdots\\
    e_n
\end{pmatrix}=\begin{pmatrix}
    e_{\sigma(1)}\\
    \vdots\\
    e_{\sigma(n)}
\end{pmatrix}$$
\end{box2}
\begin{definitionenv}
    If $\underline{r}=(r_1,r_2,\dots,r_n)\in K^n$, we denote by $D_{\underline{r}}$ the matrix $\mathrm{diag}(r_1,\dots,r_n)$.
    If for any $i\in \{1,\dots,n\}$, $r_i$ is left invertible and is a inverse of $s_i$, then 
    $$D_{\underline{s}}D_{\underline{r}}=I_n.$$
\end{definitionenv}
\begin{definitionenv}
    Let $n\in \NN, i\in \{1,\dots,n\},c=\left(c_1,\dots,c_n\right)\in K^n,c_i=0$. Denote by 
    $$S_{i,c}:K^n\longrightarrow K^n,$$
    $$S_{i,c}(t_1,\dots,t_n):=\left(t_1,\dots,t_{i-1},t_i+\sum_{j=1}^{n}t_jc_j,t_{i+1},\dots,t_n\right)$$
\end{definitionenv}
\begin{box2}
    $$S_{i,c}S_{i,-c}=S_{i,-c}S_{i,c}=I_n$$
    $$S_{i,c}\begin{pmatrix}
 x_1\\
 \vdots\\
x_n
\end{pmatrix}:\left ( t_1,\dots,t_n \right ) \longmapsto\sum_{j=1}^{n}t_jx_j+\sum_{j=1}^{n} t_jc_jx_i $$
$$S_{i,c}\begin{pmatrix}
 x_1\\
 \vdots\\
x_n
\end{pmatrix}=\begin{pmatrix}
 x_1+c_1x_i\\
 \vdots\\
 x_i\\
 \vdots\\
x_n+c_nx_i
\end{pmatrix}$$
\end{box2}
\begin{definitionenv}
    Let $G_n(K)$ be the subset of $M_{n,n}(K)$ consisting of matrices $S$, that can be written as $U_1,\dots,U_N$, where $N\in\NN$ (if $N=0$, by convention, $S=I_n$) and each $U_i$ is of the following forms:
    \newline
    (1) $P_\sigma$, with $\sigma:\{1,\dots,n\}\rightarrow\{1,\dots,n\}$ being a bijection.
    \newline
    (2) $D_{\underline{r}}$ with each $r_i$ being left invertible.
    \newline
    (3) $S_{i,c}$ with $i\in \{1,\dots,n\},c=\left(c_1,\dots,c_n\right)\in K^n,c_i=0$.
    \newline
    Let $p\in\NN$. We say that $A\in M_{n,p}(K)$ is \textbf{reducible by Gaussian elimination} if there exists $S\in G_n(K)$ such that $SA$ is of reduced row echelon form.
\end{definitionenv}
\begin{lemmaenv}
    If $A\in M_{n,p}(K)$ is such that $SA$ is reducible by Gaussian elimination, for some $S\in G_n(K)$, then $A$ is also reducible by Gaussian elimination.
\end{lemmaenv}
\begin{theoremenv}
    Suppose that $K$ is a division ring. For any $(n,p)\in\NN^2$, any matrix $A\in M_{n,p}(K)$ is reducible
    by Gaussian elimination.
\end{theoremenv}
\begin{proofenv}
    We reason by induction on $p$.
    \newline
    $p=0$. $A$ is already of reduced row echelon form.
    \newline
    Suppose that the statement is true for matrices of at most $p-1$ columns. ($p\ge 1$) We write $A$ as $\begin{pmatrix}
 \begin{matrix}
 \lambda _1\\
 \vdots\\
\lambda _n
\end{matrix} &B&
\end{pmatrix}$ where $B\in M_{n,p-1}(K)$. If $\lambda_1=\dots=\lambda_n=0$. By induction hypothesis, there exists $S\in G_n(K)$ such that $SB$ is of reduced row echelon form.
$$SA= \begin{pmatrix}\begin{matrix}
0\\
 \vdots\\
0
\end{matrix} &SB&
\end{pmatrix}$$
 is of reduced row echelon form. If $(\lambda_1,\dots,\lambda_n)\not=(0,\dots,0)$, by the lemma, we may suppose that $\lambda_1\not=0$ (By permuting rows). By multiplying $A$ by $\mathrm{diag}(\lambda_1^{-1},1,\dots,1)$ we may assume (by the lemma) that $\lambda_1=1$. So 
 $$A= \begin{pmatrix}\begin{matrix}
 1\\
\lambda _2\\
 \vdots\\
\lambda _n
\end{matrix} &&B&&
\end{pmatrix}.$$
By multiplying $S_{1,(0,-\lambda_2,\dots,-\lambda_n)}$ and $A$, we may assume (by the lemma) that $A$ is of the form 
$$\begin{pmatrix}
 1 &\begin{matrix}
  \mu_2& \dots &\mu_n
\end{matrix} \\
 \begin{matrix}
 0\\
 \vdots\\
0
\end{matrix} &C
\end{pmatrix}.$$
Applying the induction hypothesis to $C$.
(For any $T\in G_{n-1}(K), T:K^{n-1}\rightarrow K^{n-1}, S:K^n\rightarrow K^n$, $S(t_1,\dots,t_n)=(t_1,T(t_2,\dots,t_n))$ belongs to $G_n(K).$)
We write $C$ as $\begin{pmatrix}
    c_2\\
    \vdots\\
    c_n
\end{pmatrix}$
where $c_2,\dots,c_k$ belong to $k^{p-1}\backslash\{(0,\dots,0)\},c_{k+1}=\dots=c_n=(0,\dots,0),j(c_2)<\dots<j(c_k)$. For any $i\in \{2,\dots,k\}$, we multiply $-\mu_{j(c_i)}$ times the $i^{\text{th}}$ row of $A$ to the first row. The result is a matrix of reduced row echelon form.
\end{proofenv}


\section{Quotient Modules}
\textit{Let $K$ be a unitary ring.}
\begin{propositionenv}
    Let $E$ be a left K-module, $F$ be a left sub-K-module of $E$. The mapping
    $$K\times E/ F\longrightarrow E/ F,$$
    $$(a,[x])\longmapsto [ax]$$
    (Resp. right,$[xa]$) is well defined, and determines a structure of left $K$-module on $E/ F$. Moreover, the projection mapping
    $$\pi:E\longrightarrow E/ F$$
    $$x\longmapsto [x]$$
    is a homomorphism.
\end{propositionenv}
\begin{proofenv}
    Recall that $F$ is a subgroup of $(E,+)$ such that 
    $$\forall a\in K, \forall y\in F, ay\in F,$$
    $$[x]=\{y\in E\mid y-x\in F\}.$$
    If $[x]=[y]$, then $y-x\in F$, so $ay-ax=a(y-x)\in F$, which means $[ay]=[ax]$.
    \newline
    (1) $[1x]=[x]$.
    \newline
    (2) $(ab)[x]=[(ab)x]=[a(bx)]=a[bx]=a(b[x])$.
    \newline
    (3) $$(a+b)[x]=[(a+b)x]=[ax+bx]=[ax]+[bx]=a[x]+b[x].$$
    $$a[x+y]=[a(x+y)]=[ax+ay]=[ax]+[ay]=a[x]+a[y].$$
    Finally,
    $$\pi(x+ay)=[x+ay]=[x]+[ay]=[x]+a[y]=\pi(x)+a\pi(y).$$
\end{proofenv}
\begin{theoremenv}
    Let $f:V\rightarrow W$ be a homomorphism of left $K$-modules.
    \newline
    (1) $\mathrm{Im}(f)$ is a sub-$K$-module of $W$.
    \newline
    (2) $\ker(f)$ is a sub-$K$-module of $V$.
    \newline
    (3) $\tilde{f}:V/\ker(f)\longrightarrow W,[x]\longmapsto f(x)$ is a homomorphism of left $K$-modules. Moreover, as a mapping, $\tilde{f}$ is injective and has $\mathrm{Im}(f)$ as its range. Hence it defines an isomorphism between $V/\ker(f)$ and $\mathrm{Im}(f)$.
\end{theoremenv}
\begin{proofenv}
    \quad
    \newline
    (1) We have proved that $\mathrm{Im}(f)$ is a subgroup of $W$. If $y=f(x)\in \mathrm{Im}(f), \forall a\in K, ay=af(x)=f(ax)\in \mathrm{Im}(f)$. So $\mathrm{Im}(f)$ is a left sub-$K$-module of $W$.
    \newline
    (2) We have proved that $\ker(f)$ is a subgroup of $V$. If $x\in \ker(f), \forall a\in K, f(ax)=af(x)=a0=0$. So $\ker(f)$ is a left sub-$K$-module of $V$.
    \newline
    (3) We have proved that $\tilde{f}$ is an injective homomorphism of groups, with $\mathrm{Im}(\tilde{f})=\mathrm{Im}(f)$. So $\tilde{f}$ defines an isomorphism of group $V/\ker(f)\longrightarrow \mathrm{Im}(f)$. Moreover, $\tilde{f}(a[x])=\tilde{f}([ax])=f(ax)=af(x)=a\tilde{f}([x])$. So $\tilde{f}$ is a homomorphism of left $K$-modules.
\end{proofenv}


\section{Quotient Ring}
\begin{propositionenv}
    Let $A$ be a unitary ring. Let $\sim$ be an equivalence relation on $A$ that is compatible with the addition and with the multiplication. Then $A/\sim$ equipped with the quotient composition law of $+$ and $\cdot$ forms a unitary ring, and the projection mapping $\pi:A\longrightarrow A/\sim$ is a homomorphism of unitary ring.
\end{propositionenv}
\begin{proofenv}
    We have seen that $(A/\sim,+)$ forms an abelian group and $(A,\cdot)$ forms a monoid, and $\pi:A\longrightarrow A/\sim$ is a homomorphism of additive groups and multiplicative monoids. It remains to check the distributivity.
    $$[a]([b]+[c])=[a][b+c]=[a(b+c)]=[ab+ac]=[ab]+[ac]=[a][b]+[a][c].$$
    $$([b]+[c])[a]=[(b+c)a]=[ba+ca]=[b][a]+[c][a].$$
\end{proofenv}
\begin{definitionenv}
    $A/\sim$ is called the \textbf{quotient ring of $A$ }.
\end{definitionenv}
\begin{remark}
    There exists a subgroup $I$ of $A$ such that 
    $$a\sim b\Leftrightarrow b-a\in I.$$
    $\forall x\in I,[x]=0$, so  for any $ a\in A$,
    $$[ax]=[a][x]=0,\ [xa]=[x][a]=0.$$
    So $I$ is a left sub-$A$-module of $A$ and a right sub-$A$-module of $A$.
\end{remark}
\begin{definitionenv}
    Let $A$ be a unitary ring. If a subset $I$ of $A$ is a left sub-$A$-module of $A$ and a right sub-$A$-module of $A$, then we call $I$ a \textbf{ideal} of $A$. If $I$ is an ideal of $A$, then the composition laws of $A$ define by passing to quotient a structure of unitary ring on the quotient mapping $A/I$. So that $A/I$ becomes a quotient ring of $A$.
\end{definitionenv}
\begin{theoremenv}
    Let $f:A\rightarrow B$ be a homomorphism of unitary rings. Let $I=\ker(f)$.
    \newline
    (1) $I$ is an ideal of $A$.
    \newline
    (2) $f(A)$ is a unitary subring of $B$.
    \newline
    (3) $f$ induces $\tilde{f}:A/I\longrightarrow f(A)$ an isomorphism of unitary rings.
\end{theoremenv}
\begin{proofenv}
    \quad \newline
    (1) $$\forall a\in A,\forall x\in I, f(ax)=f(a)f(x)=f(a)0=0=0f(a)=f(x)f(a)=f(xa).$$
    So $\{ax,xa\}\subseteq I$. Since $I$ is a subgroup of $A$, it is actually an ideal.
    \newline
    (2) Since $f$ is a homomorphism of groups $(A,+)\longrightarrow(B,+)$ and a homomorphism of monoids $(A,\cdot)\longrightarrow(B,\cdot)$, $f(A)$ is a subgroup of $(B,+)$ and a submonoid of $(B,\cdot)$.
    \newline
    (3) $\tilde{f}$ is a homomorphism of unitary rings. $\tilde{f}([x]):=f(x)$. In the same time $\tilde{f}:A/I\longrightarrow f(A)$ is a bijection. So it is a homomorphism of rings.
\end{proofenv}
\begin{exampleenv}
    Consider $\ZZ$. Let $I$ be an ideal of $\ZZ$. If $I\not=\{0\}$, then $I\cap\NN_{\ge1}\not=\varnothing$. Let $d\in I\cap\NN_{\ge1}$ be the least element. For any $n\in I$, we can write $n$ as 
    $$n=dm+r,\text{ where }m\in\ZZ,r\in \{0,\dots,d-1\}.$$
    So $r=n-dm\in I$, which means $r=0$. Therefore, $I=d\ZZ$.
\end{exampleenv}
\begin{definitionenv}
    Let $A$ be a communitative unitary ring. If an ideal of $A$ is of the form 
    $$Ax:\{ax\mid a\in A\}\text{ with }x\in A.$$
    We say that it is a \textbf{principal ideal}. If all ideals of $A$ are principal, we say that $A$ is a \textbf{principal ideal ring}. 
\end{definitionenv}
\begin{exampleenv}
    $\ZZ$ is a principal ideal ring.
\end{exampleenv}
\begin{remark}
    If $A$ is a unitary ring, $\ZZ\longrightarrow A,\ n\longmapsto n1_A$ is the unique homomorphism of unitary rings. $\ker\left(\ZZ\longrightarrow A\right)$ is an ideal of $\ZZ$. It is of the form $d\ZZ,d\in \NN$. This natural number $d$ is called the \textbf{characteristic} of $A$, denoted as $\mathrm{char}(A)$.
\end{remark}
\begin{definitionenv}
    Let $A$ be a communitative unitary ring. Let $a\in A$. If $\exists b\in A\backslash\{0\}$ such that $ab=0$, we say that $a$ is a zero divisor. If $0\in A$ is the ONLY zero divisor, we say that $A$ is an \textbf{(integral) domain}.
\end{definitionenv}
\begin{box2}
    $A$ is an integral domain if and only if $0\not=1$, and $\forall(a,b)\in \left(A\backslash\{0\}\right)^2,\ ab\not=0$.
\end{box2}
\begin{exampleenv}
    \quad \newline
    $\ZZ$ is an integral domain.
    \newline
     All field are integral domains.
     \newline
      $\ZZ/6\ZZ$ is NOT a integral domain.$[2][3]=[6]=[0].$
\end{exampleenv}
\begin{propositionenv}
    All unitary subrings of an integral domain are integral domains.
\end{propositionenv}
\begin{propositionenv}
    Let $A$ be a unitary ring. $E$ be a left A-module and $I$ be an ideal of $A$. Suppose that 
    $$\forall (a,x)\in I\times E,\ ax=0.\ (I\text{ annihilates }E)$$
    Then the mapping
    $$\left(A/I\right)\times E\longrightarrow E,$$
    $$\left([a],x\right)\longmapsto ax$$
    is well defined and defines a left $A$-module structure on $E$.
\end{propositionenv}
\begin{proofenv}
    If $[a]=[b]$, then $b-a\in I$. So $\forall x\in E, (b-a)x=bx=ax=0$. Hence $ax=bx$. $\forall (a,b)\in A\times A,\forall (x,y)\in E\times E$:
    \newline
    (1) $[1]x=1x=x.$ $\left([a][b]\right)x=[ab]x=(ab)x=a(bx)=[a](bx)=[a]([b]x)$.
    \newline
    (2) $\left([a]+[b]\right)x=[a+b]x=(a+b)x=ax+bx=[a]x+[b]x.$ $[a](x+y)=a(x+y)=ax+ay=[a]x+[a]y$.
\end{proofenv}



\section{Free Modules}
\textit{We fix a unitary ring $K$.}
\begin{definitionenv}
    Let $V$ be a left K-module. For any family $\underline{x}:=\left(x_i\right)_{i\in I}\in V^I$, we denote by 
    $$\varphi_{\underline{x}}:K^{\oplus I}\longrightarrow V$$
    the homomorphism sending $(a_i)_{i\in I}$ to $\sum_{i\in I}a_ix_i.$
    \newline
    (1) $\mathrm{Im}\left(\varphi_{\underline{x}}\right)$ is a left $K$-submodule of $V$, called the \textbf{left sub-K-module generated by }$\underline{x}$, denote as $\mathrm{Span}_{K}\left((x_i)_{i\in I}\right)$. If $\varphi_{x}$ is surjective, we say that $(x_i)_{i\in I}$ is a system of generators of $V$.
    ($\forall y\in V,\exists (a_i)_{i\in I}\in K^{\oplus I}, y=\sum_{i\in I}a_ix_i$) Elements of $\mathrm{Span}_K\left((x_i)_{i\in I}\right)$ are called \textbf{K-linear combinations} of $(x_i)_{i\in I}$.
    \newline
    (2) If $\varphi_{\underline{x}}$ is injective, we say that $(x_i)_{i\in I}$ is \textbf{K-linearly independent}. ($\forall (a_i)_{i\in I}\in K^{\oplus I},\sum_{i\in I}a_ix_i=0\rightarrow a_i=0,\forall i\in I$)
    \newline
    (3) If $\varphi_{\underline{x}}$ is an isomorphism, we say $(x_i)_{i\in I}$ is a \textbf{basis} of $V$. If $V$ has at least a basis, we say that $V$ is a \textbf{free left K-module}. If $V$ has a system of generators $(x_i)_{i\in I}$ such that $I$ is finite, we say that $V$ is \textbf{finitely generated}, or is \textbf{finite types}.
\end{definitionenv}
\begin{exampleenv}
    $K^{\oplus I}$ is a free left $K$-module.
\end{exampleenv}
\begin{remark}
    Any left $K$-module is isomorphic to a free quotient module of a free left $K$-module.
\end{remark}
\begin{theoremenv}\label{6.10.4}
    Let $K$ be a division ring and $V$ be a left $K$-module of finite type. Let $(x_i)_{i=1}^n$ be a system of generators of $V$. There exists $I\subseteq\{1,\dots,n\}$ such that $(x_i)_{i\in I}$ forms a basis of $V$.
\end{theoremenv}
\begin{proofenv}
    By induction on $n$.
    \newline
    Case $n=0$, $V=\{0\}. (x_i)_{i\not\in \varnothing}$ is a basis of $V$. Suppose that $n\geq1$. If $(x_i)_{i=1}^n$  is K-linearly independent, it is already a basis. Otherwise there exists $0\not=(b_1,\dots,b_n)\in K^n$ such  that $b_1x_1+\dots+b_nx_n=0.$ By permuting $x_1,\dots,x_n$, we may assume that $b_n\not=0$. $x_n=-b_n^{-1}(b_1x_1+\dots+b_{n-1}x_{n-1})$. For any $y\in V$, there exists $(a_1,\dots,a_n)\in K^n$, such that 
    $$y=\sum_{i=1}^{n}a_ix_i=\sum_{i=1}^{n-1}a_ix_i-a_nb_n^{-1}\left(b_1x_1+\dots+b_{n-1}x_{n-1}\right).$$
\end{proofenv}
\begin{theoremenv}\label{6.10.5}
    Let $K$ be a unitary ring. $V$ be a left $K$-module  and $W$ bw a left sub-$K$-module of $V$. Let $(x_i)_{i=1}^{n}\in W^n$ and $\left(\alpha_j\right)_{j=1}^l\in \left(V/W\right)^l$, with $(n,l)\in \NN^2$. For any $j\in \{1,\dots,l\}$. Let $x_{n+j}$ be an element of the equivalence class of $\alpha_j$. ($[x_{n+j}]=\alpha_j$)
    \newline
    (1) If $(x_i)_{i=1}^n$ and $(\alpha_j)_{j=1}^l$ are K-linearly independent, then $(x_i)_{i=1}^{n+l}$ is K-linearly independent.
    \newline
    (2) If $(x_i)_{i=1}^{n}$ and $(\alpha_j)_{j=1}^l$ are system of generators, then $(x_i)_{i=1}^{n+l}$ is a system of generators. 
\end{theoremenv}
\begin{proofenv}
    \quad\newline
    (1) Let $(a_i)_{i=1}^l\in K^{n+l}$ such that 
    $$\sum_{i=1}^{n+l}a_ix_i=0.$$
    Taking the equivalence class of both sides in $V/W$, we get $\displaystyle \sum_{j=1}^{l}a_{n+j}\alpha_j=[0]$. So $a_{n+1}=\dots=a_{n+l}=0$. Hence $a_1x_1+\dots+a_nx_n=0$. So $a_1=\dots=a_n=0$.
    \newline
    (2)Let $y\in V$. There exists $(c_{n+1},\dots,c_{n+l})\in K^l$, such that 
    $$[y]=c_{n+1}\alpha_1+\dots +c_{n+l}\alpha_l=[c_{n+1}x_{n+1}+\dots+c_{n+l}x_{n+l}].$$
    So, $y-\left(c_{n+1}x_{n+1}+\dots+c_{n+l}x_{n+l}\right)\in W$. Hence, there exists $(c_1,\dots,c_n)\in K^n$,
    $$y-\left(c_{n+1}x_{n+1}+\dots+c_{n+l}x_{n+l}\right)=c_1x_1+\dots+c_nx_n.$$
    So $\displaystyle y= \sum_{i=1}^{n+l}c_ix_i$.
\end{proofenv}
\begin{definitionenv}
    Let $K$ be a division ring and $V$ is a left $K$-module of finite type. We denote by $\mathrm{rk}_K(V)$ or $\mathrm{rk}(V)$ the least cardinality of the bases $V$, called the \textbf{rank} of $V$. If $K$ is a field, then $\mathrm{rk}(V)$ is also denoted as $\dim(V)$, called the \textbf{dimension} of $V$. If $f:W\longrightarrow V$ is a homomorphism of left $K$-modules, the rank of $f$ is defined as the rank of $\mathrm{Im}(f)$, denoted as $\mathrm{rk}(f)$.
\end{definitionenv}







\begin{theoremenv}[rank-nullity theorem]
    Let $K$ be a division ring and $V$ be a left $K$-module of finite type, and $W$ be a left sub-$K$-module of $V$. 
    \newline
    (1) $W$ and $V/W$ are of finite type, and $\mathrm{rk}(W)+\mathrm{rk}(V/W)=\mathrm{rk}(V)$.
    \newline
    (2) Any basis of $V$ has $\mathrm{rk}(V)$ as its cardinality.
\end{theoremenv}
\begin{proofenv}
    \quad\newline
    (1) Let $\left(x_i\right)_{i=1}^n$ be a basis of $V$. Then $\left([x_i]\right)_{i=1}^n$ also form a system opf generators of $V/W$. By theorem \ref{6.10.4}, one can extract a subset $I\subseteq\{1,2\dots,n\}$ such that $\left([x_i]_{i\in I}\right)$ forms a basis of $V/W$. By permuting the elements $x_1,x_2,\dots, x_n$, we may assume, without loss of generality, that $I=\{1,2,\dots,l\},\ l\le n$. For any $j\in\{l+1,\dots, n\}$ there exists $(b_{j,1},\dots,b_{j,l})$ such that 
     $$[x_j]=\sum_{i=1}^{l}b_{j,i}[x_{i}].$$
     $$y_j:=x_j-\sum_{i=1}^{l}b_{j,i}x_i.$$
     For any $x\in W$, there exists $\left(a_i\right)_{i=1}^n\in K^n$ such that 
     $$x=\sum_{i=1}^{l}a_ix_i+\sum_{j=l+1}^{n}a_j\left(y_j+\sum_{i=1}^{l}b_{j,i}x_i\right)=\sum_{i=1}^{l}\left(a_i+\sum_{j=l+1}^{n}a_jb_{j,i}\right)x_i+\sum_{j=l+1}^{n}a_jy_j.$$
     Taking the equivalence class of $x\in V/W$ (i.e.$[0]$) we obtain. 
     $$\forall i\in \{1,\dots,l\},\ a_i+\sum_{j=l+1}^{n}a_jb_{i,j}=0.$$
     Hence, 
     $$x=\sum_{j=l+1}^{n}a_jy_j.$$
     Therefore, $W$ is of finite type, and $\mathrm{rk}(W)+\mathrm{rk}(V/W)\le \mathrm{rk}(V)$. Moreover, by theorem \ref{6.10.5},
     $$\mathrm{rk}(V)\le\mathrm{rk}(W)+\mathrm{rk}(V/W).$$
     Hence, $$\mathrm{rk}(W)+\mathrm{rk}(V/W)=\mathrm{rk}(V).$$
     (2) We reason by induction on $\mathrm{rk}(V)$. If $\mathrm{rk}(V)=0$, then $\{\varnothing\}$ is the only basis. If $\mathrm{rk}(V)=1$, then $V$ is of the form $Ke$, where $e$ is a non-zero element of $V$. Suppose $\mathrm{rk}(V)=n\ge 1$, and the statement has been proven for modules of $\mathrm{rk}<n$. Let $\left(e_i\right)_{i=1}^m$ be a basis of $V$. Let $W=K\cdot e_1$. Then, $\left([e_i]\right)_{i=2}^m$ forms a system of generators of $V/W$. Moreover, $\left(a_i\right)_{i=2}^m\in K^{m-1}$ such that 
     $$\sum_{i=2}^{m}a_i[e_i]=0,$$
     then,
     $$\sum_{i=2}^{m}a_ie_i\in W,$$
     and hence, there exists $a_1\in K$,
     $$\sum_{i=1}^{m}a_ie_i=0.$$
     We conduct that, in particular
     $$a_2=\dots=a_n=0.$$
     Hence $\left([e_i]\right)_{i=2}^m$ is a basis of $V/W$, $\mathrm{rk}(V/W)=n-1$, so $n-1=m-1\Leftrightarrow n=m.$
\end{proofenv}

\section{Algebra}
\textit{In this section, we fix a communicative unitary ring $K$.}
\begin{definitionenv}
    By $K$-algebra, we mean a $K$-module $A$ equipped with a composition law 
    $$A\times A\longrightarrow A$$
    $$(a,b)\longmapsto a\cdot b$$
    such that $A$ with addition and the compositon law forms a unitary ring and
    $$\forall (\lambda,a,b)\in K\times A\times A,\ \lambda\left(ab\right)=\left(\lambda a\right)b=a\left(\lambda b\right).$$
\end{definitionenv}





\appendix












\chapter{Axioms}

\section{Axiom of Foundation (Regularity)}

\begin{axiomenv}[Axiom of foundation]
    $$\forall A(A\not=\varnothing\rightarrow\exists x\in A(x\cap A=\varnothing)).$$
    Regularity means that :If $A$ is a non-empty set, then there exists at least one element $x$ of $A$ satisfies: Either $x$ is not a set or does not intersect with $A$.
\end{axiomenv}
Based on axiom of foundation, we have the following propositions.
\begin{propositionenv}
    
    There does not exists a set $S$,  such that $S\in S$.
\end{propositionenv}
\begin{proofenv}
    If $S=\varnothing, \varnothing\notin\varnothing$; If $S\not=\varnothing$, consider the set $\{S\}$ , it has a single element $S$. Since $S\in S, S\in (S\cap\{S\})\not=\varnothing$.
\end{proofenv}
\begin{propositionenv}

    Let $(X, <)$ be a strictly ordered set,  $(A_i)_{i\in X}$ be a family of sets.If $\forall (i, j)\in X^2, i<j\Rightarrow(\exists k\in X, i<k\le j\wedge A_i\in A_k)$,  then $(X, \le)$ is a well ordered set.
\end{propositionenv}
\begin{proofenv}
    
    Let $I$ be a subset of $X$,  $S:=\{A_i|i\in I\}$.If $(i, j)\in S^2, i<j$,  then $\exists A_k\in S, A_i\in A_k$.Since $A_i\in S$ at the same time,  $A_k\cap S\subseteq\{A_i\}\not=\varnothing$.By axiom of regularity, $\exists A_i\in S, A_i\cap S=\varnothing$.Hence, $\forall A_j\in S, A_j\notin A_i$.Thus, $j\not>i$, which leads to $\forall j\in I,  j>i$.$i$ is the least element of $I$.
\end{proofenv}
\begin{propositionenv}
   Let $(X, <)$ be a strictly ordered set,  $(A_i)_{i\in X}$ be a family of sets.If $\forall (i, j)\in X^2, i<j\Rightarrow(\exists k\in X,  A_i\in A_k)$,  then $\forall (i, j)\in X^2, i<j\Rightarrow A_j\notin A_i$ .
\end{propositionenv}
\begin{proofenv}
    If $\exists i<j\in X, A_j\in A_i$ and $i<k\le j, A_i\in A_k$, we consider the set $S=\{A_k|i\le k\le j\}, \forall A_k\in S, \exists A_m\in S, A_k\in A_m$.Therefore $A_k\cap S\not=\varnothing$.That contradicts to the axiom.
\end{proofenv}
\begin{corollaryenv}
    Let $X, Y$ be two sets,  if $X\in Y$,  then $Y\notin X$.
\end{corollaryenv}


\chapter{Inequalities}
\begin{definitionenv}[Convex functions on intervals]
    Let $X\subseteq \mathbb{R}$ bbe an interval.
    \newline 
    A function $f:X\rightarrow \mathbb{R}$ is called \textbf{convex} for all $x_1, x_2\in X, t\in \interval{0}{1}$
    $$f(tx_1+(1-t)x_2)\le tf(x_1)+(1-t)f(x_2)$$
    \newline
    A function $f:X\rightarrow \mathbb{R}$ is called \textbf{concave} for all $x_1, x_2\in X, t\in \interval{0}{1}$
    $$f(tx_1+(1-t)x_2)\ge tf(x_1)+(1-t)f(x_2)$$
\end{definitionenv}
\begin{theoremenv}[Jensen's Inequality]
    Let $f$ be a convex function $0\le \alpha_i\le 1$ for $i=1, 2, \dots , n$, such that 
    $$\sum_{i=1}^{n}\alpha_1=1.\text{show that } \forall x_i\in X$$
    $$f\left(\sum_{i=1}^{n}a_ix_i\right)\le \sum_{i=1}^{n}\alpha_if(x_i)$$
    Equality holds if and only if $x_1=x_2=\dots =x_n$ or $f$ is linear on some interval containing $x_1, x_2, \dots, x_n$.
\end{theoremenv}
\begin{remark}
    Consider $f(x)=x^2$ , we obtain Cauchy-Schwartz inequality, $f(x)=\ln (x)$,  we obtain AM-GM-HM inequality.
\end{remark}
\begin{theoremenv}[Young's Inequality]
    \quad
    \newline
    Fix $pq>1$ such that $\frac{1}{p}+\frac{1}{q}=1$, then
    $$xy\le \frac{1}{p}x^p+\frac{1}{q}y^q$$
\end{theoremenv}
\begin{theoremenv}[Hölder's Inequality]
    \quad
    \newline
    Fix $pq>1$ such that $\frac{1}{p}+\frac{1}{q}=1$.$x_1, x_2, \dots, x_n;y_1, y_2, \dots, y_n\ge 0$, then
    $$\sum_{i=1}^{n}x_iy_i\le \left(\sum_{i=1}^{n}x_i^p\right)^{\frac{1}{p}}\cdot\left(\sum_{j=1}^{n}y_j^q\right)^{\frac{1}{q}}$$
    In particular,  when $p=q=2$,  this is Cauchy-Schwartz inequality.
\end{theoremenv}
\end{document}
